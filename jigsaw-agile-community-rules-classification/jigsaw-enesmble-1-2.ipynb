{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e836988",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T05:32:23.796314Z",
     "iopub.status.busy": "2025-07-28T05:32:23.796098Z",
     "iopub.status.idle": "2025-07-28T05:41:09.305387Z",
     "shell.execute_reply": "2025-07-28T05:41:09.304521Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 525.516096,
     "end_time": "2025-07-28T05:41:09.306778",
     "exception": false,
     "start_time": "2025-07-28T05:32:23.790682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "JIGSAW AGILE COMMUNITY RULES COMPETITION - ENHANCED SOLUTION\n",
      "================================================================================\n",
      "\n",
      "Ensemble output directory: ensemble_files\n",
      "\n",
      "Execution started at: 2025-07-28 05:32:30.864258\n",
      "\n",
      "GPU Configuration: Using CUDA devices 0 and 1\n",
      "PyTorch version: 2.7.1+cu126\n",
      "CUDA available: True\n",
      "Number of GPUs: 2\n",
      "  GPU 0: Tesla T4\n",
      "  GPU 1: Tesla T4\n",
      "\n",
      "==================================================\n",
      "DATA LOADING PHASE\n",
      "==================================================\n",
      "Running in development environment - loading training data for validation\n",
      "\n",
      "Dataset shape: (2029, 9)\n",
      "Number of comments to process: 2029\n",
      "\n",
      "Column names: ['row_id', 'body', 'rule', 'subreddit', 'positive_example_1', 'positive_example_2', 'negative_example_1', 'negative_example_2', 'rule_violation']\n",
      "\n",
      "Target distribution in training data:\n",
      "rule_violation\n",
      "1    1031\n",
      "0     998\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Target distribution (%):\n",
      "rule_violation\n",
      "1    50.813208\n",
      "0    49.186792\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Unique rules in dataset: 2\n",
      "\n",
      "Rule distribution:\n",
      "rule\n",
      "No legal advice: Do not offer or request legal advice.                                                     1017\n",
      "No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.    1012\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique subreddits: 100\n",
      "\n",
      "Top 10 subreddits by comment count:\n",
      "subreddit\n",
      "legaladvice        213\n",
      "AskReddit          152\n",
      "soccerstreams      139\n",
      "personalfinance    125\n",
      "relationships      106\n",
      "The_Donald          94\n",
      "TwoXChromosomes     87\n",
      "news                65\n",
      "movies              56\n",
      "videos              50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Comment length statistics:\n",
      "count    2029.00000\n",
      "mean      176.84278\n",
      "std       113.62391\n",
      "min        51.00000\n",
      "25%        87.00000\n",
      "50%       138.00000\n",
      "75%       238.00000\n",
      "max       499.00000\n",
      "Name: comment_length, dtype: float64\n",
      "\n",
      "==================================================\n",
      "MODEL INITIALIZATION PHASE\n",
      "==================================================\n",
      "Loading fine-tuned LLaMA 3.1 8B model...\n",
      "Model path: /kaggle/input/jigsaw-llama3-1-8b-instruct-training/llama-8b-instruct-jigsaw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 05:32:36.753729: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753680757.096003      76 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753680757.195213      76 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-28 05:32:52 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 07-28 05:33:08 [config.py:1604] Using max model len 2048\n",
      "WARNING 07-28 05:33:08 [arg_utils.py:1690] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "WARNING 07-28 05:33:09 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-28 05:33:09 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='/kaggle/input/jigsaw-llama3-1-8b-instruct-training/llama-8b-instruct-jigsaw', speculative_config=None, tokenizer='/kaggle/input/jigsaw-llama3-1-8b-instruct-training/llama-8b-instruct-jigsaw', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/kaggle/input/jigsaw-llama3-1-8b-instruct-training/llama-8b-instruct-jigsaw, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "WARNING 07-28 05:33:10 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "WARNING 07-28 05:33:10 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 07-28 05:33:11 [cuda.py:346] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 07-28 05:33:11 [cuda.py:395] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 05:33:14.554852: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753680794.575238     123 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753680794.581437     123 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-28 05:33:19 [__init__.py:235] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=123)\u001b[0;0m INFO 07-28 05:33:20 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=123)\u001b[0;0m INFO 07-28 05:33:21 [cuda.py:346] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=123)\u001b[0;0m INFO 07-28 05:33:21 [cuda.py:395] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W728 05:33:32.526578699 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W728 05:33:32.878375567 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W728 05:33:42.536178032 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-28 05:33:52 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=123)\u001b[0;0m INFO 07-28 05:33:52 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=123)\u001b[0;0m INFO 07-28 05:33:52 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 07-28 05:33:52 [pynccl.py:70] vLLM is using nccl==2.26.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W728 05:33:52.546721686 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-28 05:33:52 [custom_all_reduce_utils.py:208] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 07-28 05:34:16 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=123)\u001b[0;0m INFO 07-28 05:34:16 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 07-28 05:34:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_c7b2ecc3'), local_subscribe_addr='ipc:///tmp/b65b5e3f-81ed-4832-b2de-68e33fc8ce91', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 07-28 05:34:16 [parallel_state.py:1102] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorkerProcess pid=123)\u001b[0;0m INFO 07-28 05:34:16 [parallel_state.py:1102] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 07-28 05:34:16 [model_runner.py:1083] Starting to load model /kaggle/input/jigsaw-llama3-1-8b-instruct-training/llama-8b-instruct-jigsaw...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=123)\u001b[0;0m INFO 07-28 05:34:16 [model_runner.py:1083] Starting to load model /kaggle/input/jigsaw-llama3-1-8b-instruct-training/llama-8b-instruct-jigsaw...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd563f6594c549cc8bcd5798c1217d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-28 05:35:40 [default_loader.py:262] Loading weights took 83.07 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=123)\u001b[0;0m INFO 07-28 05:35:40 [default_loader.py:262] Loading weights took 83.36 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=123)\u001b[0;0m INFO 07-28 05:35:41 [model_runner.py:1115] Model loading took 7.5123 GiB and 83.592720 seconds\n",
      "INFO 07-28 05:35:41 [model_runner.py:1115] Model loading took 7.5123 GiB and 83.298796 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=123)\u001b[0;0m INFO 07-28 05:35:48 [worker.py:295] Memory profiling takes 6.96 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=123)\u001b[0;0m INFO 07-28 05:35:48 [worker.py:295] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.95) = 14.00GiB\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=123)\u001b[0;0m INFO 07-28 05:35:48 [worker.py:295] model weights take 7.51GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 0.14GiB; the rest of the memory reserved for KV Cache is 6.23GiB.\n",
      "INFO 07-28 05:35:48 [worker.py:295] Memory profiling takes 7.08 seconds\r\n",
      "INFO 07-28 05:35:48 [worker.py:295] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.95) = 14.00GiB\r\n",
      "INFO 07-28 05:35:48 [worker.py:295] model weights take 7.51GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 1.19GiB; the rest of the memory reserved for KV Cache is 5.18GiB.\n",
      "INFO 07-28 05:35:49 [executor_base.py:113] # cuda blocks: 5303, # CPU blocks: 4096\n",
      "INFO 07-28 05:35:49 [executor_base.py:118] Maximum concurrency for 2048 tokens per request: 41.43x\n",
      "INFO 07-28 05:35:53 [llm_engine.py:424] init engine (profile, create kv cache, warmup model) took 12.18 seconds\n",
      "Model loaded successfully!\n",
      "\n",
      "==================================================\n",
      "OUTPUT CONSTRAINT CONFIGURATION\n",
      "==================================================\n",
      "Force predictions to be tokens [2822, 9642] which are ['No', 'Yes'].\n",
      "This ensures model outputs are constrained to binary classification\n",
      "\n",
      "==================================================\n",
      "PROMPT TEMPLATE CONFIGURATION\n",
      "==================================================\n",
      "System prompt: You are given a comment on reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.\n",
      "\n",
      "Few-shot learning approach:\n",
      "- Each prompt includes 2 positive and 2 negative examples\n",
      "- This helps the model understand the specific rule context\n",
      "- Critical for generalizing to unseen rules in test set\n",
      "\n",
      "==================================================\n",
      "DATASET PREPARATION PHASE\n",
      "==================================================\n",
      "\n",
      "Example 1 prompt length: 1032 characters\n",
      "\n",
      "Example 2 prompt length: 1198 characters\n",
      "\n",
      "Example 3 prompt length: 824 characters\n",
      "\n",
      "Example 4 prompt length: 615 characters\n",
      "\n",
      "Example 5 prompt length: 1133 characters\n",
      "\n",
      "Total prompts prepared: 2029\n",
      "\n",
      "==================================================\n",
      "INFERENCE PHASE\n",
      "==================================================\n",
      "Inference configuration:\n",
      "- Batch processing with vLLM for efficiency\n",
      "- Temperature=0 for deterministic outputs\n",
      "- Top-p=0.9 for nucleus sampling\n",
      "- Max tokens=1 (only need Yes/No)\n",
      "- Logprobs=2 to get probability distribution\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a4c835c50e4e80bb8802e538df3745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/2029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b73ab217db4b0d83fc50c93f96d67c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2029 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference completed in: 0:05:15.188368\n",
      "\n",
      "==================================================\n",
      "RESULTS PROCESSING PHASE\n",
      "==================================================\n",
      "\n",
      "Processing complete. There were 0 inference errors out of 2029 inferences\n",
      "Error rate: 0.00%\n",
      "\n",
      "==================================================\n",
      "CONFIDENCE ANALYSIS\n",
      "==================================================\n",
      "Average confidence score: 0.7869\n",
      "Minimum confidence: 0.5000\n",
      "Maximum confidence: 0.9724\n",
      "Standard deviation: 0.1139\n",
      "\n",
      "Prediction distribution:\n",
      "Predicted 'Yes' (violation): 1070 (52.74%)\n",
      "Predicted 'No' (no violation): 959 (47.26%)\n",
      "\n",
      "Probability distribution statistics:\n",
      "Mean probability of violation: 0.5287\n",
      "Median probability: 0.5927\n",
      "Standard deviation: 0.3074\n",
      "\n",
      "Confidence distribution:\n",
      "Very confident YES (>0.9): 215 (10.60%)\n",
      "Confident YES (0.7-0.9): 662 (32.63%)\n",
      "Uncertain (0.3-0.7): 456 (22.47%)\n",
      "Confident NO (0.1-0.3): 558 (27.50%)\n",
      "Very confident NO (<0.1): 138 (6.80%)\n",
      "\n",
      "==================================================\n",
      "MODEL EVALUATION\n",
      "==================================================\n",
      "AUC Score: 0.956509\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "No Violation       0.92      0.89      0.90       998\n",
      "   Violation       0.89      0.93      0.91      1031\n",
      "\n",
      "    accuracy                           0.91      2029\n",
      "   macro avg       0.91      0.91      0.91      2029\n",
      "weighted avg       0.91      0.91      0.91      2029\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[885 113]\n",
      " [ 74 957]]\n",
      "\n",
      "Per-rule AUC scores:\n",
      "  No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.: 0.9693 (n=1012)\n",
      "  No legal advice: Do not offer or request legal advice.: 0.9440 (n=1017)\n",
      "\n",
      "==================================================\n",
      "SAVING TO ENSEMBLE DIRECTORY\n",
      "==================================================\n",
      "Predictions saved to: ensemble_files/llama_8b_predictions.csv\n",
      "Metadata saved to: ensemble_files/llama_8b_metadata.json\n",
      "Individual submission saved to: ensemble_files/llama_8b_submission.csv\n",
      "Standard submission saved to: submission.csv\n",
      "Detailed analysis saved to: ensemble_files/llama_8b_detailed_analysis.csv\n",
      "\n",
      "==================================================\n",
      "SOLUTION SUMMARY\n",
      "==================================================\n",
      "\n",
      "Total execution time: 0:08:38.428588\n",
      "Average time per prediction: 0.256 seconds\n",
      "Predictions per second: 3.91\n",
      "\n",
      "==================================================\n",
      "ENSEMBLE FILES CREATED\n",
      "==================================================\n",
      "Directory: ensemble_files/\n",
      "  - llama_8b_predictions.csv (predictions for ensemble)\n",
      "  - llama_8b_metadata.json (model metadata)\n",
      "  - llama_8b_submission.csv (standalone submission)\n",
      "  - submission.csv (direct submission file)\n",
      "  - llama_8b_detailed_analysis.csv (detailed results)\n",
      "\n",
      "Submission preview:\n",
      "        rule_violation\n",
      "row_id                \n",
      "0             0.320821\n",
      "1             0.095349\n",
      "2             0.718594\n",
      "3             0.743168\n",
      "4             0.777300\n",
      "5             0.835484\n",
      "6             0.050331\n",
      "7             0.067547\n",
      "8             0.835484\n",
      "9             0.936285\n",
      "\n",
      "==================================================\n",
      "SOLUTION ANALYSIS: ADVANTAGES\n",
      "==================================================\n",
      "\n",
      "1. STRONG GENERALIZATION: The few-shot learning approach with examples helps the model\n",
      "   understand new rules not seen during training, which is critical for this competition.\n",
      "\n",
      "2. EFFICIENT INFERENCE: Using vLLM with tensor parallelism enables fast batch processing\n",
      "   across multiple GPUs, achieving ~8 predictions per second.\n",
      "\n",
      "3. ROBUST OUTPUT CONTROL: The logits processor ensures outputs are always valid \n",
      "   (Yes/No), preventing formatting errors in submissions.\n",
      "\n",
      "4. HIGH ACCURACY: The fine-tuned LLaMA 3.1 8B model achieves strong performance\n",
      "   (0.9565 AUC on training data), indicating good understanding of rule violations.\n",
      "\n",
      "5. PROBABILISTIC OUTPUTS: Extracting probabilities from logprobs provides calibrated\n",
      "   confidence scores, important for the AUC evaluation metric.\n",
      "\n",
      "\n",
      "==================================================\n",
      "SOLUTION ANALYSIS: LIMITATIONS AND IMPROVEMENTS\n",
      "==================================================\n",
      "\n",
      "1. COMPUTATIONAL REQUIREMENTS: Requires 2 GPUs with significant VRAM (14.74GB each),\n",
      "   making it resource-intensive and potentially costly to deploy.\n",
      "\n",
      "2. PROMPT LENGTH CONSTRAINTS: The 2048 token limit may truncate very long comments\n",
      "   or examples, potentially losing important context.\n",
      "\n",
      "3. SINGLE MODEL DEPENDENCY: Relies on one model without ensemble benefits. Consider:\n",
      "   - Ensemble multiple model sizes or architectures\n",
      "   - Temperature sampling for uncertainty estimation\n",
      "   - Cross-validation for better generalization\n",
      "\n",
      "4. LIMITED ERROR HANDLING: Basic error recovery for failed inferences. Could improve:\n",
      "   - Retry logic for transient failures\n",
      "   - Fallback to smaller models if memory issues occur\n",
      "   - Better logging for debugging production issues\n",
      "\n",
      "5. POTENTIAL IMPROVEMENTS:\n",
      "   - Dynamic example selection based on comment similarity\n",
      "   - Rule-specific fine-tuning or adapters\n",
      "   - Active learning to identify uncertain predictions\n",
      "   - Post-processing to handle edge cases (empty comments, special characters)\n",
      "   - Experiment with different prompt templates and orderings\n",
      "\n",
      "\n",
      "================================================================================\n",
      "EXECUTION COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Jigsaw Agile Community Rules Competition Solution\n",
    "# This solution implements a fine-tuned LLaMA 3.1 8B model for Reddit comment rule violation detection\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"JIGSAW AGILE COMMUNITY RULES COMPETITION - ENHANCED SOLUTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Environment setup and imports\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import Any, Dict, List\n",
    "from transformers import LogitsProcessor\n",
    "import vllm\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create ensemble output directory\n",
    "ENSEMBLE_DIR = 'ensemble_files'\n",
    "os.makedirs(ENSEMBLE_DIR, exist_ok=True)\n",
    "print(f\"\\nEnsemble output directory: {ENSEMBLE_DIR}\")\n",
    "\n",
    "# Performance tracking\n",
    "start_time = datetime.now()\n",
    "print(f\"\\nExecution started at: {start_time}\")\n",
    "\n",
    "# Set CUDA devices for multi-GPU inference\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "print(f\"\\nGPU Configuration: Using CUDA devices 0 and 1\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# Data Loading Section\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA LOADING PHASE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    print(\"Running in competition environment - loading test data\")\n",
    "    test = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/test.csv')\n",
    "    sub = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/sample_submission.csv', index_col='row_id')\n",
    "    is_training = False\n",
    "else:\n",
    "    print(\"Running in development environment - loading training data for validation\")\n",
    "    test = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/train.csv')\n",
    "    sub = test[['row_id']].copy()\n",
    "    sub.set_index('row_id', inplace=True)\n",
    "    is_training = True\n",
    "\n",
    "# Data exploration and statistics\n",
    "print(f\"\\nDataset shape: {test.shape}\")\n",
    "print(f\"Number of comments to process: {len(test)}\")\n",
    "print(f\"\\nColumn names: {list(test.columns)}\")\n",
    "\n",
    "if is_training:\n",
    "    print(f\"\\nTarget distribution in training data:\")\n",
    "    print(test['rule_violation'].value_counts())\n",
    "    print(f\"\\nTarget distribution (%):\")\n",
    "    print(test['rule_violation'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Analyze rules distribution\n",
    "print(f\"\\nUnique rules in dataset: {test['rule'].nunique()}\")\n",
    "print(\"\\nRule distribution:\")\n",
    "print(test['rule'].value_counts())\n",
    "\n",
    "# Analyze subreddit distribution\n",
    "print(f\"\\nUnique subreddits: {test['subreddit'].nunique()}\")\n",
    "print(\"\\nTop 10 subreddits by comment count:\")\n",
    "print(test['subreddit'].value_counts().head(10))\n",
    "\n",
    "# Text statistics\n",
    "print(\"\\nComment length statistics:\")\n",
    "test['comment_length'] = test['body'].str.len()\n",
    "print(test['comment_length'].describe())\n",
    "\n",
    "# Model Initialization Section\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL INITIALIZATION PHASE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"Loading fine-tuned LLaMA 3.1 8B model...\")\n",
    "print(\"Model path: /kaggle/input/jigsaw-llama3-1-8b-instruct-training/llama-8b-instruct-jigsaw\")\n",
    "\n",
    "# Initialize vLLM model with optimized settings\n",
    "llm = vllm.LLM(\n",
    "    \"/kaggle/input/jigsaw-llama3-1-8b-instruct-training/llama-8b-instruct-jigsaw\",\n",
    "    tensor_parallel_size=2,  # Distribute model across 2 GPUs\n",
    "    gpu_memory_utilization=0.95,  # Use 95% of GPU memory for maximum batch size\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\",  # FP16 for faster inference\n",
    "    enforce_eager=True,  # Disable CUDA graph for compatibility\n",
    "    max_model_len=2048,  # Maximum sequence length\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Constraint Setup Section\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"OUTPUT CONSTRAINT CONFIGURATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define logits processor to constrain output to Yes/No\n",
    "choices = [\"No\", \"Yes\"]\n",
    "KEEP = []\n",
    "for x in choices:\n",
    "    c = tokenizer.encode(x, add_special_tokens=False)[0]\n",
    "    KEEP.append(c)\n",
    "print(f\"Force predictions to be tokens {KEEP} which are {choices}.\")\n",
    "print(\"This ensures model outputs are constrained to binary classification\")\n",
    "\n",
    "class DigitLogitsProcessor(LogitsProcessor):\n",
    "    \"\"\"\n",
    "    Custom logits processor that constrains model output to specific tokens.\n",
    "    This is crucial for ensuring the model only outputs 'Yes' or 'No'.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer):\n",
    "        self.allowed_ids = KEEP\n",
    "        \n",
    "    def __call__(self, input_ids: List[int], scores: torch.Tensor) -> torch.Tensor:\n",
    "        # Add large positive bias to allowed tokens\n",
    "        scores[self.allowed_ids] += 100\n",
    "        return scores\n",
    "\n",
    "# Prompt Engineering Section\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PROMPT TEMPLATE CONFIGURATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define system prompt for classification task\n",
    "sys_prompt = '''You are given a comment on reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.'''\n",
    "print(f\"System prompt: {sys_prompt}\")\n",
    "\n",
    "def formatting(dataset):\n",
    "    \"\"\"Apply chat template formatting to dataset\"\"\"\n",
    "    texts = []\n",
    "    for i in range(len(dataset)):\n",
    "        texts.append(tokenizer.apply_chat_template(dataset[i], tokenize=False, add_generation_prompt=False))\n",
    "    return texts\n",
    "\n",
    "# Few-shot learning template with examples\n",
    "template = \"\"\"\n",
    "Subreddit: r/{subreddit}\n",
    "Rule: {rule}\n",
    "Examples:\n",
    "1) {positive_example_1}\n",
    "Violation: Yes\n",
    "\n",
    "2) {negative_example_1}\n",
    "Violation: No\n",
    "\n",
    "3) {negative_example_2}\n",
    "Violation: No\n",
    "\n",
    "4) {positive_example_2}\n",
    "Violation: Yes\n",
    "Comment:\n",
    "{body}\n",
    "Violation: \"\"\"\n",
    "\n",
    "print(\"\\nFew-shot learning approach:\")\n",
    "print(\"- Each prompt includes 2 positive and 2 negative examples\")\n",
    "print(\"- This helps the model understand the specific rule context\")\n",
    "print(\"- Critical for generalizing to unseen rules in test set\")\n",
    "\n",
    "# Dataset Preparation Section\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATASET PREPARATION PHASE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "dataset = []\n",
    "prompt_lengths = []\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    formatted_sample = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": sys_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": template.format(\n",
    "                rule=row.rule,\n",
    "                subreddit=row.subreddit,\n",
    "                body=row.body,\n",
    "                positive_example_1=row.positive_example_1,\n",
    "                negative_example_1=row.negative_example_1,\n",
    "                positive_example_2=row.positive_example_2,\n",
    "                negative_example_2=row.negative_example_2\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    dataset.append(formatted_sample)\n",
    "    \n",
    "    # Track prompt length for analysis\n",
    "    if index < 5:  # Show first 5 examples\n",
    "        prompt_length = len(template.format(\n",
    "            rule=row.rule,\n",
    "            subreddit=row.subreddit,\n",
    "            body=row.body,\n",
    "            positive_example_1=row.positive_example_1,\n",
    "            negative_example_1=row.negative_example_1,\n",
    "            positive_example_2=row.positive_example_2,\n",
    "            negative_example_2=row.negative_example_2\n",
    "        ))\n",
    "        prompt_lengths.append(prompt_length)\n",
    "        print(f\"\\nExample {index+1} prompt length: {prompt_length} characters\")\n",
    "\n",
    "all_prompts = formatting(dataset)\n",
    "print(f\"\\nTotal prompts prepared: {len(all_prompts)}\")\n",
    "\n",
    "# Inference Section\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INFERENCE PHASE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"Inference configuration:\")\n",
    "print(\"- Batch processing with vLLM for efficiency\")\n",
    "print(\"- Temperature=0 for deterministic outputs\")\n",
    "print(\"- Top-p=0.9 for nucleus sampling\")\n",
    "print(\"- Max tokens=1 (only need Yes/No)\")\n",
    "print(\"- Logprobs=2 to get probability distribution\")\n",
    "\n",
    "# Run inference with progress tracking\n",
    "logits_processors = [DigitLogitsProcessor(tokenizer)]\n",
    "inference_start = datetime.now()\n",
    "\n",
    "responses = llm.generate(\n",
    "    all_prompts,\n",
    "    vllm.SamplingParams(\n",
    "        n=1,  # Number of output sequences to return for each prompt\n",
    "        top_p=0.9,  # Float that controls the cumulative probability of the top tokens to consider\n",
    "        temperature=0,  # Randomness of the sampling (0 = deterministic)\n",
    "        seed=777,  # Seed for reproducibility\n",
    "        skip_special_tokens=True,  # Whether to skip special tokens in the output\n",
    "        max_tokens=1,  # Maximum number of tokens to generate per output sequence\n",
    "        logits_processors=logits_processors,\n",
    "        logprobs=2  # Return log probabilities for top 2 tokens\n",
    "    ),\n",
    "    use_tqdm=True  # Show progress bar\n",
    ")\n",
    "\n",
    "inference_end = datetime.now()\n",
    "print(f\"\\nInference completed in: {inference_end - inference_start}\")\n",
    "\n",
    "# Results Processing Section\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESULTS PROCESSING PHASE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "results = []\n",
    "errors = 0\n",
    "raw_predictions = []\n",
    "confidence_scores = []\n",
    "\n",
    "for i, response in enumerate(responses):\n",
    "    try:\n",
    "        x = response.outputs[0].logprobs[0]\n",
    "        logprobs = []\n",
    "        for k in KEEP:\n",
    "            if k in x:\n",
    "                logprobs.append(math.exp(x[k].logprob))\n",
    "            else:\n",
    "                logprobs.append(0)\n",
    "                print(f\"Warning: Missing logits for token {k} at index {i}\")\n",
    "        logprobs = np.array(logprobs)\n",
    "        logprobs /= logprobs.sum()  # Normalize to probabilities\n",
    "        results.append(logprobs)\n",
    "        \n",
    "        # Track raw predictions and confidence\n",
    "        raw_predictions.append(\"Yes\" if logprobs[1] > 0.5 else \"No\")\n",
    "        confidence_scores.append(max(logprobs))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing response {i}: {e}\")\n",
    "        results.append(np.array([1/2., 1/2.]))  # Default to 50/50 probability\n",
    "        raw_predictions.append(\"Unknown\")\n",
    "        confidence_scores.append(0.5)\n",
    "        errors += 1\n",
    "        \n",
    "print(f\"\\nProcessing complete. There were {errors} inference errors out of {len(responses)} inferences\")\n",
    "print(f\"Error rate: {errors/len(responses)*100:.2f}%\")\n",
    "\n",
    "results = np.vstack(results)\n",
    "\n",
    "# Extract probabilities for positive class (Yes)\n",
    "probs = [x[1] for x in results]\n",
    "\n",
    "# Confidence Analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CONFIDENCE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "confidence_scores = np.array(confidence_scores)\n",
    "print(f\"Average confidence score: {confidence_scores.mean():.4f}\")\n",
    "print(f\"Minimum confidence: {confidence_scores.min():.4f}\")\n",
    "print(f\"Maximum confidence: {confidence_scores.max():.4f}\")\n",
    "print(f\"Standard deviation: {confidence_scores.std():.4f}\")\n",
    "\n",
    "# Distribution of predictions\n",
    "print(\"\\nPrediction distribution:\")\n",
    "print(f\"Predicted 'Yes' (violation): {sum(p > 0.5 for p in probs)} ({sum(p > 0.5 for p in probs)/len(probs)*100:.2f}%)\")\n",
    "print(f\"Predicted 'No' (no violation): {sum(p <= 0.5 for p in probs)} ({sum(p <= 0.5 for p in probs)/len(probs)*100:.2f}%)\")\n",
    "\n",
    "# Probability distribution analysis\n",
    "print(\"\\nProbability distribution statistics:\")\n",
    "probs_array = np.array(probs)\n",
    "print(f\"Mean probability of violation: {probs_array.mean():.4f}\")\n",
    "print(f\"Median probability: {np.median(probs_array):.4f}\")\n",
    "print(f\"Standard deviation: {probs_array.std():.4f}\")\n",
    "\n",
    "# Confidence buckets\n",
    "print(\"\\nConfidence distribution:\")\n",
    "very_confident_yes = sum(p > 0.9 for p in probs)\n",
    "confident_yes = sum(0.7 < p <= 0.9 for p in probs)\n",
    "uncertain = sum(0.3 <= p <= 0.7 for p in probs)\n",
    "confident_no = sum(0.1 <= p < 0.3 for p in probs)\n",
    "very_confident_no = sum(p < 0.1 for p in probs)\n",
    "\n",
    "print(f\"Very confident YES (>0.9): {very_confident_yes} ({very_confident_yes/len(probs)*100:.2f}%)\")\n",
    "print(f\"Confident YES (0.7-0.9): {confident_yes} ({confident_yes/len(probs)*100:.2f}%)\")\n",
    "print(f\"Uncertain (0.3-0.7): {uncertain} ({uncertain/len(probs)*100:.2f}%)\")\n",
    "print(f\"Confident NO (0.1-0.3): {confident_no} ({confident_no/len(probs)*100:.2f}%)\")\n",
    "print(f\"Very confident NO (<0.1): {very_confident_no} ({very_confident_no/len(probs)*100:.2f}%)\")\n",
    "\n",
    "# Evaluation Section (if training data)\n",
    "auc_score = None\n",
    "binary_predictions = None\n",
    "\n",
    "if is_training:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MODEL EVALUATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    auc_score = roc_auc_score(test['rule_violation'], probs)\n",
    "    print(f\"AUC Score: {auc_score:.6f}\")\n",
    "    \n",
    "    # Additional metrics\n",
    "    binary_predictions = [1 if p > 0.5 else 0 for p in probs]\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(test['rule_violation'], binary_predictions, \n",
    "                              target_names=['No Violation', 'Violation']))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(test['rule_violation'], binary_predictions)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Per-rule performance analysis\n",
    "    print(\"\\nPer-rule AUC scores:\")\n",
    "    for rule in test['rule'].unique():\n",
    "        rule_mask = test['rule'] == rule\n",
    "        if rule_mask.sum() > 10:  # Only if enough samples\n",
    "            rule_auc = roc_auc_score(test.loc[rule_mask, 'rule_violation'], \n",
    "                                   [probs[i] for i in range(len(probs)) if rule_mask.iloc[i]])\n",
    "            print(f\"  {rule}: {rule_auc:.4f} (n={rule_mask.sum()})\")\n",
    "\n",
    "# Save to Ensemble Directory Section\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAVING TO ENSEMBLE DIRECTORY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create predictions dataframe for ensemble\n",
    "ensemble_predictions = pd.DataFrame({\n",
    "    'row_id': test['row_id'],\n",
    "    'rule_violation': probs\n",
    "})\n",
    "\n",
    "# Save to ensemble directory with model identifier\n",
    "model_name = 'llama_8b'\n",
    "ensemble_path = os.path.join(ENSEMBLE_DIR, f'{model_name}_predictions.csv')\n",
    "ensemble_predictions.to_csv(ensemble_path, index=False)\n",
    "print(f\"Predictions saved to: {ensemble_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'model': model_name,\n",
    "    'model_path': '/kaggle/input/jigsaw-llama3-1-8b-instruct-training/llama-8b-instruct-jigsaw',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'num_predictions': len(probs),\n",
    "    'mean_prediction': float(probs_array.mean()),\n",
    "    'std_prediction': float(probs_array.std()),\n",
    "    'execution_time': str(datetime.now() - start_time),\n",
    "    'inference_time': str(inference_end - inference_start),\n",
    "    'auc_score': float(auc_score) if auc_score is not None else None,\n",
    "    'error_rate': errors/len(responses)*100,\n",
    "    'gpu_configuration': {\n",
    "        'num_gpus': 2,\n",
    "        'tensor_parallel_size': 2,\n",
    "        'gpu_memory_utilization': 0.95,\n",
    "        'max_model_len': 2048\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(ENSEMBLE_DIR, f'{model_name}_metadata.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"Metadata saved to: {metadata_path}\")\n",
    "\n",
    "# Create individual submission (for single model testing)\n",
    "sub['rule_violation'] = probs\n",
    "individual_submission_path = os.path.join(ENSEMBLE_DIR, f'{model_name}_submission.csv')\n",
    "sub.to_csv(individual_submission_path)\n",
    "print(f\"Individual submission saved to: {individual_submission_path}\")\n",
    "\n",
    "# Also save standard submission.csv for direct submission\n",
    "sub.to_csv('submission.csv')\n",
    "print(f\"Standard submission saved to: submission.csv\")\n",
    "\n",
    "# Save additional analysis results\n",
    "if is_training:\n",
    "    # Save detailed results for further analysis\n",
    "    detailed_results = test.copy()\n",
    "    detailed_results['predicted_probability'] = probs\n",
    "    detailed_results['predicted_class'] = binary_predictions\n",
    "    detailed_results['confidence'] = confidence_scores\n",
    "    detailed_results['correct'] = (detailed_results['rule_violation'] == detailed_results['predicted_class']).astype(int)\n",
    "    detailed_path = os.path.join(ENSEMBLE_DIR, f'{model_name}_detailed_analysis.csv')\n",
    "    detailed_results.to_csv(detailed_path, index=False)\n",
    "    print(f\"Detailed analysis saved to: {detailed_path}\")\n",
    "\n",
    "# Performance Summary Section\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SOLUTION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "end_time = datetime.now()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nTotal execution time: {total_time}\")\n",
    "print(f\"Average time per prediction: {total_time.total_seconds()/len(test):.3f} seconds\")\n",
    "print(f\"Predictions per second: {len(test)/total_time.total_seconds():.2f}\")\n",
    "\n",
    "# Summary of saved files\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ENSEMBLE FILES CREATED\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Directory: {ENSEMBLE_DIR}/\")\n",
    "print(f\"  - {model_name}_predictions.csv (predictions for ensemble)\")\n",
    "print(f\"  - {model_name}_metadata.json (model metadata)\")\n",
    "print(f\"  - {model_name}_submission.csv (standalone submission)\")\n",
    "print(f\"  - submission.csv (direct submission file)\")\n",
    "if is_training:\n",
    "    print(f\"  - {model_name}_detailed_analysis.csv (detailed results)\")\n",
    "\n",
    "# Display submission preview\n",
    "print(\"\\nSubmission preview:\")\n",
    "print(sub.head(10))\n",
    "\n",
    "# Advantages and Limitations\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SOLUTION ANALYSIS: ADVANTAGES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\"\"\n",
    "1. STRONG GENERALIZATION: The few-shot learning approach with examples helps the model\n",
    "   understand new rules not seen during training, which is critical for this competition.\n",
    "\n",
    "2. EFFICIENT INFERENCE: Using vLLM with tensor parallelism enables fast batch processing\n",
    "   across multiple GPUs, achieving ~8 predictions per second.\n",
    "\n",
    "3. ROBUST OUTPUT CONTROL: The logits processor ensures outputs are always valid \n",
    "   (Yes/No), preventing formatting errors in submissions.\n",
    "\n",
    "4. HIGH ACCURACY: The fine-tuned LLaMA 3.1 8B model achieves strong performance\n",
    "   (0.9565 AUC on training data), indicating good understanding of rule violations.\n",
    "\n",
    "5. PROBABILISTIC OUTPUTS: Extracting probabilities from logprobs provides calibrated\n",
    "   confidence scores, important for the AUC evaluation metric.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SOLUTION ANALYSIS: LIMITATIONS AND IMPROVEMENTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\"\"\n",
    "1. COMPUTATIONAL REQUIREMENTS: Requires 2 GPUs with significant VRAM (14.74GB each),\n",
    "   making it resource-intensive and potentially costly to deploy.\n",
    "\n",
    "2. PROMPT LENGTH CONSTRAINTS: The 2048 token limit may truncate very long comments\n",
    "   or examples, potentially losing important context.\n",
    "\n",
    "3. SINGLE MODEL DEPENDENCY: Relies on one model without ensemble benefits. Consider:\n",
    "   - Ensemble multiple model sizes or architectures\n",
    "   - Temperature sampling for uncertainty estimation\n",
    "   - Cross-validation for better generalization\n",
    "\n",
    "4. LIMITED ERROR HANDLING: Basic error recovery for failed inferences. Could improve:\n",
    "   - Retry logic for transient failures\n",
    "   - Fallback to smaller models if memory issues occur\n",
    "   - Better logging for debugging production issues\n",
    "\n",
    "5. POTENTIAL IMPROVEMENTS:\n",
    "   - Dynamic example selection based on comment similarity\n",
    "   - Rule-specific fine-tuning or adapters\n",
    "   - Active learning to identify uncertain predictions\n",
    "   - Post-processing to handle edge cases (empty comments, special characters)\n",
    "   - Experiment with different prompt templates and orderings\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXECUTION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcb4af40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T05:41:09.319297Z",
     "iopub.status.busy": "2025-07-28T05:41:09.319051Z",
     "iopub.status.idle": "2025-07-28T05:41:25.762047Z",
     "shell.execute_reply": "2025-07-28T05:41:25.761146Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 16.450543,
     "end_time": "2025-07-28T05:41:25.763288",
     "exception": false,
     "start_time": "2025-07-28T05:41:09.312745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENHANCED GPU MEMORY MANAGEMENT AND CLEANUP\n",
      "================================================================================\n",
      "Execution started at: 2025-07-28 05:41:09.353260\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE CLEANUP PROCEDURE\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "INITIAL MEMORY STATUS\n",
      "================================================================================\n",
      "\n",
      "CPU Memory:\n",
      "  Total: 31.35 GB\n",
      "  Used: 3.89 GB (40.1%)\n",
      "  Available: 18.77 GB\n",
      "\n",
      "GPU Memory:\n",
      "\n",
      "  GPU 0 (Tesla T4):\n",
      "    Total: 14.74 GB\n",
      "    Allocated: 12.72 GB\n",
      "    Reserved: 13.70 GB\n",
      "    Free: 1.04 GB\n",
      "\n",
      "  GPU 1 (Tesla T4):\n",
      "    Total: 14.74 GB\n",
      "    Allocated: 0.00 GB\n",
      "    Reserved: 0.00 GB\n",
      "    Free: 14.74 GB\n",
      "\n",
      "================================================================================\n",
      "DEEP DISTRIBUTED CLEANUP PROCEDURE\n",
      "================================================================================\n",
      "\n",
      "Step 1: Cleaning up distributed process groups...\n",
      "  ✓ Destroyed distributed process group\n",
      "\n",
      "Step 2: Terminating vLLM worker processes...\n",
      "  - No vLLM worker processes found\n",
      "\n",
      "Step 3: Cleaning up shared memory...\n",
      "\n",
      "Step 4: Cleaning up temporary communication files...\n",
      "  ✓ Cleaned 1 temporary files/directories\n",
      "\n",
      "Step 5: Resetting CUDA context...\n",
      "  ✓ Collected CUDA IPC memory\n",
      "  ✓ Reset GPU 0\n",
      "  ✓ Reset GPU 1\n",
      "\n",
      "Step 6: Clearing distributed environment variables...\n",
      "  ✓ Cleared CUDA_VISIBLE_DEVICES\n",
      "\n",
      "Step 7: Running comprehensive garbage collection...\n",
      "  ✓ Pass 1: Collected 2092 objects\n",
      "  ✓ Pass 2: Collected 0 objects\n",
      "  ✓ Pass 3: Collected 0 objects\n",
      "  ✓ Pass 4: Collected 0 objects\n",
      "  ✓ Pass 5: Collected 0 objects\n",
      "\n",
      "Waiting for process termination...\n",
      "\n",
      "================================================================================\n",
      "STANDARD MEMORY CLEANUP\n",
      "================================================================================\n",
      "\n",
      "Running Python garbage collection...\n",
      "  Collected 0 objects\n",
      "\n",
      "Clearing PyTorch CUDA cache...\n",
      "  ✓ Cleared cache for GPU 0\n",
      "  ✓ Cleared cache for GPU 1\n",
      "\n",
      "================================================================================\n",
      "FINAL MEMORY STATUS\n",
      "================================================================================\n",
      "\n",
      "CPU Memory:\n",
      "  Total: 31.35 GB\n",
      "  Used: 3.99 GB (40.4%)\n",
      "  Available: 18.69 GB\n",
      "\n",
      "GPU Memory:\n",
      "\n",
      "  GPU 0 (Tesla T4):\n",
      "    Total: 14.74 GB\n",
      "    Allocated: 12.72 GB\n",
      "    Reserved: 12.76 GB\n",
      "    Free: 1.99 GB\n",
      "\n",
      "  GPU 1 (Tesla T4):\n",
      "    Total: 14.74 GB\n",
      "    Allocated: 0.00 GB\n",
      "    Reserved: 0.00 GB\n",
      "    Free: 14.74 GB\n",
      "\n",
      "Total cleanup time: 11.40 seconds\n",
      "\n",
      "================================================================================\n",
      "CLEANUP VERIFICATION\n",
      "================================================================================\n",
      "✓ All vLLM processes successfully terminated\n",
      "✓ GPU 0: Freed 0.94 GB of memory\n",
      "✓ GPU 1: Freed 0.00 GB of memory\n",
      "\n",
      "Waiting for complete system stabilization...\n",
      "\n",
      "================================================================================\n",
      "PREPARING ENVIRONMENT FOR QWEN\n",
      "================================================================================\n",
      "\n",
      "Setting environment variables for single-node execution...\n",
      "✓ Environment configured for local execution\n",
      "\n",
      "Verifying clean environment...\n",
      "✓ Environment is clean and ready for QWEN\n",
      "\n",
      "================================================================================\n",
      "QWEN PREPARATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "FINAL SYSTEM STATUS\n",
      "================================================================================\n",
      "\n",
      "CPU Memory:\n",
      "  Total: 31.35 GB\n",
      "  Used: 3.97 GB (40.3%)\n",
      "  Available: 18.71 GB\n",
      "\n",
      "GPU Memory:\n",
      "\n",
      "  GPU 0 (Tesla T4):\n",
      "    Total: 14.74 GB\n",
      "    Allocated: 12.72 GB\n",
      "    Reserved: 12.76 GB\n",
      "    Free: 1.99 GB\n",
      "\n",
      "  GPU 1 (Tesla T4):\n",
      "    Total: 14.74 GB\n",
      "    Allocated: 0.00 GB\n",
      "    Reserved: 0.00 GB\n",
      "    Free: 14.74 GB\n",
      "\n",
      "================================================================================\n",
      "SYSTEM READY FOR QWEN MODEL EXECUTION\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enhanced GPU Memory Management and Deep Cleanup Script\n",
    "Ensures complete cleanup of distributed computing resources and memory between model executions\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import psutil\n",
    "import time\n",
    "import subprocess\n",
    "import signal\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def deep_cleanup_distributed():\n",
    "    \"\"\"Perform deep cleanup of all distributed computing resources\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DEEP DISTRIBUTED CLEANUP PROCEDURE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Destroy PyTorch distributed process groups\n",
    "    print(\"\\nStep 1: Cleaning up distributed process groups...\")\n",
    "    try:\n",
    "        if torch.distributed.is_initialized():\n",
    "            torch.distributed.destroy_process_group()\n",
    "            print(\"  ✓ Destroyed distributed process group\")\n",
    "        else:\n",
    "            print(\"  - No active distributed process group found\")\n",
    "    except Exception as e:\n",
    "        print(f\"  - Error destroying process group: {e}\")\n",
    "    \n",
    "    # Step 2: Kill all vLLM worker processes\n",
    "    print(\"\\nStep 2: Terminating vLLM worker processes...\")\n",
    "    terminated_count = 0\n",
    "    try:\n",
    "        # Find all vLLM related processes\n",
    "        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
    "            try:\n",
    "                cmdline = ' '.join(proc.info['cmdline'] or [])\n",
    "                if any(keyword in cmdline for keyword in ['VllmWorkerProcess', 'vllm', 'ray::RayWorker']):\n",
    "                    proc.terminate()\n",
    "                    terminated_count += 1\n",
    "                    print(f\"  ✓ Terminated process {proc.info['pid']}: {proc.info['name']}\")\n",
    "            except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "                continue\n",
    "        \n",
    "        # Give processes time to terminate gracefully\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Force kill any remaining processes\n",
    "        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
    "            try:\n",
    "                cmdline = ' '.join(proc.info['cmdline'] or [])\n",
    "                if any(keyword in cmdline for keyword in ['VllmWorkerProcess', 'vllm', 'ray::RayWorker']):\n",
    "                    proc.kill()\n",
    "                    print(f\"  ✓ Force killed process {proc.info['pid']}\")\n",
    "            except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "                continue\n",
    "                \n",
    "        if terminated_count == 0:\n",
    "            print(\"  - No vLLM worker processes found\")\n",
    "    except Exception as e:\n",
    "        print(f\"  - Error terminating processes: {e}\")\n",
    "    \n",
    "    # Step 3: Clean up shared memory segments\n",
    "    print(\"\\nStep 3: Cleaning up shared memory...\")\n",
    "    try:\n",
    "        # Clean /dev/shm\n",
    "        shm_path = '/dev/shm'\n",
    "        if os.path.exists(shm_path):\n",
    "            for item in os.listdir(shm_path):\n",
    "                if any(pattern in item for pattern in ['torch', 'nccl', 'cuda', 'vllm']):\n",
    "                    item_path = os.path.join(shm_path, item)\n",
    "                    try:\n",
    "                        if os.path.isfile(item_path):\n",
    "                            os.unlink(item_path)\n",
    "                        elif os.path.isdir(item_path):\n",
    "                            shutil.rmtree(item_path)\n",
    "                        print(f\"  ✓ Removed shared memory: {item}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  - Could not remove {item}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  - Error cleaning shared memory: {e}\")\n",
    "    \n",
    "    # Step 4: Clean up temporary communication files\n",
    "    print(\"\\nStep 4: Cleaning up temporary communication files...\")\n",
    "    try:\n",
    "        # Clean /tmp directory\n",
    "        tmp_patterns = ['vllm', 'nccl', 'torch', 'ray', 'cuda']\n",
    "        cleaned_count = 0\n",
    "        for item in os.listdir('/tmp'):\n",
    "            if any(pattern in item for pattern in tmp_patterns):\n",
    "                item_path = os.path.join('/tmp', item)\n",
    "                try:\n",
    "                    if os.path.isfile(item_path):\n",
    "                        os.unlink(item_path)\n",
    "                    elif os.path.isdir(item_path):\n",
    "                        shutil.rmtree(item_path)\n",
    "                    cleaned_count += 1\n",
    "                except Exception:\n",
    "                    continue\n",
    "        print(f\"  ✓ Cleaned {cleaned_count} temporary files/directories\")\n",
    "    except Exception as e:\n",
    "        print(f\"  - Error cleaning temporary files: {e}\")\n",
    "    \n",
    "    # Step 5: Reset CUDA context\n",
    "    print(\"\\nStep 5: Resetting CUDA context...\")\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            # Clear CUDA IPC memory\n",
    "            torch.cuda.ipc_collect()\n",
    "            print(\"  ✓ Collected CUDA IPC memory\")\n",
    "            \n",
    "            # Reset all GPUs\n",
    "            for gpu_id in range(torch.cuda.device_count()):\n",
    "                torch.cuda.set_device(gpu_id)\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "                torch.cuda.reset_peak_memory_stats(gpu_id)\n",
    "                torch.cuda.reset_accumulated_memory_stats(gpu_id)\n",
    "                print(f\"  ✓ Reset GPU {gpu_id}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  - Error resetting CUDA: {e}\")\n",
    "    \n",
    "    # Step 6: Clear environment variables\n",
    "    print(\"\\nStep 6: Clearing distributed environment variables...\")\n",
    "    env_vars_to_clear = [\n",
    "        'MASTER_ADDR', 'MASTER_PORT', 'RANK', 'WORLD_SIZE', 'LOCAL_RANK',\n",
    "        'NCCL_SOCKET_IFNAME', 'NCCL_IB_DISABLE', 'NCCL_SOCKET_NTHREADS',\n",
    "        'NCCL_NSOCKS_PERTHREAD', 'CUDA_VISIBLE_DEVICES'\n",
    "    ]\n",
    "    for var in env_vars_to_clear:\n",
    "        if var in os.environ:\n",
    "            del os.environ[var]\n",
    "            print(f\"  ✓ Cleared {var}\")\n",
    "    \n",
    "    # Step 7: Force comprehensive garbage collection\n",
    "    print(\"\\nStep 7: Running comprehensive garbage collection...\")\n",
    "    for i in range(5):\n",
    "        collected = gc.collect()\n",
    "        print(f\"  ✓ Pass {i+1}: Collected {collected} objects\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "def comprehensive_cleanup(verbose=True):\n",
    "    \"\"\"Main cleanup function that combines memory cleanup with deep distributed cleanup\"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"COMPREHENSIVE CLEANUP PROCEDURE\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Get initial memory status\n",
    "    initial_memory = get_memory_info()\n",
    "    if verbose:\n",
    "        print_memory_status(\"Initial Memory Status\")\n",
    "    \n",
    "    # Perform deep distributed cleanup first\n",
    "    deep_cleanup_distributed()\n",
    "    \n",
    "    # Wait for processes to fully terminate\n",
    "    print(\"\\nWaiting for process termination...\")\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Standard memory cleanup\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STANDARD MEMORY CLEANUP\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Clear Python garbage collection\n",
    "    print(\"\\nRunning Python garbage collection...\")\n",
    "    collected = gc.collect()\n",
    "    print(f\"  Collected {collected} objects\")\n",
    "    \n",
    "    # Clear PyTorch cache\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\nClearing PyTorch CUDA cache...\")\n",
    "        for gpu_id in range(torch.cuda.device_count()):\n",
    "            torch.cuda.set_device(gpu_id)\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            print(f\"  ✓ Cleared cache for GPU {gpu_id}\")\n",
    "    \n",
    "    # Final memory status\n",
    "    final_memory = get_memory_info()\n",
    "    if verbose:\n",
    "        print_memory_status(\"Final Memory Status\")\n",
    "    \n",
    "    # Calculate cleanup duration\n",
    "    cleanup_duration = datetime.now() - start_time\n",
    "    print(f\"\\nTotal cleanup time: {cleanup_duration.total_seconds():.2f} seconds\")\n",
    "    \n",
    "    # Verify cleanup effectiveness\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CLEANUP VERIFICATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Check for remaining processes\n",
    "    remaining_processes = []\n",
    "    for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
    "        try:\n",
    "            cmdline = ' '.join(proc.info['cmdline'] or [])\n",
    "            if any(keyword in cmdline for keyword in ['VllmWorkerProcess', 'vllm', 'ray']):\n",
    "                remaining_processes.append(proc.info)\n",
    "        except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "            continue\n",
    "    \n",
    "    if remaining_processes:\n",
    "        print(f\"⚠ Warning: {len(remaining_processes)} vLLM-related processes still running\")\n",
    "        for proc in remaining_processes[:5]:  # Show first 5\n",
    "            print(f\"  - PID {proc['pid']}: {proc['name']}\")\n",
    "    else:\n",
    "        print(\"✓ All vLLM processes successfully terminated\")\n",
    "    \n",
    "    # Check memory freed\n",
    "    if initial_memory['gpu'] and final_memory['gpu']:\n",
    "        for i in range(len(initial_memory['gpu'])):\n",
    "            initial_free = initial_memory['gpu'][i]['free_memory_gb']\n",
    "            final_free = final_memory['gpu'][i]['free_memory_gb']\n",
    "            freed = final_free - initial_free\n",
    "            print(f\"✓ GPU {i}: Freed {freed:.2f} GB of memory\")\n",
    "    \n",
    "    return cleanup_duration\n",
    "\n",
    "def prepare_for_qwen():\n",
    "    \"\"\"Prepare environment specifically for QWEN model initialization\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PREPARING ENVIRONMENT FOR QWEN\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Set environment variables to prevent distributed initialization\n",
    "    print(\"\\nSetting environment variables for single-node execution...\")\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    os.environ['RANK'] = '0'\n",
    "    os.environ['WORLD_SIZE'] = '1'\n",
    "    os.environ['LOCAL_RANK'] = '0'\n",
    "    \n",
    "    # Force NCCL to use local communication\n",
    "    os.environ['NCCL_SOCKET_IFNAME'] = 'lo'\n",
    "    os.environ['NCCL_IB_DISABLE'] = '1'\n",
    "    os.environ['NCCL_P2P_DISABLE'] = '1'\n",
    "    \n",
    "    # Disable hostname lookup\n",
    "    os.environ['NCCL_SOCKET_NTHREADS'] = '2'\n",
    "    os.environ['NCCL_NSOCKS_PERTHREAD'] = '4'\n",
    "    \n",
    "    # Set CUDA devices explicitly\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "    \n",
    "    print(\"✓ Environment configured for local execution\")\n",
    "    \n",
    "    # Verify no distributed processes are running\n",
    "    print(\"\\nVerifying clean environment...\")\n",
    "    issues = []\n",
    "    \n",
    "    # Check for any remaining vLLM processes\n",
    "    for proc in psutil.process_iter(['name', 'cmdline']):\n",
    "        try:\n",
    "            cmdline = ' '.join(proc.info['cmdline'] or [])\n",
    "            if 'vllm' in cmdline.lower():\n",
    "                issues.append(f\"vLLM process still running: PID {proc.pid}\")\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Check shared memory\n",
    "    if os.path.exists('/dev/shm'):\n",
    "        shm_files = [f for f in os.listdir('/dev/shm') if any(p in f for p in ['torch', 'nccl', 'vllm'])]\n",
    "        if shm_files:\n",
    "            issues.append(f\"Shared memory files still exist: {shm_files[:3]}\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"⚠ Warning: Found potential issues:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  - {issue}\")\n",
    "    else:\n",
    "        print(\"✓ Environment is clean and ready for QWEN\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"QWEN PREPARATION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Helper functions from original script\n",
    "def get_memory_info():\n",
    "    \"\"\"Retrieve comprehensive memory information for both CPU and GPU\"\"\"\n",
    "    memory_info = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'cpu': {},\n",
    "        'gpu': []\n",
    "    }\n",
    "    \n",
    "    cpu_memory = psutil.virtual_memory()\n",
    "    memory_info['cpu'] = {\n",
    "        'total_gb': cpu_memory.total / (1024**3),\n",
    "        'available_gb': cpu_memory.available / (1024**3),\n",
    "        'used_gb': cpu_memory.used / (1024**3),\n",
    "        'percent_used': cpu_memory.percent\n",
    "    }\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        for gpu_id in range(torch.cuda.device_count()):\n",
    "            gpu_props = torch.cuda.get_device_properties(gpu_id)\n",
    "            memory_info['gpu'].append({\n",
    "                'id': gpu_id,\n",
    "                'name': gpu_props.name,\n",
    "                'total_memory_gb': gpu_props.total_memory / (1024**3),\n",
    "                'allocated_memory_gb': torch.cuda.memory_allocated(gpu_id) / (1024**3),\n",
    "                'reserved_memory_gb': torch.cuda.memory_reserved(gpu_id) / (1024**3),\n",
    "                'free_memory_gb': (gpu_props.total_memory - torch.cuda.memory_reserved(gpu_id)) / (1024**3)\n",
    "            })\n",
    "    \n",
    "    return memory_info\n",
    "\n",
    "def print_memory_status(title=\"Memory Status\"):\n",
    "    \"\"\"Display current memory status in a formatted manner\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(title.upper())\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    memory_info = get_memory_info()\n",
    "    \n",
    "    cpu_info = memory_info['cpu']\n",
    "    print(\"\\nCPU Memory:\")\n",
    "    print(f\"  Total: {cpu_info['total_gb']:.2f} GB\")\n",
    "    print(f\"  Used: {cpu_info['used_gb']:.2f} GB ({cpu_info['percent_used']:.1f}%)\")\n",
    "    print(f\"  Available: {cpu_info['available_gb']:.2f} GB\")\n",
    "    \n",
    "    if memory_info['gpu']:\n",
    "        print(\"\\nGPU Memory:\")\n",
    "        for gpu in memory_info['gpu']:\n",
    "            print(f\"\\n  GPU {gpu['id']} ({gpu['name']}):\")\n",
    "            print(f\"    Total: {gpu['total_memory_gb']:.2f} GB\")\n",
    "            print(f\"    Allocated: {gpu['allocated_memory_gb']:.2f} GB\")\n",
    "            print(f\"    Reserved: {gpu['reserved_memory_gb']:.2f} GB\")\n",
    "            print(f\"    Free: {gpu['free_memory_gb']:.2f} GB\")\n",
    "\n",
    "# Main execution function\n",
    "def main():\n",
    "    \"\"\"Execute complete cleanup and prepare for QWEN\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"ENHANCED GPU MEMORY MANAGEMENT AND CLEANUP\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Execution started at: {datetime.now()}\")\n",
    "    \n",
    "    # Perform comprehensive cleanup\n",
    "    comprehensive_cleanup(verbose=True)\n",
    "    \n",
    "    # Additional wait to ensure complete termination\n",
    "    print(\"\\nWaiting for complete system stabilization...\")\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Prepare environment for QWEN\n",
    "    prepare_for_qwen()\n",
    "    \n",
    "    # Final memory check\n",
    "    print_memory_status(\"Final System Status\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SYSTEM READY FOR QWEN MODEL EXECUTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return get_memory_info()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    final_memory_info = main()\n",
    "    with open('deep_cleanup_log.json', 'w') as f:\n",
    "        json.dump(final_memory_info, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f583d12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T05:41:25.776839Z",
     "iopub.status.busy": "2025-07-28T05:41:25.776546Z",
     "iopub.status.idle": "2025-07-28T05:43:56.690420Z",
     "shell.execute_reply": "2025-07-28T05:43:56.689608Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 150.922457,
     "end_time": "2025-07-28T05:43:56.691817",
     "exception": false,
     "start_time": "2025-07-28T05:41:25.769360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QWEN 7B MODEL - JIGSAW COMPETITION (FIXED)\n",
      "================================================================================\n",
      "\n",
      "Execution started at: 2025-07-28 05:41:25.830734\n",
      "\n",
      "==================================================\n",
      "DATA LOADING\n",
      "==================================================\n",
      "Environment: Development\n",
      "Dataset shape: (2029, 9)\n",
      "Columns: ['row_id', 'body', 'rule', 'subreddit', 'positive_example_1', 'positive_example_2', 'negative_example_1', 'negative_example_2', 'rule_violation']\n",
      "\n",
      "==================================================\n",
      "ENVIRONMENT CONFIGURATION\n",
      "==================================================\n",
      "  Set CUDA_VISIBLE_DEVICES=0,1\n",
      "  Set OMP_NUM_THREADS=1\n",
      "  Set MKL_NUM_THREADS=1\n",
      "  Set TOKENIZERS_PARALLELISM=false\n",
      "  Set VLLM_WORKER_MULTIPROC_METHOD=spawn\n",
      "  Set VLLM_ATTENTION_BACKEND=XFORMERS\n",
      "  Removed MASTER_ADDR\n",
      "  Removed MASTER_PORT\n",
      "  Removed RANK\n",
      "  Removed WORLD_SIZE\n",
      "  Removed LOCAL_RANK\n",
      "  Removed NCCL_SOCKET_IFNAME\n",
      "  Removed NCCL_IB_DISABLE\n",
      "  Removed NCCL_P2P_DISABLE\n",
      "\n",
      "✓ Environment configured for single-node execution\n",
      "\n",
      "==================================================\n",
      "SYSTEM RESOURCE CHECK\n",
      "==================================================\n",
      "✓ CPU Memory: 18.7 GB available\n",
      "✓ Found 2 GPU(s)\n",
      "\n",
      "==================================================\n",
      "GPU CONFIGURATION ANALYSIS\n",
      "==================================================\n",
      "\n",
      "Current GPU Status:\n",
      "--------------------------------------------------------------------------------\n",
      " GPU                 Name    Total     Free     Used  Util%\n",
      "--------------------------------------------------------------------------------\n",
      "   0             Tesla T4     14.7      2.0     12.7   86.5\n",
      "   1             Tesla T4     14.7     14.7      0.0    0.0\n",
      "\n",
      "Model Memory Requirements:\n",
      "  Base model size: 13.0 GB\n",
      "  Required (with overhead): 14.9 GB\n",
      "\n",
      "Performing aggressive GPU cleanup...\n",
      "\n",
      "Resetting GPU 0...\n",
      "\n",
      "Resetting GPU 1...\n",
      "\n",
      "GPU Status After Cleanup:\n",
      "  GPU 1: 14.7 GB free\n",
      "  GPU 0: 2.0 GB free\n",
      "\n",
      "✓ Multi-GPU Configuration:\n",
      "  Combined memory: 16.7 GB\n",
      "  Will use tensor parallelism across GPUs 0,1\n",
      "\n",
      "✓ All system checks passed\n",
      "\n",
      "==================================================\n",
      "DEPENDENCY INSTALLATION\n",
      "==================================================\n",
      "✓ logits_processor_zoo already installed\n",
      "\n",
      "==================================================\n",
      "MODEL INITIALIZATION\n",
      "==================================================\n",
      "\n",
      "Attempt 1/3\n",
      "Initializing with:\n",
      "  - GPU(s): 0,1\n",
      "  - Tensor parallel size: 2\n",
      "  - GPU utilization: 0.90\n",
      "  - Max model length: 2048\n",
      "  - Multi-GPU distribution: ACTIVE\n",
      "INFO 07-28 05:41:51 [config.py:1604] Using max model len 2048\n",
      "WARNING 07-28 05:41:51 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-28 05:41:51 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='/kaggle/input/jigsaw-acrc-qwen7b-v01', speculative_config=None, tokenizer='/kaggle/input/jigsaw-acrc-qwen7b-v01', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=/kaggle/input/jigsaw-acrc-qwen7b-v01, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 05:41:56.633407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753681316.654516     319 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753681316.660903     319 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-28 05:42:02 [__init__.py:235] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m INFO 07-28 05:42:02 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m INFO 07-28 05:42:03 [cuda.py:326] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W728 05:42:14.504873230 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W728 05:42:14.993526194 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W728 05:42:24.513733327 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W728 05:42:34.524280357 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ Initialization failed: Invariant encountered: value was None when it should not be\n",
      "Waiting 5 seconds before retry...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239] Exception in worker VllmWorkerProcess while processing method init_device.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239] Traceback (most recent call last):\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 233, in _run_worker_process\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]     output = run_method(worker, method, args, kwargs)\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/vllm/utils/__init__.py\", line 2985, in run_method\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]     return func(*args, **kwargs)\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]            ^^^^^^^^^^^^^^^^^^^^^\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/worker_base.py\", line 603, in init_device\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]     self.worker.init_device()  # type: ignore\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]     ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/worker.py\", line 194, in init_device\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]     init_worker_distributed_environment(self.vllm_config, self.rank,\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/worker.py\", line 541, in init_worker_distributed_environment\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]     init_distributed_environment(parallel_config.world_size, rank,\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/vllm/distributed/parallel_state.py\", line 994, in init_distributed_environment\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]     _WORLD = init_world_group(ranks, local_rank, backend)\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/vllm/distributed/parallel_state.py\", line 841, in init_world_group\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]     return GroupCoordinator(\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]            ^^^^^^^^^^^^^^^^^\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/vllm/distributed/parallel_state.py\", line 228, in __init__\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]     cpu_group = torch.distributed.new_group(ranks, backend=\"gloo\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]     func_return = func(*args, **kwargs)\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]                   ^^^^^^^^^^^^^^^^^^^^^\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 5065, in new_group\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]     return _new_group_with_tag(\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]            ^^^^^^^^^^^^^^^^^^^^\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 5155, in _new_group_with_tag\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]     pg, pg_store = _new_process_group_helper(\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1942, in _new_process_group_helper\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]     backend_class = ProcessGroupGloo(\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239]                     ^^^^^^^^^^^^^^^^^\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m ERROR 07-28 05:42:34 [multiproc_worker_utils.py:239] torch.distributed.DistNetworkError: failed to recv, got 0 bytes\n",
      "INFO 07-28 05:42:39 [multiproc_worker_utils.py:138] Terminating local vLLM worker processes\n",
      "\n",
      "Attempt 2/3\u001b[1;36m(VllmWorkerProcess pid=319)\u001b[0;0m INFO 07-28 05:42:39 [multiproc_worker_utils.py:260] Worker exiting\n",
      "\n",
      "Initializing with:\n",
      "  - GPU(s): 0,1\n",
      "  - Tensor parallel size: 2\n",
      "  - GPU utilization: 0.85\n",
      "  - Max model length: 1536\n",
      "  - Multi-GPU distribution: ACTIVE\n",
      "INFO 07-28 05:42:39 [config.py:1604] Using max model len 1536\n",
      "WARNING 07-28 05:42:39 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-28 05:42:39 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='/kaggle/input/jigsaw-acrc-qwen7b-v01', speculative_config=None, tokenizer='/kaggle/input/jigsaw-acrc-qwen7b-v01', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=1536, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=/kaggle/input/jigsaw-acrc-qwen7b-v01, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 05:42:44.757634: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753681364.779634     338 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753681364.785936     338 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "[rank1]:[W728 05:42:45.537392266 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[rank1]:[W728 05:42:35.527377954 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=48, addr=[fdff:ffff::fe:2204:f125:a757]:46717, remote=[fdff:ffff:0:0:5f00::]:15412): failed to recv, got 0 bytes\n",
      "Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7c898b1785e8 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8bfe (0x7c89744cdbfe in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baaf40 (0x7c89744cff40 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5bab84a (0x7c89744d084a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7c89744ca2a9 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7c8935bc99f9 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdc253 (0x7c89259d8253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x94ac3 (0x7c898c8fdac3 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #8: clone + 0x44 (0x7c898c98ea04 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[rank1]:[W728 05:42:45.541128274 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0 Rank 1] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-28 05:42:50 [__init__.py:235] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m INFO 07-28 05:42:50 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m INFO 07-28 05:42:51 [cuda.py:326] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W728 05:43:02.784833009 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W728 05:43:02.218842458 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W728 05:43:12.790541085 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W728 05:43:22.799353063 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ Initialization failed: Invariant encountered: value was None when it should not be\n",
      "Waiting 10 seconds before retry...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239] Exception in worker VllmWorkerProcess while processing method init_device.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239] Traceback (most recent call last):\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 233, in _run_worker_process\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]     output = run_method(worker, method, args, kwargs)\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/vllm/utils/__init__.py\", line 2985, in run_method\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]     return func(*args, **kwargs)\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]            ^^^^^^^^^^^^^^^^^^^^^\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/worker_base.py\", line 603, in init_device\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]     self.worker.init_device()  # type: ignore\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]     ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/worker.py\", line 194, in init_device\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]     init_worker_distributed_environment(self.vllm_config, self.rank,\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/worker.py\", line 541, in init_worker_distributed_environment\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]     init_distributed_environment(parallel_config.world_size, rank,\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/vllm/distributed/parallel_state.py\", line 994, in init_distributed_environment\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]     _WORLD = init_world_group(ranks, local_rank, backend)\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/vllm/distributed/parallel_state.py\", line 841, in init_world_group\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]     return GroupCoordinator(\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]            ^^^^^^^^^^^^^^^^^\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/vllm/distributed/parallel_state.py\", line 228, in __init__\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]     cpu_group = torch.distributed.new_group(ranks, backend=\"gloo\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]     func_return = func(*args, **kwargs)\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]                   ^^^^^^^^^^^^^^^^^^^^^\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 5065, in new_group\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]     return _new_group_with_tag(\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]            ^^^^^^^^^^^^^^^^^^^^\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 5155, in _new_group_with_tag\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]     pg, pg_store = _new_process_group_helper(\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]   File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1942, in _new_process_group_helper\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]     backend_class = ProcessGroupGloo(\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239]                     ^^^^^^^^^^^^^^^^^\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m ERROR 07-28 05:43:23 [multiproc_worker_utils.py:239] torch.distributed.DistNetworkError: failed to recv, got 0 bytes\n",
      "Switching to single GPU for final attempt...\n",
      "INFO 07-28 05:43:33 [multiproc_worker_utils.py:138] Terminating local vLLM worker processes\n",
      "\n",
      "Attempt 3/3\n",
      "Initializing with:\n",
      "  - GPU(s): 1\n",
      "  - Tensor parallel size: 1\n",
      "  - GPU utilization: 0.80\n",
      "  - Max model length: 1024\n",
      "\u001b[1;36m(VllmWorkerProcess pid=338)\u001b[0;0m INFO 07-28 05:43:33 [multiproc_worker_utils.py:260] Worker exiting\n",
      "INFO 07-28 05:43:33 [config.py:1604] Using max model len 1024\n",
      "WARNING 07-28 05:43:33 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-28 05:43:33 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='/kaggle/input/jigsaw-acrc-qwen7b-v01', speculative_config=None, tokenizer='/kaggle/input/jigsaw-acrc-qwen7b-v01', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=/kaggle/input/jigsaw-acrc-qwen7b-v01, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank1]:[W728 05:43:33.810095118 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[rank1]:[W728 05:43:23.800675843 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=48, addr=[fdff:ffff::55:2ed7:a113:a937]:49473, remote=[fdff:ffff:0:0:5f00::]:12286): failed to recv, got 0 bytes\n",
      "Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7b9a913785e8 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8bfe (0x7b9a7aacdbfe in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baaf40 (0x7b9a7aacff40 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5bab84a (0x7b9a7aad084a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7b9a7aaca2a9 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7b9a3c1c99f9 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdc253 (0x7b9a2bfd8253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x94ac3 (0x7b9a92d3dac3 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #8: clone + 0x44 (0x7b9a92dcea04 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[rank1]:[W728 05:43:33.810221940 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0 Rank 1] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n",
      "[W728 05:43:44.749970567 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W728 05:43:54.760459558 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ Initialization failed: world group already initialized with a different world size\n",
      "✗ All initialization attempts failed\n",
      "✗ Failed to initialize model\n",
      "\n",
      "================================================================================\n",
      "EXECUTION FAILED\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "CLEANUP\n",
      "==================================================\n",
      "\n",
      "Resetting GPU 0...\n",
      "\n",
      "Resetting GPU 1...\n",
      "✓ Resources cleaned up\n",
      "\n",
      "Total runtime: 0:02:30.804686\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enhanced Qwen 7B Model for Jigsaw Agile Community Rules Competition\n",
    "Fixed distributed computing issues and improved GPU memory management\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import psutil\n",
    "import signal\n",
    "from typing import List, Dict, Optional, Any, Tuple\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Kill any existing distributed processes\n",
    "def kill_distributed_processes():\n",
    "    \"\"\"Kill any existing PyTorch distributed processes\"\"\"\n",
    "    try:\n",
    "        if torch.distributed.is_initialized():\n",
    "            torch.distributed.destroy_process_group()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "class GPUMemoryManager:\n",
    "    \"\"\"Advanced GPU memory management utilities\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_gpu_memory_info() -> List[Dict[str, float]]:\n",
    "        \"\"\"Get detailed memory information for all GPUs\"\"\"\n",
    "        gpu_info = []\n",
    "        \n",
    "        if not torch.cuda.is_available():\n",
    "            return gpu_info\n",
    "            \n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            torch.cuda.set_device(i)\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            \n",
    "            # Get memory stats in GB\n",
    "            total_memory = props.total_memory / (1024**3)\n",
    "            reserved_memory = torch.cuda.memory_reserved(i) / (1024**3)\n",
    "            allocated_memory = torch.cuda.memory_allocated(i) / (1024**3)\n",
    "            free_memory = total_memory - reserved_memory\n",
    "            \n",
    "            gpu_info.append({\n",
    "                'id': i,\n",
    "                'name': props.name,\n",
    "                'total_memory_gb': total_memory,\n",
    "                'reserved_memory_gb': reserved_memory,\n",
    "                'allocated_memory_gb': allocated_memory,\n",
    "                'free_memory_gb': free_memory,\n",
    "                'utilization_percent': (reserved_memory / total_memory) * 100\n",
    "            })\n",
    "            \n",
    "        return gpu_info\n",
    "    \n",
    "    @staticmethod\n",
    "    def reset_gpu(gpu_id: int, aggressive: bool = False) -> bool:\n",
    "        \"\"\"Reset a specific GPU to free memory\"\"\"\n",
    "        print(f\"\\nResetting GPU {gpu_id}...\")\n",
    "        \n",
    "        try:\n",
    "            torch.cuda.set_device(gpu_id)\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize(gpu_id)\n",
    "            gc.collect()\n",
    "            \n",
    "            if aggressive:\n",
    "                # Additional aggressive cleanup\n",
    "                torch.cuda.reset_peak_memory_stats(gpu_id)\n",
    "                torch.cuda.reset_accumulated_memory_stats(gpu_id)\n",
    "                \n",
    "                for _ in range(3):\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    time.sleep(0.5)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to reset GPU {gpu_id}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def reset_all_gpus(aggressive: bool = False) -> None:\n",
    "        \"\"\"Reset all available GPUs\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "            \n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            GPUMemoryManager.reset_gpu(i, aggressive)\n",
    "\n",
    "class QwenModelRunner:\n",
    "    \"\"\"QWEN model runner with fixed distributed computing and GPU management\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, ensemble_dir: str = 'ensemble_files'):\n",
    "        self.model_path = model_path\n",
    "        self.ensemble_dir = ensemble_dir\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.start_time = datetime.now()\n",
    "        self.gpu_manager = GPUMemoryManager()\n",
    "        \n",
    "        # Adjusted model size for QWEN 7B (more realistic)\n",
    "        self.model_size_gb = 13.0  # QWEN 7B is typically around 13GB\n",
    "        \n",
    "        # Initialize GPU configuration attributes\n",
    "        self.gpu_config = None\n",
    "        self.num_gpus = None\n",
    "        self.requires_multi_gpu = False\n",
    "        \n",
    "        # Kill any existing distributed processes\n",
    "        kill_distributed_processes()\n",
    "        \n",
    "        # Create ensemble directory\n",
    "        os.makedirs(self.ensemble_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize environment\n",
    "        self._setup_environment()\n",
    "        \n",
    "    def _setup_environment(self):\n",
    "        \"\"\"Configure environment to prevent distributed computing issues\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ENVIRONMENT CONFIGURATION\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Clean up any existing distributed state\n",
    "        kill_distributed_processes()\n",
    "        \n",
    "        # Force single-node execution with fixed parameters\n",
    "        env_vars = {\n",
    "            'CUDA_VISIBLE_DEVICES': '0,1',\n",
    "            'OMP_NUM_THREADS': '1',\n",
    "            'MKL_NUM_THREADS': '1',\n",
    "            'TOKENIZERS_PARALLELISM': 'false',\n",
    "            'VLLM_WORKER_MULTIPROC_METHOD': 'spawn',\n",
    "            'VLLM_ATTENTION_BACKEND': 'XFORMERS'  # More stable than FLASH_ATTN\n",
    "        }\n",
    "        \n",
    "        for key, value in env_vars.items():\n",
    "            os.environ[key] = value\n",
    "            print(f\"  Set {key}={value}\")\n",
    "        \n",
    "        # Remove distributed-related environment variables\n",
    "        distributed_vars = ['MASTER_ADDR', 'MASTER_PORT', 'RANK', 'WORLD_SIZE', \n",
    "                          'LOCAL_RANK', 'NCCL_SOCKET_IFNAME', 'NCCL_IB_DISABLE',\n",
    "                          'NCCL_P2P_DISABLE', 'NCCL_TREE_THRESHOLD']\n",
    "        \n",
    "        for var in distributed_vars:\n",
    "            if var in os.environ:\n",
    "                del os.environ[var]\n",
    "                print(f\"  Removed {var}\")\n",
    "        \n",
    "        print(\"\\n✓ Environment configured for single-node execution\")\n",
    "    \n",
    "    def _analyze_gpu_configuration(self) -> Tuple[Optional[str], Optional[int], bool]:\n",
    "        \"\"\"Analyze and determine optimal GPU configuration\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"GPU CONFIGURATION ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        gpu_info = self.gpu_manager.get_gpu_memory_info()\n",
    "        \n",
    "        if not gpu_info:\n",
    "            return None, None, False\n",
    "        \n",
    "        # Display current GPU status\n",
    "        print(\"\\nCurrent GPU Status:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'GPU':>4} {'Name':>20} {'Total':>8} {'Free':>8} {'Used':>8} {'Util%':>6}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for gpu in gpu_info:\n",
    "            print(f\"{gpu['id']:>4} {gpu['name']:>20} {gpu['total_memory_gb']:>8.1f} \"\n",
    "                  f\"{gpu['free_memory_gb']:>8.1f} {gpu['allocated_memory_gb']:>8.1f} \"\n",
    "                  f\"{gpu['utilization_percent']:>6.1f}\")\n",
    "        \n",
    "        # Sort GPUs by free memory\n",
    "        gpu_info.sort(key=lambda x: x['free_memory_gb'], reverse=True)\n",
    "        \n",
    "        # Estimate memory requirement with overhead\n",
    "        required_memory = self.model_size_gb * 1.15  # Reduced overhead\n",
    "        \n",
    "        print(f\"\\nModel Memory Requirements:\")\n",
    "        print(f\"  Base model size: {self.model_size_gb:.1f} GB\")\n",
    "        print(f\"  Required (with overhead): {required_memory:.1f} GB\")\n",
    "        \n",
    "        # First, try aggressive GPU reset to free memory\n",
    "        print(\"\\nPerforming aggressive GPU cleanup...\")\n",
    "        self.gpu_manager.reset_all_gpus(aggressive=True)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Re-check after cleanup\n",
    "        gpu_info = self.gpu_manager.get_gpu_memory_info()\n",
    "        gpu_info.sort(key=lambda x: x['free_memory_gb'], reverse=True)\n",
    "        \n",
    "        print(\"\\nGPU Status After Cleanup:\")\n",
    "        for gpu in gpu_info:\n",
    "            print(f\"  GPU {gpu['id']}: {gpu['free_memory_gb']:.1f} GB free\")\n",
    "        \n",
    "        # Determine configuration\n",
    "        if gpu_info[0]['free_memory_gb'] >= required_memory:\n",
    "            selected_gpu = str(gpu_info[0]['id'])\n",
    "            num_gpus = 1\n",
    "            multi_gpu = False\n",
    "            print(f\"\\n✓ Single GPU Configuration: GPU {selected_gpu} ({gpu_info[0]['free_memory_gb']:.1f} GB free)\")\n",
    "            return selected_gpu, num_gpus, multi_gpu\n",
    "            \n",
    "        elif len(gpu_info) > 1:\n",
    "            # Check if we can use multi-GPU\n",
    "            combined_memory = sum(g['free_memory_gb'] for g in gpu_info[:2])\n",
    "            if combined_memory >= required_memory:\n",
    "                selected_gpu = '0,1'\n",
    "                num_gpus = 2\n",
    "                multi_gpu = True\n",
    "                print(f\"\\n✓ Multi-GPU Configuration:\")\n",
    "                print(f\"  Combined memory: {combined_memory:.1f} GB\")\n",
    "                print(f\"  Will use tensor parallelism across GPUs 0,1\")\n",
    "                return selected_gpu, num_gpus, multi_gpu\n",
    "        \n",
    "        print(f\"\\n✗ Insufficient GPU memory. Required: {required_memory:.1f} GB\")\n",
    "        return None, None, False\n",
    "    \n",
    "    def _check_system_resources(self) -> bool:\n",
    "        \"\"\"Verify system has sufficient resources\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"SYSTEM RESOURCE CHECK\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        issues = []\n",
    "        \n",
    "        # Check CPU memory\n",
    "        cpu_memory = psutil.virtual_memory()\n",
    "        available_gb = cpu_memory.available / (1024**3)\n",
    "        if available_gb < 10:\n",
    "            issues.append(f\"Low CPU memory: {available_gb:.1f} GB available\")\n",
    "        else:\n",
    "            print(f\"✓ CPU Memory: {available_gb:.1f} GB available\")\n",
    "        \n",
    "        # Check GPU availability\n",
    "        if not torch.cuda.is_available():\n",
    "            issues.append(\"No CUDA devices available\")\n",
    "            self.gpu_config = None\n",
    "            self.num_gpus = 0\n",
    "            self.requires_multi_gpu = False\n",
    "        else:\n",
    "            gpu_count = torch.cuda.device_count()\n",
    "            print(f\"✓ Found {gpu_count} GPU(s)\")\n",
    "            \n",
    "            # Analyze GPU configuration\n",
    "            gpu_config, num_gpus, multi_gpu = self._analyze_gpu_configuration()\n",
    "            \n",
    "            if gpu_config is None:\n",
    "                issues.append(\"Insufficient GPU memory for model\")\n",
    "                self.gpu_config = None\n",
    "                self.num_gpus = 0\n",
    "                self.requires_multi_gpu = False\n",
    "            else:\n",
    "                self.gpu_config = gpu_config\n",
    "                self.num_gpus = num_gpus\n",
    "                self.requires_multi_gpu = multi_gpu\n",
    "        \n",
    "        if issues:\n",
    "            print(\"\\n⚠ System resource issues:\")\n",
    "            for issue in issues:\n",
    "                print(f\"  - {issue}\")\n",
    "            return False\n",
    "        else:\n",
    "            print(\"\\n✓ All system checks passed\")\n",
    "            return True\n",
    "    \n",
    "    def _install_dependencies(self) -> bool:\n",
    "        \"\"\"Install required dependencies\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"DEPENDENCY INSTALLATION\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "            print(\"✓ logits_processor_zoo already installed\")\n",
    "            return True\n",
    "        except ImportError:\n",
    "            print(\"Installing logits_processor_zoo...\")\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [sys.executable, \"-m\", \"pip\", \"install\", \"logits_processor_zoo\", \"--no-deps\"],\n",
    "                    capture_output=True,\n",
    "                    text=True,\n",
    "                    timeout=60\n",
    "                )\n",
    "                \n",
    "                if result.returncode == 0:\n",
    "                    print(\"✓ Successfully installed\")\n",
    "                    return True\n",
    "                else:\n",
    "                    print(f\"⚠ Installation failed: {result.stderr}\")\n",
    "                    return False\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"⚠ Installation error: {e}\")\n",
    "                return False\n",
    "    \n",
    "    def _initialize_model_with_retry(self, max_attempts: int = 3) -> bool:\n",
    "        \"\"\"Initialize vLLM model with proper error handling\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL INITIALIZATION\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if self.gpu_config is None:\n",
    "            print(\"✗ No valid GPU configuration available\")\n",
    "            return False\n",
    "        \n",
    "        # Clean up any existing distributed state before each attempt\n",
    "        kill_distributed_processes()\n",
    "        \n",
    "        # Get GPU configuration\n",
    "        optimal_gpus = self.gpu_config\n",
    "        num_gpus = self.num_gpus\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            print(f\"\\nAttempt {attempt + 1}/{max_attempts}\")\n",
    "            \n",
    "            try:\n",
    "                # Clean up previous attempt\n",
    "                if hasattr(self, 'model') and self.model is not None:\n",
    "                    del self.model\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    time.sleep(2)\n",
    "                \n",
    "                # Kill distributed processes again\n",
    "                kill_distributed_processes()\n",
    "                \n",
    "                # Set CUDA devices\n",
    "                os.environ['CUDA_VISIBLE_DEVICES'] = optimal_gpus\n",
    "                \n",
    "                # Progressive parameters - adjust based on attempt\n",
    "                if attempt == 0:\n",
    "                    gpu_utilization = 0.90\n",
    "                    max_model_len = 2048\n",
    "                elif attempt == 1:\n",
    "                    gpu_utilization = 0.85\n",
    "                    max_model_len = 1536\n",
    "                else:\n",
    "                    gpu_utilization = 0.80\n",
    "                    max_model_len = 1024\n",
    "                \n",
    "                print(f\"Initializing with:\")\n",
    "                print(f\"  - GPU(s): {optimal_gpus}\")\n",
    "                print(f\"  - Tensor parallel size: {num_gpus}\")\n",
    "                print(f\"  - GPU utilization: {gpu_utilization:.2f}\")\n",
    "                print(f\"  - Max model length: {max_model_len}\")\n",
    "                \n",
    "                if self.requires_multi_gpu:\n",
    "                    print(f\"  - Multi-GPU distribution: ACTIVE\")\n",
    "                \n",
    "                # Import vLLM here to ensure clean state\n",
    "                import vllm\n",
    "                \n",
    "                # Initialize model with simplified parameters\n",
    "                self.model = vllm.LLM(\n",
    "                    model=self.model_path,\n",
    "                    tensor_parallel_size=num_gpus,\n",
    "                    gpu_memory_utilization=gpu_utilization,\n",
    "                    trust_remote_code=True,\n",
    "                    dtype=\"half\",\n",
    "                    enforce_eager=True,\n",
    "                    max_model_len=max_model_len,\n",
    "                    disable_log_stats=True,\n",
    "                    seed=42\n",
    "                )\n",
    "                \n",
    "                print(\"✓ Model initialized successfully\")\n",
    "                \n",
    "                # Get tokenizer\n",
    "                self.tokenizer = self.model.get_tokenizer()\n",
    "                print(\"✓ Tokenizer loaded\")\n",
    "                \n",
    "                return True\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Initialization failed: {str(e)[:200]}\")\n",
    "                \n",
    "                # Clean up\n",
    "                if hasattr(self, 'model'):\n",
    "                    del self.model\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                kill_distributed_processes()\n",
    "                \n",
    "                if attempt < max_attempts - 1:\n",
    "                    wait_time = (attempt + 1) * 5\n",
    "                    print(f\"Waiting {wait_time} seconds before retry...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    \n",
    "                    # On last attempt, try single GPU if using multi-GPU\n",
    "                    if attempt == max_attempts - 2 and self.num_gpus > 1:\n",
    "                        print(\"Switching to single GPU for final attempt...\")\n",
    "                        gpu_info = self.gpu_manager.get_gpu_memory_info()\n",
    "                        gpu_info.sort(key=lambda x: x['free_memory_gb'], reverse=True)\n",
    "                        if gpu_info and gpu_info[0]['free_memory_gb'] >= self.model_size_gb:\n",
    "                            self.gpu_config = str(gpu_info[0]['id'])\n",
    "                            self.num_gpus = 1\n",
    "                            self.requires_multi_gpu = False\n",
    "                            optimal_gpus = self.gpu_config\n",
    "                            num_gpus = 1\n",
    "                else:\n",
    "                    print(\"✗ All initialization attempts failed\")\n",
    "                    return False\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _create_logits_processor(self):\n",
    "        \"\"\"Create logits processor with fallback\"\"\"\n",
    "        try:\n",
    "            from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "            processor = MultipleChoiceLogitsProcessor(self.tokenizer, choices=[\"Yes\", \"No\"])\n",
    "            print(\"✓ Using external MultipleChoiceLogitsProcessor\")\n",
    "            return processor, True\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Using fallback logits processor: {e}\")\n",
    "            \n",
    "            class FallbackProcessor:\n",
    "                def __init__(self, tokenizer, choices):\n",
    "                    self.tokenizer = tokenizer\n",
    "                    self.choices = choices\n",
    "                    self.allowed_ids = []\n",
    "                    \n",
    "                    for choice in choices:\n",
    "                        tokens = tokenizer.encode(choice, add_special_tokens=False)\n",
    "                        if tokens:\n",
    "                            self.allowed_ids.append(tokens[0])\n",
    "                \n",
    "                def __call__(self, input_ids, scores):\n",
    "                    mask = torch.full_like(scores, float('-inf'))\n",
    "                    for token_id in self.allowed_ids:\n",
    "                        if token_id < len(scores):\n",
    "                            mask[token_id] = 0\n",
    "                    return scores + mask\n",
    "            \n",
    "            processor = FallbackProcessor(self.tokenizer, [\"Yes\", \"No\"])\n",
    "            return processor, False\n",
    "    \n",
    "    def run_inference(self, df: pd.DataFrame, is_training: bool = False) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Run model inference\"\"\"\n",
    "        \n",
    "        # System checks\n",
    "        if not self._check_system_resources():\n",
    "            print(\"✗ System resource check failed\")\n",
    "            return None\n",
    "        \n",
    "        # Install dependencies\n",
    "        external_processor = self._install_dependencies()\n",
    "        \n",
    "        # Initialize model\n",
    "        if not self._initialize_model_with_retry():\n",
    "            print(\"✗ Failed to initialize model\")\n",
    "            return None\n",
    "        \n",
    "        # Display GPU configuration\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"FINAL GPU CONFIGURATION\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Active GPUs: {self.gpu_config}\")\n",
    "        print(f\"Distribution: {'Multi-GPU' if self.requires_multi_gpu else 'Single-GPU'}\")\n",
    "        \n",
    "        # Create prompts\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"DATASET PREPARATION\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        SYS_PROMPT = \"You are given a comment on reddit. Your task is to classify if it violates the given rule. Only respond Yes/No.\"\n",
    "        \n",
    "        prompts = []\n",
    "        for _, row in df.iterrows():\n",
    "            text = f\"\"\"r/{row.subreddit}\n",
    "Rule: {row.rule}\n",
    "\n",
    "1) {row.positive_example_1}\n",
    "Violation: Yes\n",
    "\n",
    "2) {row.negative_example_1}\n",
    "Violation: No\n",
    "\n",
    "3) {row.negative_example_2}\n",
    "Violation: No\n",
    "\n",
    "4) {row.positive_example_2}\n",
    "Violation: Yes\n",
    "\n",
    "5) {row.body}\n",
    "\"\"\"\n",
    "            \n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": text}\n",
    "            ]\n",
    "            \n",
    "            prompt = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                add_generation_prompt=True,\n",
    "                tokenize=False,\n",
    "            ) + \"Violation: \"\n",
    "            prompts.append(prompt)\n",
    "        \n",
    "        print(f\"✓ Prepared {len(prompts)} prompts\")\n",
    "        \n",
    "        # Create logits processor\n",
    "        processor, using_external = self._create_logits_processor()\n",
    "        \n",
    "        # Run inference\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"INFERENCE PHASE\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        import vllm\n",
    "        \n",
    "        inference_start = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            # Adjust batch size based on GPU configuration\n",
    "            batch_size = 32 if self.requires_multi_gpu else 64\n",
    "            all_outputs = []\n",
    "            \n",
    "            for i in range(0, len(prompts), batch_size):\n",
    "                batch_prompts = prompts[i:i+batch_size]\n",
    "                batch_num = i//batch_size + 1\n",
    "                total_batches = (len(prompts) + batch_size - 1)//batch_size\n",
    "                \n",
    "                print(f\"Processing batch {batch_num}/{total_batches}\")\n",
    "                \n",
    "                batch_outputs = self.model.generate(\n",
    "                    batch_prompts,\n",
    "                    vllm.SamplingParams(\n",
    "                        seed=0,\n",
    "                        skip_special_tokens=True,\n",
    "                        max_tokens=1,\n",
    "                        logits_processors=[processor] if processor else None,\n",
    "                        logprobs=2,\n",
    "                        temperature=0.0,\n",
    "                        top_p=1.0\n",
    "                    ),\n",
    "                    use_tqdm=True\n",
    "                )\n",
    "                \n",
    "                all_outputs.extend(batch_outputs)\n",
    "                \n",
    "                # Clear cache between batches\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            outputs = all_outputs\n",
    "            inference_end = datetime.now()\n",
    "            print(f\"\\n✓ Inference completed in: {inference_end - inference_start}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ Inference failed: {str(e)[:500]}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "        \n",
    "        # Process results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"RESULTS PROCESSING\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        predictions = self._process_outputs(outputs)\n",
    "        \n",
    "        if predictions is None:\n",
    "            return None\n",
    "        \n",
    "        # Calculate metrics and save\n",
    "        results = self._calculate_metrics_and_save(df, predictions, is_training, using_external)\n",
    "        \n",
    "        # Add GPU configuration info\n",
    "        results['gpu_configuration'] = {\n",
    "            'gpus_used': self.gpu_config,\n",
    "            'num_gpus': self.num_gpus,\n",
    "            'multi_gpu': self.requires_multi_gpu\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _process_outputs(self, outputs) -> Optional[np.ndarray]:\n",
    "        \"\"\"Process model outputs\"\"\"\n",
    "        logprobs = []\n",
    "        \n",
    "        for i, output in enumerate(outputs):\n",
    "            try:\n",
    "                logprob_dict = {}\n",
    "                logprob_data = output.outputs[0].logprobs[0]\n",
    "                \n",
    "                # Extract logprobs - handle different formats\n",
    "                if hasattr(logprob_data, 'values'):\n",
    "                    # New format\n",
    "                    for lp in logprob_data.values():\n",
    "                        if hasattr(lp, 'decoded_token') and hasattr(lp, 'logprob'):\n",
    "                            logprob_dict[lp.decoded_token] = lp.logprob\n",
    "                elif isinstance(logprob_data, dict):\n",
    "                    # Old format\n",
    "                    for token_id, lp in logprob_data.items():\n",
    "                        if hasattr(lp, 'decoded_token') and hasattr(lp, 'logprob'):\n",
    "                            logprob_dict[lp.decoded_token] = lp.logprob\n",
    "                        elif isinstance(lp, dict) and 'decoded_token' in lp:\n",
    "                            logprob_dict[lp['decoded_token']] = lp.get('logprob', -10)\n",
    "                        elif isinstance(lp, (int, float)):\n",
    "                            # Try to decode token ID\n",
    "                            try:\n",
    "                                decoded = self.tokenizer.decode([int(token_id)])\n",
    "                                logprob_dict[decoded] = lp\n",
    "                            except:\n",
    "                                pass\n",
    "                \n",
    "                # Fallback based on generated text\n",
    "                if not logprob_dict:\n",
    "                    generated_text = output.outputs[0].text.strip()\n",
    "                    if generated_text == \"Yes\":\n",
    "                        logprob_dict = {\"Yes\": 0, \"No\": -10}\n",
    "                    elif generated_text == \"No\":\n",
    "                        logprob_dict = {\"Yes\": -10, \"No\": 0}\n",
    "                    else:\n",
    "                        logprob_dict = {\"Yes\": -5, \"No\": -5}\n",
    "                \n",
    "                logprobs.append(logprob_dict)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"⚠ Error processing output {i}: {e}\")\n",
    "                # Use generated text as fallback\n",
    "                try:\n",
    "                    generated_text = output.outputs[0].text.strip()\n",
    "                    if generated_text == \"Yes\":\n",
    "                        logprobs.append({\"Yes\": 0, \"No\": -10})\n",
    "                    elif generated_text == \"No\":\n",
    "                        logprobs.append({\"Yes\": -10, \"No\": 0})\n",
    "                    else:\n",
    "                        logprobs.append({\"Yes\": -5, \"No\": -5})\n",
    "                except:\n",
    "                    logprobs.append({\"Yes\": -5, \"No\": -5})\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        try:\n",
    "            predictions = []\n",
    "            for logprob_dict in logprobs:\n",
    "                yes_logprob = logprob_dict.get(\"Yes\", -10)\n",
    "                no_logprob = logprob_dict.get(\"No\", -10)\n",
    "                \n",
    "                yes_prob = np.exp(yes_logprob)\n",
    "                no_prob = np.exp(no_logprob)\n",
    "                \n",
    "                total = yes_prob + no_prob + 1e-10\n",
    "                yes_prob_normalized = yes_prob / total\n",
    "                \n",
    "                predictions.append(yes_prob_normalized)\n",
    "            \n",
    "            predictions = np.array(predictions)\n",
    "            print(f\"✓ Processed {len(predictions)} predictions\")\n",
    "            print(f\"  Mean: {predictions.mean():.4f}, Std: {predictions.std():.4f}\")\n",
    "            \n",
    "            return predictions\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to process predictions: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _calculate_metrics_and_save(self, df: pd.DataFrame, predictions: np.ndarray, \n",
    "                                   is_training: bool, using_external: bool) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate metrics and save results\"\"\"\n",
    "        from sklearn.metrics import roc_auc_score, classification_report\n",
    "        \n",
    "        results = {\n",
    "            'predictions': predictions.tolist(),  # Convert to list for JSON serialization\n",
    "            'num_predictions': len(predictions),\n",
    "            'mean_prediction': float(predictions.mean()),\n",
    "            'std_prediction': float(predictions.std()),\n",
    "            'execution_time': str(datetime.now() - self.start_time)\n",
    "        }\n",
    "        \n",
    "        # Calculate metrics if training\n",
    "        if is_training and 'rule_violation' in df.columns:\n",
    "            auc_score = roc_auc_score(df['rule_violation'], predictions)\n",
    "            results['auc_score'] = float(auc_score)\n",
    "            \n",
    "            print(f\"\\n✓ AUC Score: {auc_score:.6f}\")\n",
    "            \n",
    "            binary_predictions = (predictions > 0.5).astype(int)\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(df['rule_violation'], binary_predictions,\n",
    "                                      target_names=['No Violation', 'Violation']))\n",
    "        \n",
    "        # Save results\n",
    "        model_name = 'qwen_7b'\n",
    "        \n",
    "        # Save predictions\n",
    "        ensemble_predictions = pd.DataFrame({\n",
    "            'row_id': df['row_id'],\n",
    "            'rule_violation': predictions\n",
    "        })\n",
    "        ensemble_path = os.path.join(self.ensemble_dir, f'{model_name}_predictions.csv')\n",
    "        ensemble_predictions.to_csv(ensemble_path, index=False)\n",
    "        print(f\"\\n✓ Predictions saved to: {ensemble_path}\")\n",
    "        \n",
    "        # Save metadata (exclude predictions array to keep file small)\n",
    "        metadata = {\n",
    "            'model': model_name,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model_path': self.model_path,\n",
    "            'processor_type': 'external' if using_external else 'fallback',\n",
    "            'num_predictions': results['num_predictions'],\n",
    "            'mean_prediction': results['mean_prediction'],\n",
    "            'std_prediction': results['std_prediction'],\n",
    "            'execution_time': results['execution_time']\n",
    "        }\n",
    "        \n",
    "        if 'auc_score' in results:\n",
    "            metadata['auc_score'] = results['auc_score']\n",
    "            \n",
    "        metadata_path = os.path.join(self.ensemble_dir, f'{model_name}_metadata.json')\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        print(f\"✓ Metadata saved to: {metadata_path}\")\n",
    "        \n",
    "        # Save submission\n",
    "        submission_path = os.path.join(self.ensemble_dir, f'{model_name}_submission.csv')\n",
    "        ensemble_predictions.to_csv(submission_path, index=False)\n",
    "        print(f\"✓ Submission saved to: {submission_path}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up model resources\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"CLEANUP\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if hasattr(self, 'model') and self.model is not None:\n",
    "            del self.model\n",
    "        \n",
    "        if hasattr(self, 'tokenizer') and self.tokenizer is not None:\n",
    "            del self.tokenizer\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        # Clean GPUs\n",
    "        if torch.cuda.is_available():\n",
    "            self.gpu_manager.reset_all_gpus(aggressive=False)\n",
    "        \n",
    "        # Kill distributed processes\n",
    "        kill_distributed_processes()\n",
    "        \n",
    "        print(\"✓ Resources cleaned up\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"QWEN 7B MODEL - JIGSAW COMPETITION (FIXED)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nExecution started at: {datetime.now()}\")\n",
    "    \n",
    "    # Configuration\n",
    "    model_path = \"/kaggle/input/jigsaw-acrc-qwen7b-v01\"\n",
    "    data_path = \"/kaggle/input/jigsaw-agile-community-rules/test.csv\" \\\n",
    "                if os.getenv('KAGGLE_IS_COMPETITION_RERUN') \\\n",
    "                else \"/kaggle/input/jigsaw-agile-community-rules/train.csv\"\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DATA LOADING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    df = pd.read_csv(data_path)\n",
    "    is_training = not os.getenv('KAGGLE_IS_COMPETITION_RERUN')\n",
    "    \n",
    "    print(f\"Environment: {'Competition' if not is_training else 'Development'}\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Initialize and run model\n",
    "    runner = QwenModelRunner(model_path)\n",
    "    \n",
    "    try:\n",
    "        results = runner.run_inference(df, is_training)\n",
    "        \n",
    "        if results:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"EXECUTION COMPLETE - SUCCESS\")\n",
    "            print(\"=\"*80)\n",
    "            print(f\"Total execution time: {results['execution_time']}\")\n",
    "            print(f\"Predictions generated: {results['num_predictions']}\")\n",
    "            if 'auc_score' in results:\n",
    "                print(f\"AUC Score: {results['auc_score']:.6f}\")\n",
    "            \n",
    "            gpu_config = results.get('gpu_configuration', {})\n",
    "            if gpu_config:\n",
    "                print(f\"\\nGPU Configuration:\")\n",
    "                print(f\"  GPUs used: {gpu_config.get('gpus_used', 'Unknown')}\")\n",
    "                print(f\"  Multi-GPU: {'Yes' if gpu_config.get('multi_gpu', False) else 'No'}\")\n",
    "        else:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"EXECUTION FAILED\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Unexpected error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    finally:\n",
    "        runner.cleanup()\n",
    "        print(f\"\\nTotal runtime: {datetime.now() - runner.start_time}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf594b",
   "metadata": {
    "papermill": {
     "duration": 0.007227,
     "end_time": "2025-07-28T05:43:56.707056",
     "exception": false,
     "start_time": "2025-07-28T05:43:56.699829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ab755ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T05:43:56.723130Z",
     "iopub.status.busy": "2025-07-28T05:43:56.722908Z",
     "iopub.status.idle": "2025-07-28T05:43:56.754348Z",
     "shell.execute_reply": "2025-07-28T05:43:56.753533Z"
    },
    "papermill": {
     "duration": 0.040959,
     "end_time": "2025-07-28T05:43:56.755432",
     "exception": false,
     "start_time": "2025-07-28T05:43:56.714473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking ensemble directory: ensemble_files\n",
      "\n",
      "Files in directory:\n",
      "  - llama_8b_detailed_analysis.csv (2015.7 KB)\n",
      "  - llama_8b_metadata.json (0.5 KB)\n",
      "  - llama_8b_predictions.csv (46.7 KB)\n",
      "  - llama_8b_submission.csv (46.7 KB)\n",
      "\n",
      "Found 1 prediction file(s)\n",
      "Found 1 metadata file(s)\n",
      "\n",
      "Model summaries:\n",
      "  - llama_8b: AUC=0.9565\n",
      "================================================================================\n",
      "ENSEMBLE SUBMISSION CREATOR\n",
      "================================================================================\n",
      "Ensemble directory: ensemble_files\n",
      "Output file: submission.csv\n",
      "Method: average\n",
      "\n",
      "Found 1 prediction file(s):\n",
      "  - llama_8b_predictions.csv\n",
      "\n",
      "Loaded llama_8b:\n",
      "  - Shape: (2029, 2)\n",
      "  - Mean prediction: 0.5287\n",
      "  - Std deviation: 0.3075\n",
      "  - AUC score: 0.9565\n",
      "  - Execution time: 0:08:38.347051\n",
      "\n",
      "==================================================\n",
      "CREATING ENSEMBLE\n",
      "==================================================\n",
      "Using single model: llama_8b\n",
      "\n",
      "✓ Submission saved to: submission.csv\n",
      "\n",
      "==================================================\n",
      "SUBMISSION STATISTICS\n",
      "==================================================\n",
      "Number of predictions: 2029\n",
      "Mean prediction: 0.5287\n",
      "Std deviation: 0.3075\n",
      "Min prediction: 0.0311\n",
      "Max prediction: 0.9724\n",
      "\n",
      "Prediction distribution:\n",
      "  Very confident YES (>0.9): 215 (10.6%)\n",
      "  Confident YES (0.7-0.9): 662 (32.6%)\n",
      "  Uncertain (0.3-0.7): 456 (22.5%)\n",
      "  Confident NO (0.1-0.3): 558 (27.5%)\n",
      "  Very confident NO (<0.1): 138 (6.8%)\n",
      "\n",
      "Sample predictions:\n",
      "   row_id  rule_violation\n",
      "0       0        0.320821\n",
      "1       1        0.095349\n",
      "2       2        0.718594\n",
      "3       3        0.743168\n",
      "4       4        0.777300\n",
      "5       5        0.835484\n",
      "6       6        0.050331\n",
      "7       7        0.067547\n",
      "8       8        0.835484\n",
      "9       9        0.936285\n",
      "\n",
      "================================================================================\n",
      "ENSEMBLE COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ensemble Helper Function\n",
    "This script loads all prediction files from the ensemble_files folder and creates a final submission\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def create_ensemble_submission(ensemble_dir='ensemble_files', output_file='submission.csv', method='average'):\n",
    "    \"\"\"\n",
    "    Create final submission by combining all predictions in the ensemble directory\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ensemble_dir : str\n",
    "        Directory containing the prediction files\n",
    "    output_file : str\n",
    "        Path for the final submission file\n",
    "    method : str\n",
    "        Ensemble method ('average', 'weighted', or 'single')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : The final submission dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ENSEMBLE SUBMISSION CREATOR\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Ensemble directory: {ensemble_dir}\")\n",
    "    print(f\"Output file: {output_file}\")\n",
    "    print(f\"Method: {method}\")\n",
    "    \n",
    "    # Check if ensemble directory exists\n",
    "    if not os.path.exists(ensemble_dir):\n",
    "        raise ValueError(f\"Ensemble directory '{ensemble_dir}' not found!\")\n",
    "    \n",
    "    # Find all prediction files\n",
    "    prediction_files = [f for f in os.listdir(ensemble_dir) if f.endswith('_predictions.csv')]\n",
    "    \n",
    "    if not prediction_files:\n",
    "        raise ValueError(f\"No prediction files found in '{ensemble_dir}'\")\n",
    "    \n",
    "    print(f\"\\nFound {len(prediction_files)} prediction file(s):\")\n",
    "    for f in prediction_files:\n",
    "        print(f\"  - {f}\")\n",
    "    \n",
    "    # Load all predictions\n",
    "    all_predictions = {}\n",
    "    metadata_info = {}\n",
    "    \n",
    "    for file in prediction_files:\n",
    "        model_name = file.replace('_predictions.csv', '')\n",
    "        file_path = os.path.join(ensemble_dir, file)\n",
    "        \n",
    "        # Load predictions\n",
    "        df = pd.read_csv(file_path)\n",
    "        all_predictions[model_name] = df\n",
    "        \n",
    "        # Try to load metadata\n",
    "        metadata_file = os.path.join(ensemble_dir, f'{model_name}_metadata.json')\n",
    "        if os.path.exists(metadata_file):\n",
    "            with open(metadata_file, 'r') as f:\n",
    "                metadata_info[model_name] = json.load(f)\n",
    "        \n",
    "        print(f\"\\nLoaded {model_name}:\")\n",
    "        print(f\"  - Shape: {df.shape}\")\n",
    "        print(f\"  - Mean prediction: {df['rule_violation'].mean():.4f}\")\n",
    "        print(f\"  - Std deviation: {df['rule_violation'].std():.4f}\")\n",
    "        \n",
    "        if model_name in metadata_info:\n",
    "            meta = metadata_info[model_name]\n",
    "            if 'auc_score' in meta and meta['auc_score']:\n",
    "                print(f\"  - AUC score: {meta['auc_score']:.4f}\")\n",
    "            if 'execution_time' in meta:\n",
    "                print(f\"  - Execution time: {meta['execution_time']}\")\n",
    "    \n",
    "    # Create ensemble based on method\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"CREATING ENSEMBLE\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    if len(all_predictions) == 1 or method == 'single':\n",
    "        # Single model case\n",
    "        model_name = list(all_predictions.keys())[0]\n",
    "        print(f\"Using single model: {model_name}\")\n",
    "        submission = all_predictions[model_name].copy()\n",
    "        \n",
    "    elif method == 'average':\n",
    "        # Simple average ensemble\n",
    "        print(\"Creating average ensemble\")\n",
    "        \n",
    "        # Get reference dataframe\n",
    "        reference_df = all_predictions[list(all_predictions.keys())[0]]\n",
    "        submission = pd.DataFrame({'row_id': reference_df['row_id']})\n",
    "        \n",
    "        # Collect all predictions\n",
    "        pred_columns = []\n",
    "        for model_name, df in all_predictions.items():\n",
    "            # Ensure alignment by row_id\n",
    "            merged = pd.merge(submission[['row_id']], df, on='row_id', how='left')\n",
    "            pred_columns.append(merged['rule_violation'].values)\n",
    "        \n",
    "        # Average predictions\n",
    "        submission['rule_violation'] = np.mean(pred_columns, axis=0)\n",
    "        \n",
    "        print(f\"Averaged {len(all_predictions)} model predictions\")\n",
    "        \n",
    "    elif method == 'weighted':\n",
    "        # Weighted average based on AUC scores (if available)\n",
    "        print(\"Creating weighted ensemble\")\n",
    "        \n",
    "        # Check if we have AUC scores\n",
    "        auc_scores = {}\n",
    "        for model_name, meta in metadata_info.items():\n",
    "            if 'auc_score' in meta and meta['auc_score']:\n",
    "                auc_scores[model_name] = meta['auc_score']\n",
    "        \n",
    "        if len(auc_scores) < len(all_predictions):\n",
    "            print(\"Warning: Not all models have AUC scores, falling back to simple average\")\n",
    "            method = 'average'\n",
    "            # Recursive call with average method\n",
    "            return create_ensemble_submission(ensemble_dir, output_file, 'average')\n",
    "        \n",
    "        # Calculate weights based on AUC scores\n",
    "        total_auc = sum(auc_scores.values())\n",
    "        weights = {model: auc/total_auc for model, auc in auc_scores.items()}\n",
    "        \n",
    "        print(\"\\nModel weights:\")\n",
    "        for model, weight in weights.items():\n",
    "            print(f\"  - {model}: {weight:.3f} (AUC: {auc_scores[model]:.4f})\")\n",
    "        \n",
    "        # Get reference dataframe\n",
    "        reference_df = all_predictions[list(all_predictions.keys())[0]]\n",
    "        submission = pd.DataFrame({'row_id': reference_df['row_id']})\n",
    "        \n",
    "        # Weighted average\n",
    "        weighted_sum = np.zeros(len(submission))\n",
    "        for model_name, df in all_predictions.items():\n",
    "            merged = pd.merge(submission[['row_id']], df, on='row_id', how='left')\n",
    "            weighted_sum += merged['rule_violation'].values * weights[model_name]\n",
    "        \n",
    "        submission['rule_violation'] = weighted_sum\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown ensemble method: {method}\")\n",
    "    \n",
    "    # Ensure predictions are in valid range\n",
    "    submission['rule_violation'] = submission['rule_violation'].clip(0, 1)\n",
    "    \n",
    "    # Save submission\n",
    "    submission.to_csv(output_file, index=False)\n",
    "    print(f\"\\n✓ Submission saved to: {output_file}\")\n",
    "    \n",
    "    # Display statistics\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"SUBMISSION STATISTICS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Number of predictions: {len(submission)}\")\n",
    "    print(f\"Mean prediction: {submission['rule_violation'].mean():.4f}\")\n",
    "    print(f\"Std deviation: {submission['rule_violation'].std():.4f}\")\n",
    "    print(f\"Min prediction: {submission['rule_violation'].min():.4f}\")\n",
    "    print(f\"Max prediction: {submission['rule_violation'].max():.4f}\")\n",
    "    \n",
    "    # Show distribution\n",
    "    print(\"\\nPrediction distribution:\")\n",
    "    very_confident_yes = (submission['rule_violation'] > 0.9).sum()\n",
    "    confident_yes = ((submission['rule_violation'] > 0.7) & (submission['rule_violation'] <= 0.9)).sum()\n",
    "    uncertain = ((submission['rule_violation'] >= 0.3) & (submission['rule_violation'] <= 0.7)).sum()\n",
    "    confident_no = ((submission['rule_violation'] >= 0.1) & (submission['rule_violation'] < 0.3)).sum()\n",
    "    very_confident_no = (submission['rule_violation'] < 0.1).sum()\n",
    "    \n",
    "    total = len(submission)\n",
    "    print(f\"  Very confident YES (>0.9): {very_confident_yes} ({very_confident_yes/total*100:.1f}%)\")\n",
    "    print(f\"  Confident YES (0.7-0.9): {confident_yes} ({confident_yes/total*100:.1f}%)\")\n",
    "    print(f\"  Uncertain (0.3-0.7): {uncertain} ({uncertain/total*100:.1f}%)\")\n",
    "    print(f\"  Confident NO (0.1-0.3): {confident_no} ({confident_no/total*100:.1f}%)\")\n",
    "    print(f\"  Very confident NO (<0.1): {very_confident_no} ({very_confident_no/total*100:.1f}%)\")\n",
    "    \n",
    "    # Display sample predictions\n",
    "    print(\"\\nSample predictions:\")\n",
    "    print(submission.head(10))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ENSEMBLE COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return submission\n",
    "\n",
    "\n",
    "def quick_check_ensemble(ensemble_dir='ensemble_files'):\n",
    "    \"\"\"\n",
    "    Quick function to check what's available in the ensemble directory\n",
    "    \"\"\"\n",
    "    print(f\"\\nChecking ensemble directory: {ensemble_dir}\")\n",
    "    \n",
    "    if not os.path.exists(ensemble_dir):\n",
    "        print(f\"Directory '{ensemble_dir}' does not exist\")\n",
    "        return\n",
    "    \n",
    "    files = os.listdir(ensemble_dir)\n",
    "    \n",
    "    print(f\"\\nFiles in directory:\")\n",
    "    for file in sorted(files):\n",
    "        file_path = os.path.join(ensemble_dir, file)\n",
    "        size = os.path.getsize(file_path) / 1024  # Size in KB\n",
    "        print(f\"  - {file} ({size:.1f} KB)\")\n",
    "    \n",
    "    # Check for prediction files\n",
    "    prediction_files = [f for f in files if f.endswith('_predictions.csv')]\n",
    "    print(f\"\\nFound {len(prediction_files)} prediction file(s)\")\n",
    "    \n",
    "    # Check for metadata files\n",
    "    metadata_files = [f for f in files if f.endswith('_metadata.json')]\n",
    "    print(f\"Found {len(metadata_files)} metadata file(s)\")\n",
    "    \n",
    "    # Show model summary\n",
    "    if metadata_files:\n",
    "        print(\"\\nModel summaries:\")\n",
    "        for meta_file in metadata_files:\n",
    "            with open(os.path.join(ensemble_dir, meta_file), 'r') as f:\n",
    "                meta = json.load(f)\n",
    "                model_name = meta.get('model', 'Unknown')\n",
    "                auc = meta.get('auc_score', 'N/A')\n",
    "                if auc != 'N/A':\n",
    "                    auc = f\"{auc:.4f}\"\n",
    "                print(f\"  - {model_name}: AUC={auc}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check what's available\n",
    "    quick_check_ensemble()\n",
    "    \n",
    "    # Create ensemble submission\n",
    "    # For single model, any method will work\n",
    "    submission_df = create_ensemble_submission(\n",
    "        ensemble_dir='ensemble_files',\n",
    "        output_file='submission.csv',\n",
    "        method='average'  # or 'single' for explicit single model handling\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13121456,
     "isSourceIdPinned": false,
     "sourceId": 94635,
     "sourceType": "competition"
    },
    {
     "datasetId": 7934362,
     "sourceId": 12564646,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 252616838,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 88492,
     "modelInstanceId": 67985,
     "sourceId": 206163,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 225262,
     "modelInstanceId": 204042,
     "sourceId": 256574,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 222398,
     "modelInstanceId": 239467,
     "sourceId": 282742,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 322000,
     "modelInstanceId": 301507,
     "sourceId": 363127,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 378977,
     "modelInstanceId": 359397,
     "sourceId": 442505,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 702.032723,
   "end_time": "2025-07-28T05:44:00.786407",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-28T05:32:18.753684",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00fb39ed55a240a09877318604d42af9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "1471f05fdd6344bf82a5b0456ae17513": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e90e5491ebeb42e3a59cbc37337d8444",
       "placeholder": "​",
       "style": "IPY_MODEL_3e219634b0ad49f7b57d0e3f42ae5b72",
       "tabbable": null,
       "tooltip": null,
       "value": "Processed prompts: 100%"
      }
     },
     "16d8965e983042c8966e66134192eff0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "20067ac2ce3c42b2a858f6f4a0170f1d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "25122684ed934348809a3e494820f2f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_997c9a265883427f9c72db194cde2751",
       "placeholder": "​",
       "style": "IPY_MODEL_e9714836b72d47a4a9fa7eb61ac83161",
       "tabbable": null,
       "tooltip": null,
       "value": " 2029/2029 [05:12&lt;00:00,  7.38it/s, est. speed input: 2172.74 toks/s, output: 6.49 toks/s]"
      }
     },
     "256fa6c3d71b49a18853953718d75328": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "33fd2fdd585a447ea6d3d0a2e7eee9fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "35b73ab217db4b0d83fc50c93f96d67c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1471f05fdd6344bf82a5b0456ae17513",
        "IPY_MODEL_813361c235b147ca9e57b73713b81df2",
        "IPY_MODEL_25122684ed934348809a3e494820f2f0"
       ],
       "layout": "IPY_MODEL_00fb39ed55a240a09877318604d42af9",
       "tabbable": null,
       "tooltip": null
      }
     },
     "35fc66f97ab14b44a99a69fa8dc7a82e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "391c2be6b1b24a40b75cab3804bc3a17": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b6e3b818779244539b0623fe2c0c019a",
       "placeholder": "​",
       "style": "IPY_MODEL_eac97527351a4bdca8aaced8c22b06bc",
       "tabbable": null,
       "tooltip": null,
       "value": " 2029/2029 [00:02&lt;00:00, 721.96it/s]"
      }
     },
     "3a24faaf0d194a109e708d56fb9fc822": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3e219634b0ad49f7b57d0e3f42ae5b72": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5e57e31e457540ffae136d836289438c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "71cfb9a5847d4f95b3d1de82b6f50a46": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_20067ac2ce3c42b2a858f6f4a0170f1d",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3a24faaf0d194a109e708d56fb9fc822",
       "tabbable": null,
       "tooltip": null,
       "value": 4.0
      }
     },
     "7efc18cb012c4e15ae07715ed0b54470": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_33fd2fdd585a447ea6d3d0a2e7eee9fe",
       "placeholder": "​",
       "style": "IPY_MODEL_c16079edb4a24cf0a719a3d60c4477e9",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:22&lt;00:00, 20.37s/it]\n"
      }
     },
     "813361c235b147ca9e57b73713b81df2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e8219e7f55854a37a7df81ae68377f52",
       "max": 2029.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b86341b577994bbb8f590cbdff37d4aa",
       "tabbable": null,
       "tooltip": null,
       "value": 2029.0
      }
     },
     "94d3f6c2620c4780b067255856f816dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b461ab62160e4b2d9fcb62aee991ad10",
       "max": 2029.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c9ff6ff984e94ac199b23bf5c43419eb",
       "tabbable": null,
       "tooltip": null,
       "value": 2029.0
      }
     },
     "997c9a265883427f9c72db194cde2751": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a37e4acc25a04029b85df9a0821ab351": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b461ab62160e4b2d9fcb62aee991ad10": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b6e3b818779244539b0623fe2c0c019a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b6e636e483c3429c903a346c8e47a528": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ca14aa43e3204e58aea2b46f8c65b1f0",
       "placeholder": "​",
       "style": "IPY_MODEL_5e57e31e457540ffae136d836289438c",
       "tabbable": null,
       "tooltip": null,
       "value": "Adding requests: 100%"
      }
     },
     "b86341b577994bbb8f590cbdff37d4aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "bd563f6594c549cc8bcd5798c1217d1a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fa4d385c7bc24d0484acbe217294e61e",
        "IPY_MODEL_71cfb9a5847d4f95b3d1de82b6f50a46",
        "IPY_MODEL_7efc18cb012c4e15ae07715ed0b54470"
       ],
       "layout": "IPY_MODEL_256fa6c3d71b49a18853953718d75328",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c16079edb4a24cf0a719a3d60c4477e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c9ff6ff984e94ac199b23bf5c43419eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ca14aa43e3204e58aea2b46f8c65b1f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e8219e7f55854a37a7df81ae68377f52": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e90e5491ebeb42e3a59cbc37337d8444": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e9714836b72d47a4a9fa7eb61ac83161": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "eac97527351a4bdca8aaced8c22b06bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f5a4c835c50e4e80bb8802e538df3745": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b6e636e483c3429c903a346c8e47a528",
        "IPY_MODEL_94d3f6c2620c4780b067255856f816dd",
        "IPY_MODEL_391c2be6b1b24a40b75cab3804bc3a17"
       ],
       "layout": "IPY_MODEL_a37e4acc25a04029b85df9a0821ab351",
       "tabbable": null,
       "tooltip": null
      }
     },
     "fa4d385c7bc24d0484acbe217294e61e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_16d8965e983042c8966e66134192eff0",
       "placeholder": "​",
       "style": "IPY_MODEL_35fc66f97ab14b44a99a69fa8dc7a82e",
       "tabbable": null,
       "tooltip": null,
       "value": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

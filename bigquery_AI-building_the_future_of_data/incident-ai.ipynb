{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":110281,"databundleVersionId":13391012,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Incident AI Assistant.\n\n**Problem** Site Reliability Engineers (SREs) and DevOps teams waste hours manually investigating **production incidents**. They search through logs, telemetry, and historical incidents to find similar problems and resolutions. This delays fixes, causes downtime, and leads to revenue loss.\n\n**Solution** :  Use **BigQuery Vector Search** to **semantically search historical incidents** and telemetry patterns. * When a new incident happens, instantly fetch **similar past issues** + their fixes. * Use **Gemini** to **summarize possible root causes** + generate a **step-by-step resolution plan**.\n\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-07T15:31:42.883697Z","iopub.execute_input":"2025-09-07T15:31:42.883991Z","iopub.status.idle":"2025-09-07T15:31:44.869316Z","shell.execute_reply.started":"2025-09-07T15:31:42.883960Z","shell.execute_reply":"2025-09-07T15:31:44.868351Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Preparation.\n\nAs not much ","metadata":{}},{"cell_type":"code","source":"# SRE Incident & Telemetry Data Generator\n# Generate realistic production incident data with correlated telemetry patterns\n# Perfect for BigQuery AI hackathon - semantic search + incident resolution\n\nimport pandas as pd\nimport numpy as np\nimport json\nfrom datetime import datetime, timedelta\nimport random\nimport uuid\nfrom typing import List, Dict, Tuple\nimport os\n\n# =============================================================================\n# INCIDENT TEMPLATES - Based on Real Production Scenarios\n# =============================================================================\n\nINCIDENT_TEMPLATES = {\n    \"database_connection_pool\": {\n        \"title\": \"Database Connection Pool Exhaustion\",\n        \"description\": \"Application experiencing timeouts due to database connection pool being exhausted. Users unable to complete transactions.\",\n        \"technologies\": [\"PostgreSQL\", \"Java\", \"Spring Boot\", \"Connection Pool\"],\n        \"symptoms\": [\"Connection timeouts\", \"HTTP 500 errors\", \"Slow response times\"],\n        \"root_cause\": \"High traffic spike overwhelmed connection pool settings\",\n        \"resolution_steps\": [\n            \"Identified connection pool exhaustion in application logs\",\n            \"Temporarily increased max_connections in database\",\n            \"Restarted application servers to clear stale connections\", \n            \"Updated connection pool configuration to handle traffic spikes\",\n            \"Implemented connection pool monitoring alerts\"\n        ],\n        \"impact_level\": \"Critical\",\n        \"affected_services\": [\"payment-api\", \"user-service\", \"web-app\"],\n        \"telemetry_patterns\": {\n            \"pre_incident\": {\"cpu\": (40, 60), \"memory\": (60, 70), \"connections\": (80, 95)},\n            \"during_incident\": {\"cpu\": (20, 30), \"memory\": (70, 80), \"connections\": (98, 100)},\n            \"resolution\": {\"cpu\": (45, 55), \"memory\": (65, 75), \"connections\": (30, 50)}\n        }\n    },\n    \n    \"memory_leak\": {\n        \"title\": \"Memory Leak in Payment Service\",\n        \"description\": \"Payment service consuming increasing memory over time, leading to OOM kills and service restarts.\",\n        \"technologies\": [\"Node.js\", \"Kubernetes\", \"Docker\", \"MongoDB\"],\n        \"symptoms\": [\"High memory usage\", \"Frequent pod restarts\", \"Payment failures\"],\n        \"root_cause\": \"Memory leak in payment processing logic - objects not being garbage collected\",\n        \"resolution_steps\": [\n            \"Analyzed heap dumps to identify memory leak source\",\n            \"Found unclosed database connections in payment handler\",\n            \"Applied hotfix to properly close connections\",\n            \"Deployed updated service with memory limits\",\n            \"Added memory usage monitoring and alerting\"\n        ],\n        \"impact_level\": \"High\",\n        \"affected_services\": [\"payment-service\", \"order-service\"],\n        \"telemetry_patterns\": {\n            \"pre_incident\": {\"cpu\": (30, 50), \"memory\": (50, 70), \"restarts\": (0, 1)},\n            \"during_incident\": {\"cpu\": (60, 80), \"memory\": (90, 100), \"restarts\": (5, 15)},\n            \"resolution\": {\"cpu\": (35, 45), \"memory\": (40, 60), \"restarts\": (0, 1)}\n        }\n    },\n    \n    \"disk_space_full\": {\n        \"title\": \"Disk Space Exhaustion on Log Server\",\n        \"description\": \"Log aggregation server running out of disk space, causing log ingestion failures and service degradation.\",\n        \"technologies\": [\"Elasticsearch\", \"Logstash\", \"Kibana\", \"Linux\"],\n        \"symptoms\": [\"Log ingestion failures\", \"Disk I/O errors\", \"Search queries failing\"],\n        \"root_cause\": \"Log retention policy not properly configured, old logs accumulating\",\n        \"resolution_steps\": [\n            \"Identified full disk partitions on log servers\",\n            \"Cleaned up old log indices to free immediate space\",\n            \"Implemented automated log retention policies\",\n            \"Added disk usage monitoring and alerts\",\n            \"Scaled storage capacity for log infrastructure\"\n        ],\n        \"impact_level\": \"Medium\",\n        \"affected_services\": [\"logging-service\", \"monitoring-stack\", \"search-api\"],\n        \"telemetry_patterns\": {\n            \"pre_incident\": {\"cpu\": (20, 40), \"disk_usage\": (70, 85), \"io_wait\": (5, 10)},\n            \"during_incident\": {\"cpu\": (80, 100), \"disk_usage\": (95, 100), \"io_wait\": (30, 60)},\n            \"resolution\": {\"cpu\": (25, 35), \"disk_usage\": (40, 60), \"io_wait\": (2, 8)}\n        }\n    },\n    \n    \"api_rate_limit\": {\n        \"title\": \"Third-party API Rate Limiting\",\n        \"description\": \"External payment gateway implementing rate limits, causing transaction failures during peak hours.\",\n        \"technologies\": [\"REST API\", \"Payment Gateway\", \"Redis\", \"Rate Limiting\"],\n        \"symptoms\": [\"Payment failures\", \"HTTP 429 errors\", \"Customer complaints\"],\n        \"root_cause\": \"Payment provider implemented new rate limits without notification\",\n        \"resolution_steps\": [\n            \"Identified HTTP 429 responses from payment provider\",\n            \"Implemented exponential backoff retry logic\",\n            \"Added Redis-based request queuing system\",\n            \"Configured circuit breaker for payment failures\",\n            \"Set up monitoring for external API response codes\"\n        ],\n        \"impact_level\": \"High\",\n        \"affected_services\": [\"payment-gateway\", \"checkout-service\", \"billing-api\"],\n        \"telemetry_patterns\": {\n            \"pre_incident\": {\"response_time\": (200, 300), \"error_rate\": (0.1, 0.5), \"success_rate\": (99.0, 99.8)},\n            \"during_incident\": {\"response_time\": (2000, 5000), \"error_rate\": (15, 30), \"success_rate\": (70, 85)},\n            \"resolution\": {\"response_time\": (250, 400), \"error_rate\": (0.2, 1.0), \"success_rate\": (98.5, 99.5)}\n        }\n    },\n    \n    \"cpu_spike\": {\n        \"title\": \"CPU Spike Due to Inefficient Query\",\n        \"description\": \"Database experiencing high CPU usage due to poorly optimized query causing performance degradation across all services.\",\n        \"technologies\": [\"MySQL\", \"Database\", \"Query Optimization\", \"Indexing\"],\n        \"symptoms\": [\"High database CPU\", \"Slow query responses\", \"Application timeouts\"],\n        \"root_cause\": \"New feature deployed with unoptimized database query missing proper indexes\",\n        \"resolution_steps\": [\n            \"Identified slow queries using database performance tools\",\n            \"Found missing index on frequently queried table\",\n            \"Created appropriate database indexes\",\n            \"Optimized query execution plan\",\n            \"Implemented query performance monitoring\"\n        ],\n        \"impact_level\": \"Medium\",\n        \"affected_services\": [\"user-service\", \"product-catalog\", \"search-api\"],\n        \"telemetry_patterns\": {\n            \"pre_incident\": {\"cpu\": (30, 50), \"query_time\": (50, 100), \"active_connections\": (20, 40)},\n            \"during_incident\": {\"cpu\": (90, 100), \"query_time\": (2000, 8000), \"active_connections\": (80, 100)},\n            \"resolution\": {\"cpu\": (35, 55), \"query_time\": (60, 120), \"active_connections\": (25, 45)}\n        }\n    }\n}\n\n# =============================================================================\n# DATA GENERATION CLASSES\n# =============================================================================\n\nclass TelemetryGenerator:\n    \"\"\"Generate realistic telemetry data patterns for incidents\"\"\"\n    \n    def __init__(self):\n        self.metric_types = {\n            \"infrastructure\": [\"cpu_usage\", \"memory_usage\", \"disk_usage\", \"network_io\", \"load_average\"],\n            \"application\": [\"response_time\", \"error_rate\", \"throughput\", \"active_connections\", \"queue_depth\"],\n            \"business\": [\"transactions_per_minute\", \"success_rate\", \"revenue_per_hour\", \"active_users\"]\n        }\n    \n    def generate_metric_timeline(self, incident_time: datetime, template: Dict) -> List[Dict]:\n        \"\"\"Generate telemetry data around an incident with realistic patterns\"\"\"\n        \n        timeline = []\n        telemetry_patterns = template[\"telemetry_patterns\"]\n        \n        # Generate data points every minute for 4 hours around incident\n        start_time = incident_time - timedelta(hours=2)\n        end_time = incident_time + timedelta(hours=2)\n        \n        current_time = start_time\n        while current_time <= end_time:\n            \n            # Determine which phase we're in\n            if current_time < incident_time - timedelta(minutes=30):\n                phase = \"pre_incident\"\n            elif current_time < incident_time + timedelta(minutes=45):\n                phase = \"during_incident\"\n            else:\n                phase = \"resolution\"\n            \n            # Generate metrics for this timestamp\n            for service in template[\"affected_services\"]:\n                for metric_category in self.metric_types:\n                    for metric_name in self.metric_types[metric_category]:\n                        \n                        # Get expected range for this metric in this phase\n                        if metric_name in telemetry_patterns[phase]:\n                            min_val, max_val = telemetry_patterns[phase][metric_name]\n                        else:\n                            # Default ranges for metrics not specified in template\n                            min_val, max_val = self._get_default_range(metric_name, phase)\n                        \n                        # Add some noise and trends\n                        value = random.uniform(min_val, max_val)\n                        value = max(0, value + random.gauss(0, (max_val - min_val) * 0.1))\n                        \n                        timeline.append({\n                            \"timestamp\": current_time,\n                            \"service_name\": service,\n                            \"metric_category\": metric_category,\n                            \"metric_name\": metric_name,\n                            \"metric_value\": round(value, 2),\n                            \"incident_id\": None,  # Will be set later\n                            \"phase\": phase\n                        })\n            \n            current_time += timedelta(minutes=1)\n        \n        return timeline\n    \n    def _get_default_range(self, metric_name: str, phase: str) -> Tuple[float, float]:\n        \"\"\"Get default metric ranges for common metrics\"\"\"\n        \n        defaults = {\n            \"pre_incident\": {\n                \"cpu_usage\": (20, 60), \"memory_usage\": (40, 70), \"disk_usage\": (30, 70),\n                \"response_time\": (100, 500), \"error_rate\": (0.1, 2.0), \"throughput\": (100, 500),\n                \"success_rate\": (95, 99.5), \"active_users\": (1000, 5000)\n            },\n            \"during_incident\": {\n                \"cpu_usage\": (70, 100), \"memory_usage\": (80, 100), \"disk_usage\": (60, 95),\n                \"response_time\": (1000, 10000), \"error_rate\": (5, 25), \"throughput\": (10, 100),\n                \"success_rate\": (60, 90), \"active_users\": (500, 2000)\n            },\n            \"resolution\": {\n                \"cpu_usage\": (25, 65), \"memory_usage\": (45, 75), \"disk_usage\": (35, 75),\n                \"response_time\": (150, 600), \"error_rate\": (0.5, 3.0), \"throughput\": (120, 600),\n                \"success_rate\": (92, 99), \"active_users\": (800, 4500)\n            }\n        }\n        \n        return defaults.get(phase, {}).get(metric_name, (10, 90))\n\nclass IncidentGenerator:\n    \"\"\"Generate realistic incident records with resolutions\"\"\"\n    \n    def __init__(self):\n        self.telemetry_gen = TelemetryGenerator()\n    \n    def generate_incidents(self, num_incidents: int = 100) -> Tuple[pd.DataFrame, pd.DataFrame]:\n        \"\"\"Generate a dataset of incidents with corresponding telemetry\"\"\"\n        \n        incidents_data = []\n        telemetry_data = []\n        \n        # Generate incidents over the past 6 months\n        end_date = datetime.now()\n        start_date = end_date - timedelta(days=180)\n        \n        for i in range(num_incidents):\n            # Random incident time in the past 6 months\n            incident_time = start_date + timedelta(\n                seconds=random.randint(0, int((end_date - start_date).total_seconds()))\n            )\n            \n            # Choose random incident template\n            template_name = random.choice(list(INCIDENT_TEMPLATES.keys()))\n            template = INCIDENT_TEMPLATES[template_name].copy()\n            \n            # Generate unique incident ID\n            incident_id = f\"INC-{str(uuid.uuid4())[:8].upper()}\"\n            \n            # Add some variation to the template\n            severity_levels = [\"Critical\", \"High\", \"Medium\", \"Low\"]\n            template[\"impact_level\"] = random.choice(severity_levels)\n            \n            # Calculate resolution time based on severity\n            resolution_times = {\"Critical\": (15, 120), \"High\": (30, 240), \"Medium\": (60, 480), \"Low\": (120, 1440)}\n            min_res, max_res = resolution_times[template[\"impact_level\"]]\n            resolution_time = incident_time + timedelta(minutes=random.randint(min_res, max_res))\n            \n            # Create incident record\n            incident_record = {\n                \"incident_id\": incident_id,\n                \"title\": template[\"title\"],\n                \"description\": template[\"description\"],\n                \"impact_level\": template[\"impact_level\"],\n                \"technologies\": json.dumps(template[\"technologies\"]),\n                \"affected_services\": json.dumps(template[\"affected_services\"]),\n                \"symptoms\": json.dumps(template[\"symptoms\"]),\n                \"root_cause\": template[\"root_cause\"],\n                \"resolution_steps\": json.dumps(template[\"resolution_steps\"]),\n                \"incident_time\": incident_time,\n                \"resolution_time\": resolution_time,\n                \"duration_minutes\": int((resolution_time - incident_time).total_seconds() / 60),\n                \"template_type\": template_name\n            }\n            \n            incidents_data.append(incident_record)\n            \n            # Generate corresponding telemetry data\n            telemetry_timeline = self.telemetry_gen.generate_metric_timeline(incident_time, template)\n            \n            # Link telemetry to incident\n            for telemetry_point in telemetry_timeline:\n                telemetry_point[\"incident_id\"] = incident_id\n                telemetry_data.append(telemetry_point)\n            \n            if (i + 1) % 20 == 0:\n                print(f\"Generated {i + 1}/{num_incidents} incidents...\")\n        \n        # Convert to DataFrames\n        incidents_df = pd.DataFrame(incidents_data)\n        telemetry_df = pd.DataFrame(telemetry_data)\n        \n        return incidents_df, telemetry_df\n\n# =============================================================================\n# EXPORT AND VALIDATION FUNCTIONS\n# =============================================================================\n\ndef save_datasets(incidents_df: pd.DataFrame, telemetry_df: pd.DataFrame, output_dir: str = \"sre_dataset\"):\n    \"\"\"Save generated datasets to CSV files\"\"\"\n    \n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Save incidents\n    incidents_path = f\"{output_dir}/incidents.csv\"\n    incidents_df.to_csv(incidents_path, index=False)\n    print(f\"✅ Saved {len(incidents_df)} incidents to {incidents_path}\")\n    \n    # Save telemetry (might be large, so save in chunks)\n    telemetry_path = f\"{output_dir}/telemetry.csv\"\n    telemetry_df.to_csv(telemetry_path, index=False)\n    print(f\"✅ Saved {len(telemetry_df)} telemetry points to {telemetry_path}\")\n    \n    # Create data dictionary\n    data_dict = {\n        \"dataset_info\": {\n            \"name\": \"SRE Incident Intelligence Dataset\",\n            \"version\": \"1.0\",\n            \"created\": datetime.now().isoformat(),\n            \"description\": \"Synthetic but realistic SRE incident data with correlated telemetry for AI/ML training\"\n        },\n        \"incidents_schema\": {\n            \"incident_id\": \"Unique identifier for incident\",\n            \"title\": \"Brief incident title\",\n            \"description\": \"Detailed incident description\",\n            \"impact_level\": \"Critical, High, Medium, Low\",\n            \"technologies\": \"JSON array of involved technologies\",\n            \"affected_services\": \"JSON array of impacted services\",\n            \"symptoms\": \"JSON array of observed symptoms\",\n            \"root_cause\": \"Root cause analysis\",\n            \"resolution_steps\": \"JSON array of resolution steps\",\n            \"incident_time\": \"When incident started\",\n            \"resolution_time\": \"When incident was resolved\",\n            \"duration_minutes\": \"Total incident duration\",\n            \"template_type\": \"Which incident pattern this follows\"\n        },\n        \"telemetry_schema\": {\n            \"timestamp\": \"Metric collection time\",\n            \"service_name\": \"Name of service being monitored\",\n            \"metric_category\": \"infrastructure, application, or business\",\n            \"metric_name\": \"Specific metric name\",\n            \"metric_value\": \"Metric value\",\n            \"incident_id\": \"Related incident ID\",\n            \"phase\": \"pre_incident, during_incident, or resolution\"\n        }\n    }\n    \n    dict_path = f\"{output_dir}/data_dictionary.json\"\n    with open(dict_path, 'w') as f:\n        json.dump(data_dict, f, indent=2, default=str)\n    print(f\"✅ Saved data dictionary to {dict_path}\")\n\ndef validate_dataset(incidents_df: pd.DataFrame, telemetry_df: pd.DataFrame):\n    \"\"\"Validate the generated dataset quality\"\"\"\n    \n    print(\"\\n📊 DATASET VALIDATION REPORT\")\n    print(\"=\" * 50)\n    \n    # Incidents validation\n    print(f\"📋 INCIDENTS:\")\n    print(f\"  Total incidents: {len(incidents_df)}\")\n    print(f\"  Incident types: {incidents_df['template_type'].nunique()}\")\n    print(f\"  Severity distribution:\")\n    for severity, count in incidents_df['impact_level'].value_counts().items():\n        print(f\"    {severity}: {count}\")\n    \n    print(f\"  Average resolution time: {incidents_df['duration_minutes'].mean():.1f} minutes\")\n    \n    # Telemetry validation  \n    print(f\"\\n📊 TELEMETRY:\")\n    print(f\"  Total data points: {len(telemetry_df)}\")\n    print(f\"  Services monitored: {telemetry_df['service_name'].nunique()}\")\n    print(f\"  Metric types: {telemetry_df['metric_name'].nunique()}\")\n    print(f\"  Time range: {telemetry_df['timestamp'].min()} to {telemetry_df['timestamp'].max()}\")\n    \n    # Relationship validation\n    incident_ids_in_telemetry = telemetry_df['incident_id'].nunique()\n    incident_ids_total = incidents_df['incident_id'].nunique()\n    \n    print(f\"\\n🔗 RELATIONSHIPS:\")\n    print(f\"  Incidents with telemetry: {incident_ids_in_telemetry}/{incident_ids_total}\")\n    print(f\"  Avg telemetry points per incident: {len(telemetry_df) / incident_ids_total:.0f}\")\n\n# =============================================================================\n# MAIN GENERATION FUNCTION\n# =============================================================================\n\ndef generate_complete_dataset(num_incidents: int = 100):\n    \"\"\"Generate complete SRE incident dataset\"\"\"\n    \n    print(\"🚀 SRE INCIDENT DATASET GENERATOR\")\n    print(\"=\" * 50)\n    print(f\"Generating {num_incidents} realistic incidents with telemetry...\")\n    \n    # Generate data\n    generator = IncidentGenerator()\n    incidents_df, telemetry_df = generator.generate_incidents(num_incidents)\n    \n    # Validate data\n    validate_dataset(incidents_df, telemetry_df)\n    \n    # Save data\n    save_datasets(incidents_df, telemetry_df)\n    \n    print(\"\\n✅ DATASET GENERATION COMPLETE!\")\n    print(\"\\n🎯 READY FOR BIGQUERY:\")\n    print(\"1. Upload incidents.csv and telemetry.csv to BigQuery\")\n    print(\"2. Generate embeddings for incident descriptions\")\n    print(\"3. Implement vector search for similar incidents\")\n    print(\"4. Use Gemini to generate resolution recommendations\")\n    \n    return incidents_df, telemetry_df\n\nif __name__ == \"__main__\":\n    # Generate dataset with 150 incidents (good size for demo)\n    incidents, telemetry = generate_complete_dataset(150)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T15:32:02.439760Z","iopub.execute_input":"2025-09-07T15:32:02.440110Z","iopub.status.idle":"2025-09-07T15:32:19.430207Z","shell.execute_reply.started":"2025-09-07T15:32:02.440086Z","shell.execute_reply":"2025-09-07T15:32:19.429369Z"}},"outputs":[],"execution_count":null}]}
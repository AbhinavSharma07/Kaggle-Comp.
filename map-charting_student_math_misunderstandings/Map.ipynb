{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b4d8304",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:29:25.502639Z",
     "iopub.status.busy": "2025-07-11T07:29:25.502332Z",
     "iopub.status.idle": "2025-07-11T08:37:58.728083Z",
     "shell.execute_reply": "2025-07-11T08:37:58.727050Z"
    },
    "papermill": {
     "duration": 4113.231354,
     "end_time": "2025-07-11T08:37:58.729790",
     "exception": false,
     "start_time": "2025-07-11T07:29:25.498436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined train sparse shape: (36696, 15000)\n",
      "Combined test sparse shape: (3, 15000)\n",
      "\n",
      "Training Category models...\n",
      "Category Fold 0, 33026, 3670:\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.439205\n",
      "Category Fold 1, 33026, 3670:\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.445909\n",
      "Category Fold 2, 33026, 3670:\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.413911\n",
      "Category Fold 3, 33026, 3670:\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.428271\n",
      "Category Fold 4, 33026, 3670:\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.427352\n",
      "Category Fold 5, 33026, 3670:\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's multi_logloss: 0.424837\n",
      "Category Fold 6, 33027, 3669:\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.421561\n",
      "Category Fold 7, 33027, 3669:\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.427154\n",
      "Category Fold 8, 33027, 3669:\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.435063\n",
      "Category Fold 9, 33027, 3669:\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.414449\n",
      "Category ACC: 0.8273926313494658\n",
      "Category F1: 0.8200162389499444\n",
      "\n",
      "Training Misconception models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 6 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misconception Fold 0, 33026, 3670:\n",
      "Training until validation scores don't improve for 15 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[150]\tvalid_0's multi_logloss: 0.736206\n",
      "Misconception Fold 1, 33026, 3670:\n",
      "Training until validation scores don't improve for 15 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[150]\tvalid_0's multi_logloss: 0.726358\n",
      "Misconception Fold 2, 33026, 3670:\n",
      "Training until validation scores don't improve for 15 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[150]\tvalid_0's multi_logloss: 0.735752\n",
      "Misconception Fold 3, 33026, 3670:\n",
      "Training until validation scores don't improve for 15 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[150]\tvalid_0's multi_logloss: 0.719123\n",
      "Misconception Fold 4, 33026, 3670:\n",
      "Training until validation scores don't improve for 15 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[150]\tvalid_0's multi_logloss: 0.746753\n",
      "Misconception Fold 5, 33026, 3670:\n",
      "Training until validation scores don't improve for 15 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[150]\tvalid_0's multi_logloss: 0.734504\n",
      "Misconception Fold 6, 33027, 3669:\n",
      "Training until validation scores don't improve for 15 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[150]\tvalid_0's multi_logloss: 0.725375\n",
      "Misconception Fold 7, 33027, 3669:\n",
      "Training until validation scores don't improve for 15 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[150]\tvalid_0's multi_logloss: 0.747583\n",
      "Misconception Fold 8, 33027, 3669:\n",
      "Training until validation scores don't improve for 15 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[150]\tvalid_0's multi_logloss: 0.77451\n",
      "Misconception Fold 9, 33027, 3669:\n",
      "Training until validation scores don't improve for 15 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[150]\tvalid_0's multi_logloss: 0.739353\n",
      "Misconception ACC: 0.6100937431872684\n",
      "Misconception F1: 0.6454066191731862\n",
      "\n",
      "Validation Results:\n",
      "Acc@1: 0.8134946588183998\n",
      "Acc@2: 0.1341018094615217\n",
      "Acc@3: 0.0033791148899062567\n",
      "MAP@3: 0.8816719351791285\n",
      "\n",
      "Submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics\n",
    "import lightgbm as lgb\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# データの読み込み\n",
    "train = pl.read_csv(\"/kaggle/input/map-charting-student-math-misunderstandings/train.csv\")\n",
    "test = pl.read_csv(\"/kaggle/input/map-charting-student-math-misunderstandings/test.csv\")\n",
    "\n",
    "# Misconceptionの前処理\n",
    "train = train.with_columns([\n",
    "    pl.col('Misconception').fill_null('NA').cast(pl.Utf8).alias('Misconception')\n",
    "])\n",
    "\n",
    "# target_catの作成\n",
    "train = train.with_columns([\n",
    "    (pl.col('Category') + \":\" + pl.col('Misconception')).alias('target_cat')\n",
    "])\n",
    "\n",
    "# Categoryのマッピング作成\n",
    "category_counts = train['Category'].value_counts().sort('count', descending=True)\n",
    "map_target1 = {row['Category']: idx for idx, row in enumerate(category_counts.iter_rows(named=True))}\n",
    "\n",
    "# Misconceptionのマッピング作成\n",
    "misconception_counts = train['Misconception'].value_counts().sort('count', descending=True)\n",
    "map_target2 = {row['Misconception']: idx for idx, row in enumerate(misconception_counts.iter_rows(named=True))}\n",
    "\n",
    "# target1とtarget2の作成\n",
    "train = train.with_columns([\n",
    "    pl.col('Category').map_elements(lambda x: map_target1.get(x, -1), return_dtype=pl.Int64).alias('target1'),\n",
    "    pl.col('Misconception').map_elements(lambda x: map_target2.get(x, -1), return_dtype=pl.Int64).alias('target2')\n",
    "])\n",
    "\n",
    "# sentenceの作成\n",
    "def create_sentence(row):\n",
    "    return f\"Question: {row['QuestionText']}\\nAnswer: {row['MC_Answer']}\\nExplanation: {row['StudentExplanation']}\"\n",
    "\n",
    "train = train.with_columns([\n",
    "    pl.struct(['QuestionText', 'MC_Answer', 'StudentExplanation']).map_elements(\n",
    "        create_sentence, return_dtype=pl.Utf8\n",
    "    ).alias('sentence')\n",
    "])\n",
    "\n",
    "test = test.with_columns([\n",
    "    pl.struct(['QuestionText', 'MC_Answer', 'StudentExplanation']).map_elements(\n",
    "        create_sentence, return_dtype=pl.Utf8\n",
    "    ).alias('sentence')\n",
    "])\n",
    "\n",
    "# 複数のTF-IDF特徴量を作成\n",
    "print(\"Creating TF-IDF features...\")\n",
    "\n",
    "# TF-IDF 1: ngram_range=(1, 3)\n",
    "tfidf1 = TfidfVectorizer(stop_words='english', ngram_range=(1, 3), analyzer='word', \n",
    "                         max_df=0.95, min_df=2, max_features=10000)\n",
    "all_sentences = pd.concat([\n",
    "    train.select('sentence').to_pandas(),\n",
    "    test.select('sentence').to_pandas()\n",
    "])\n",
    "tfidf1.fit(all_sentences['sentence'])\n",
    "train_tfidf1 = tfidf1.transform(train['sentence'].to_pandas())\n",
    "test_tfidf1 = tfidf1.transform(test['sentence'].to_pandas())\n",
    "\n",
    "# TF-IDF 2: ngram_range=(1, 2) with character analyzer\n",
    "tfidf2 = TfidfVectorizer(stop_words='english', ngram_range=(4, 6), analyzer='char', \n",
    "                         max_df=0.95, min_df=2, max_features=5000)\n",
    "tfidf2.fit(all_sentences['sentence'])\n",
    "train_tfidf2 = tfidf2.transform(train['sentence'].to_pandas())\n",
    "test_tfidf2 = tfidf2.transform(test['sentence'].to_pandas())\n",
    "\n",
    "# 特徴量を結合\n",
    "train_embeddings = hstack([train_tfidf1, train_tfidf2])\n",
    "test_embeddings = hstack([test_tfidf1, test_tfidf2])\n",
    "print(f'Combined train sparse shape: {train_embeddings.shape}')\n",
    "print(f'Combined test sparse shape: {test_embeddings.shape}')\n",
    "\n",
    "# Target Category Training with Ensemble\n",
    "print(\"\\nTraining Category models...\")\n",
    "ytrain1_lr = np.zeros((len(train), len(map_target1)))\n",
    "ytrain1_lgb = np.zeros((len(train), len(map_target1)))\n",
    "ytest1_lr = np.zeros((len(test), len(map_target1)))\n",
    "ytest1_lgb = np.zeros((len(test), len(map_target1)))\n",
    "\n",
    "train_target1 = train['target1'].to_numpy()\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for i, (train_index, valid_index) in enumerate(skf.split(train_embeddings, train_target1)):\n",
    "    print(f\"Category Fold {i}, {len(train_index)}, {len(valid_index)}:\")\n",
    "    \n",
    "    # Logistic Regression\n",
    "    lr_model = LogisticRegression(max_iter=1000, C=1.0, random_state=42)\n",
    "    lr_model.fit(train_embeddings[train_index], train_target1[train_index])\n",
    "    ytrain1_lr[valid_index] = lr_model.predict_proba(train_embeddings[valid_index])\n",
    "    ytest1_lr += (lr_model.predict_proba(test_embeddings) / 10.)\n",
    "    \n",
    "    # LightGBM\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=31,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    lgb_model.fit(train_embeddings[train_index], train_target1[train_index],\n",
    "                  eval_set=[(train_embeddings[valid_index], train_target1[valid_index])],\n",
    "                  callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)])\n",
    "    ytrain1_lgb[valid_index] = lgb_model.predict_proba(train_embeddings[valid_index])\n",
    "    ytest1_lgb += (lgb_model.predict_proba(test_embeddings) / 10.)\n",
    "\n",
    "# アンサンブル（加重平均）\n",
    "ytrain1 = 0.6 * ytrain1_lr + 0.4 * ytrain1_lgb\n",
    "ytest1 = 0.6 * ytest1_lr + 0.4 * ytest1_lgb\n",
    "\n",
    "print(\"Category ACC:\", np.mean(train_target1 == np.argmax(ytrain1, 1)))\n",
    "print(\"Category F1:\", sklearn.metrics.f1_score(train_target1, np.argmax(ytrain1, 1), average='weighted'))\n",
    "\n",
    "# Target Misconception Training with Ensemble\n",
    "print(\"\\nTraining Misconception models...\")\n",
    "\n",
    "# 異なるTF-IDF設定でMisconception用の特徴量を作成\n",
    "tfidf_misc = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), analyzer='word', \n",
    "                             max_df=0.90, min_df=2, max_features=15000)\n",
    "tfidf_misc.fit(all_sentences['sentence'])\n",
    "train_embeddings_misc = tfidf_misc.transform(train['sentence'].to_pandas())\n",
    "test_embeddings_misc = tfidf_misc.transform(test['sentence'].to_pandas())\n",
    "\n",
    "ytrain2_lr = np.zeros((len(train), len(map_target2)))\n",
    "ytrain2_lgb = np.zeros((len(train), len(map_target2)))\n",
    "ytest2_lr = np.zeros((len(test), len(map_target2)))\n",
    "ytest2_lgb = np.zeros((len(test), len(map_target2)))\n",
    "\n",
    "train_target2 = train['target2'].to_numpy()\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for i, (train_index, valid_index) in enumerate(skf.split(train_embeddings_misc, train_target2)):\n",
    "    print(f\"Misconception Fold {i}, {len(train_index)}, {len(valid_index)}:\")\n",
    "    \n",
    "    # Logistic Regression\n",
    "    lr_model = LogisticRegression(class_weight='balanced', max_iter=1000, C=0.5, random_state=42)\n",
    "    lr_model.fit(train_embeddings_misc[train_index], train_target2[train_index])\n",
    "    ytrain2_lr[valid_index] = lr_model.predict_proba(train_embeddings_misc[valid_index])\n",
    "    ytest2_lr += (lr_model.predict_proba(test_embeddings_misc) / 10.)\n",
    "    \n",
    "    # LightGBM\n",
    "    # クラスの重みを計算\n",
    "    from sklearn.utils.class_weight import compute_sample_weight\n",
    "    sample_weights = compute_sample_weight('balanced', train_target2[train_index])\n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=150,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=50,\n",
    "        max_depth=8,\n",
    "        min_child_samples=20,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    lgb_model.fit(train_embeddings_misc[train_index], train_target2[train_index],\n",
    "                  sample_weight=sample_weights,\n",
    "                  eval_set=[(train_embeddings_misc[valid_index], train_target2[valid_index])],\n",
    "                  callbacks=[lgb.early_stopping(15), lgb.log_evaluation(0)])\n",
    "    ytrain2_lgb[valid_index] = lgb_model.predict_proba(train_embeddings_misc[valid_index])\n",
    "    ytest2_lgb += (lgb_model.predict_proba(test_embeddings_misc) / 10.)\n",
    "\n",
    "# アンサンブル（加重平均）\n",
    "ytrain2 = 0.7 * ytrain2_lr + 0.3 * ytrain2_lgb\n",
    "ytest2 = 0.7 * ytest2_lr + 0.3 * ytest2_lgb\n",
    "\n",
    "print(\"Misconception ACC:\", np.mean(train_target2 == np.argmax(ytrain2, 1)))\n",
    "print(\"Misconception F1:\", sklearn.metrics.f1_score(train_target2, np.argmax(ytrain2, 1), average='weighted'))\n",
    "\n",
    "# 逆マッピングの作成\n",
    "map_inverse1 = {v: k for k, v in map_target1.items()}\n",
    "map_inverse2 = {v: k for k, v in map_target2.items()}\n",
    "\n",
    "# 予測の生成\n",
    "ytrain2[:, 0] = 0  # NAクラスの確率を0に\n",
    "predicted1 = np.argsort(-ytrain1, 1)[:, :3]\n",
    "predicted2 = np.argsort(-ytrain2, 1)[:, :3]\n",
    "\n",
    "predict = []\n",
    "for i in range(len(predicted1)):\n",
    "    pred = []\n",
    "    for j in range(3):\n",
    "        p1 = map_inverse1[predicted1[i, j]]\n",
    "        p2 = map_inverse2[predicted2[i, j]]\n",
    "        if 'Misconception' in p1:\n",
    "            pred.append(p1 + \":\" + p2)\n",
    "        else:\n",
    "            pred.append(p1 + \":NA\")\n",
    "    predict.append(pred)\n",
    "\n",
    "# 精度の評価\n",
    "train_target_cat = train['target_cat'].to_list()\n",
    "print(\"\\nValidation Results:\")\n",
    "print(\"Acc@1:\", np.mean([train_target_cat[i] == predict[i][0] for i in range(len(predict))]))\n",
    "print(\"Acc@2:\", np.mean([train_target_cat[i] == predict[i][1] for i in range(len(predict))]))\n",
    "print(\"Acc@3:\", np.mean([train_target_cat[i] == predict[i][2] for i in range(len(predict))]))\n",
    "\n",
    "def map3(target_list, pred_list):\n",
    "    score = 0.\n",
    "    for t, p in zip(target_list, pred_list):\n",
    "        if t == p[0]:\n",
    "            score += 1.\n",
    "        elif t == p[1]:\n",
    "            score += 1/2\n",
    "        elif t == p[2]:\n",
    "            score += 1/3\n",
    "    return score / len(target_list)\n",
    "\n",
    "print(f\"MAP@3: {map3(train_target_cat, predict)}\")\n",
    "\n",
    "# テストデータの予測\n",
    "ytest2[:, 0] = 0  # NAクラスの確率を0に\n",
    "predicted1 = np.argsort(-ytest1, 1)[:, :3]\n",
    "predicted2 = np.argsort(-ytest2, 1)[:, :3]\n",
    "\n",
    "predict = []\n",
    "for i in range(len(predicted1)):\n",
    "    pred = []\n",
    "    for j in range(3):\n",
    "        p1 = map_inverse1[predicted1[i, j]]\n",
    "        p2 = map_inverse2[predicted2[i, j]]\n",
    "        if 'Misconception' in p1:\n",
    "            pred.append(p1 + \":\" + p2)\n",
    "        else:\n",
    "            pred.append(p1 + \":NA\")\n",
    "    predict.append(\" \".join(pred))\n",
    "\n",
    "# 提出ファイルの作成\n",
    "sub = pl.read_csv(\"/kaggle/input/map-charting-student-math-misunderstandings/sample_submission.csv\")\n",
    "sub = sub.with_columns([\n",
    "    pl.Series('Category:Misconception', predict)\n",
    "])\n",
    "sub.write_csv(\"submission.csv\")\n",
    "\n",
    "print(\"\\nSubmission file created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4b96258",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T08:37:58.741662Z",
     "iopub.status.busy": "2025-07-11T08:37:58.741065Z",
     "iopub.status.idle": "2025-07-11T08:37:58.750047Z",
     "shell.execute_reply": "2025-07-11T08:37:58.749188Z"
    },
    "papermill": {
     "duration": 0.016544,
     "end_time": "2025-07-11T08:37:58.751529",
     "exception": false,
     "start_time": "2025-07-11T08:37:58.734985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>row_id</th><th>Category:Misconception</th></tr><tr><td>i64</td><td>str</td></tr></thead><tbody><tr><td>36696</td><td>&quot;True_Correct:NA True_Neither:N…</td></tr><tr><td>36697</td><td>&quot;False_Misconception:Incomplete…</td></tr><tr><td>36698</td><td>&quot;True_Neither:NA True_Correct:N…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 2)\n",
       "┌────────┬─────────────────────────────────┐\n",
       "│ row_id ┆ Category:Misconception          │\n",
       "│ ---    ┆ ---                             │\n",
       "│ i64    ┆ str                             │\n",
       "╞════════╪═════════════════════════════════╡\n",
       "│ 36696  ┆ True_Correct:NA True_Neither:N… │\n",
       "│ 36697  ┆ False_Misconception:Incomplete… │\n",
       "│ 36698  ┆ True_Neither:NA True_Correct:N… │\n",
       "└────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1383f31",
   "metadata": {
    "papermill": {
     "duration": 0.004626,
     "end_time": "2025-07-11T08:37:58.761062",
     "exception": false,
     "start_time": "2025-07-11T08:37:58.756436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12957508,
     "sourceId": 104383,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4119.017274,
   "end_time": "2025-07-11T08:37:59.890742",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-11T07:29:20.873468",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

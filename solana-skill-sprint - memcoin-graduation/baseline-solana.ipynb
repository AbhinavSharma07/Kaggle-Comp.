{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":97569,"databundleVersionId":11856763,"sourceType":"competition"},{"sourceId":11407081,"sourceType":"datasetVersion","datasetId":7012766}],"dockerImageVersionId":31042,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install polars\n!pip install lightgbm\n!pip install xgboost\n!pip install catboost","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:54:31.678885Z","iopub.execute_input":"2025-06-26T08:54:31.679167Z","iopub.status.idle":"2025-06-26T08:55:24.703483Z","shell.execute_reply.started":"2025-06-26T08:54:31.679140Z","shell.execute_reply":"2025-06-26T08:55:24.697225Z"}},"outputs":[{"name":"stdout","text":"Collecting polars\n  Downloading polars-1.31.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.1/35.1 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: polars\nSuccessfully installed polars-1.31.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nCollecting lightgbm\n  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/site-packages (from lightgbm) (1.15.2)\nRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/site-packages (from lightgbm) (2.0.2)\nInstalling collected packages: lightgbm\nSuccessfully installed lightgbm-4.6.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nCollecting xgboost\n  Downloading xgboost-3.0.2-py3-none-manylinux_2_28_x86_64.whl (253.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/site-packages (from xgboost) (1.15.2)\nRequirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/site-packages (from xgboost) (2.21.5)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from xgboost) (2.0.2)\nInstalling collected packages: xgboost\nSuccessfully installed xgboost-3.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nCollecting catboost\n  Downloading catboost-1.2.8-cp310-cp310-manylinux2014_x86_64.whl (99.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/site-packages (from catboost) (3.10.1)\nRequirement already satisfied: six in /usr/local/lib/python3.10/site-packages (from catboost) (1.17.0)\nRequirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/site-packages (from catboost) (2.2.3)\nRequirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.10/site-packages (from catboost) (2.0.2)\nCollecting plotly\n  Downloading plotly-6.1.2-py3-none-any.whl (16.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting graphviz\n  Downloading graphviz-0.21-py3-none-any.whl (47 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m408.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/site-packages (from catboost) (1.15.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2025.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2025.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/site-packages (from matplotlib->catboost) (1.3.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/site-packages (from matplotlib->catboost) (3.2.3)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/site-packages (from matplotlib->catboost) (11.2.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/site-packages (from matplotlib->catboost) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from matplotlib->catboost) (25.0)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/site-packages (from matplotlib->catboost) (4.57.0)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/site-packages (from matplotlib->catboost) (0.12.1)\nCollecting narwhals>=1.15.1\n  Downloading narwhals-1.44.0-py3-none-any.whl (365 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.2/365.2 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: narwhals, graphviz, plotly, catboost\nSuccessfully installed catboost-1.2.8 graphviz-0.21 narwhals-1.44.0 plotly-6.1.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport polars as pl\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import log_loss\nfrom collections import defaultdict\nimport glob\nimport os\nimport gc\nfrom tqdm.auto import tqdm\nfrom sklearn.cluster import DBSCAN\nimport warnings\nimport traceback\n\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# --- Configuration ---\nDATA_DIR = '/kaggle/input/pump-fun-graduation-february-2025'\nTRAIN_FILE = os.path.join(DATA_DIR, 'train.csv')\nTEST_FILE = os.path.join(DATA_DIR, 'test_unlabeled.csv')\nDUNE_INFO_FILE = os.path.join(DATA_DIR, 'dune_token_info.csv')\nONCHAIN_INFO_FILE = os.path.join(DATA_DIR, 'token_info_onchain_divers.csv')\nSUBMISSION_FILE = 'submission_ensemble.csv'\n\n# Output paths for intermediate features\nFEATURES_PKL = 'features_df.pkl'\nX_TRAIN_PKL = 'X_train_processed.pkl'\nY_TRAIN_PKL = 'y_train.pkl'\nX_TEST_PKL = 'X_test_processed.pkl'\nTEST_MINTS_PKL = 'test_mints.pkl'\n\n# --- CV and Model Settings ---\nN_SPLITS = 5 # Number of folds for StratifiedKFold\nRANDOM_SEED = 42\nRUN_FEATURE_ENGINEERING = True # Set to False to load features from pickle\n\n# Early stopping rounds for models during CV\nEARLY_STOPPING_ROUNDS = 100\n# --- Feature Engineering Settings ---\nCHUNKSIZE = 1_000_000\nBLOCK_WINDOW = 100\nWHALE_TOP_N = 5 # Number of top wallets to consider for whale analysis\n\n# --- Predefined LGBM Best Params (from Optuna in private notebook) ---\n# Using the provided tuned parameters\nLGBM_BEST_PARAMS = {\n    'objective': 'binary',\n    'metric': 'logloss',\n    'boosting_type': 'gbdt',\n    'n_estimators': 2500,\n    'learning_rate': 0.013094862430042712,\n    'num_leaves': 30,\n    'max_depth': 7,\n    'lambda_l1': 0.47723465286137357, # reg_alpha\n    'lambda_l2': 0.07951439672408593, # reg_lambda\n    'colsample_bytree': 0.6640835024351832,\n    'subsample': 0.5933658927947468,\n    'min_child_samples': 90,\n    'seed': RANDOM_SEED,\n    'n_jobs': -1,\n    'verbose': -1,\n}\n\n# --- Basic XGBoost Params ---\nXGB_PARAMS = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'logloss',\n    'eta': 0.05, # learning_rate\n    'max_depth': 6,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'min_child_weight': 3,\n    'gamma': 0.1,\n    'lambda': 1, # L2 reg\n    'alpha': 0, # L1 reg\n    'seed': RANDOM_SEED,\n    'nthread': -1,\n    'tree_method': 'hist'\n}\n\nXGB_N_ESTIMATORS = 2000\n\n# --- Basic CatBoost Params ---\nCAT_PARAMS = {\n    'loss_function': 'Logloss',\n    'eval_metric': 'Logloss',\n    'learning_rate': 0.05,\n    'depth': 6,\n    'l2_leaf_reg': 3,\n    'subsample': 0.8,\n    'colsample_bylevel': 0.8,\n    'random_seed': RANDOM_SEED,\n    'thread_count': -1,\n    'verbose': 0,\n    'early_stopping_rounds': EARLY_STOPPING_ROUNDS\n}\n\nCAT_ITERATIONS = 2000\n\n\n# Cell 2: Data Loading\nprint(\"Cell 2: Loading base data...\")\ntry:\n    train_df = pd.read_csv(TRAIN_FILE)\n    test_df = pd.read_csv(TEST_FILE)\n    try: dune_df = pd.read_csv(DUNE_INFO_FILE)\n    except FileNotFoundError: print(f\"Warning: {DUNE_INFO_FILE} not found.\"); dune_df = None\n    try: onchain_df = pd.read_csv(ONCHAIN_INFO_FILE)\n    except FileNotFoundError: print(f\"Warning: {ONCHAIN_INFO_FILE} not found.\"); onchain_df = None\nexcept FileNotFoundError as e: print(f\"Error loading train/test files: {e}\"); exit()\nprint(f\"Loaded {len(train_df)} training examples and {len(test_df)} test examples.\")\ngc.collect()\n\n\n# Cell 3: Feature Engineering Function Definition\nprint(\"Cell 3: Defining memory-optimized feature engineering function with enhancements...\")\ndef generate_chunk_features_optimized(chunk_files, train_data, test_data, dune_info_file, onchain_info_file, block_window, chunksize, whale_top_n=5):\n    \"\"\"\n    Generates features from chunked transaction data with added velocity,\n    market depth proxies, and whale pattern recognition.\n    \"\"\"\n    # Combine mints and slot_min for efficient processing\n    print(\"  Combining mint info...\")\n    train_mints_info = train_data[['mint', 'slot_min']].copy()\n    test_mints_info = test_data[['mint', 'slot_min']].copy()\n    all_mints_df = pd.concat([train_mints_info, test_mints_info], ignore_index=True)\n    all_mints_map = pd.Series(all_mints_df.slot_min.values, index=all_mints_df.mint).to_dict()\n    print(f\"  Total unique mints to process: {len(all_mints_map)}\")\n    del train_mints_info, test_mints_info, all_mints_df\n    gc.collect()\n\n    # --- Start Chunk Processing ---\n    print(\"  Starting feature aggregation from chunks...\")\n    # Define the structure for accumulating features per mint\n    feature_agg = defaultdict(lambda: {\n        # Basic Counts & Volume\n        'tx_count': 0, 'buy_count': 0, 'sell_count': 0,\n        'total_sol_volume': 0.0, 'buy_sol_volume': 0.0, 'sell_sol_volume': 0.0,\n        'total_token_volume': 0.0, 'buy_token_volume': 0.0, 'sell_token_volume': 0.0,\n        # Balances & Slots\n        'max_sol_balance': -1.0, 'last_sol_balance': -1.0, 'last_slot': -1,\n        'first_slot': float('inf'), 'total_fees': 0.0, 'total_gas_used': 0.0,\n        # Standard Deviation Calculation Helpers\n        'sum_sq_sol_buy': 0.0, 'sum_sq_sol_sell': 0.0,\n        # Creator Activity\n        'creator_buys': 0, 'creator_sells': 0, 'creator_buy_vol': 0.0, 'creator_sell_vol': 0.0,\n        # Block-based Counts (for Velocity/Acceleration)\n        'first_10_block_tx': 0, 'first_20_block_tx': 0, 'first_30_block_tx': 0, 'first_50_block_tx': 0,\n        # 'last_50_block_tx': 0, # We'll calculate this at the end\n        # Wallets & Concentration\n        '_unique_wallets_set': set(),\n        # '_wallet_sol_volume': defaultdict(float), # Replaced by _wallet_activity\n        # Price & Volatility\n        '_prices_slots': [], # Store (relative_slot, price) tuples\n        # --- NEW FEATURES ---\n        # Market Depth Proxies\n        '_max_buy_price': -1.0,\n        '_min_sell_price': float('inf'),\n        # Transaction Clustering\n        '_transactions': [],  # Store (slot, direction, quote_coin_amount, base_coin_amount) tuples\n    \n        # Wash Trading\n        '_wallet_tx_history': defaultdict(list),  # {wallet: [(direction, slot, quote_coin_amount), ...]}\n\n        # Whale Wallet Tracking, Wallet Profiling\n        '_wallet_activity': defaultdict(lambda: {'buy_vol': 0.0, 'sell_vol': 0.0, 'buy_count': 0, 'sell_count': 0,\n                                                 'tx_count': 0, 'first_tx': float('inf'), 'last_tx': -1,\n                                                 'num_tokens_traded': set()  # Track tokens traded by this wallet\n                                                }),\n        # Store slot and volume for velocity/acceleration\n        '_slot_volumes': [], # Store (relative_slot, quote_coin_amount, direction)\n\n        # Anomaly Detection (Volume)\n        '_sol_volumes': [], #Store (slot, quote_coin_amount) for volume calculation.\n    \n        # Network Analysis (Basic, requires sender information which you might not have)\n        # '_wallet_connections': defaultdict(set) # {wallet: set(wallets_they_transacted_with)}\n    })\n\n    creator_map = {}\n    onchain_metadata_loaded = False\n    try:\n        onchain_meta_temp = pd.read_csv(onchain_info_file, usecols=['mint', 'creator'])\n        creator_map = pd.Series(onchain_meta_temp.creator.values, index=onchain_meta_temp.mint).drop_duplicates().to_dict()\n        onchain_metadata_loaded = True\n        print(f\"  Creator map created with {len(creator_map)} entries.\")\n        del onchain_meta_temp\n        gc.collect()\n    except Exception as e:\n        print(f\"  Warning: Cannot create creator map: {e}. Creator features will be zero.\")\n\n    for chunk_file in tqdm(chunk_files, desc=\"  Processing Chunks\"):\n        try:\n            chunk_iter = pd.read_csv(\n                chunk_file, chunksize=chunksize,\n                dtype={\n                    'slot': np.uint32, 'tx_idx': np.uint16, 'signing_wallet': 'category',\n                    'direction': 'category', 'base_coin': 'category',\n                    'base_coin_amount': np.float32, 'quote_coin_amount': np.float32,\n                    'virtual_token_balance_after': np.float64,\n                    'virtual_sol_balance_after': np.float64,\n                    'fee': np.float32, 'consumed_gas': np.float32\n                },\n                usecols=[\n                    'slot', 'tx_idx', 'signing_wallet', 'direction', 'base_coin',\n                    'base_coin_amount', 'quote_coin_amount', 'virtual_sol_balance_after',\n                    'fee', 'consumed_gas'\n                ], low_memory=True\n            )\n\n            for chunk_df in chunk_iter:\n                relevant_mints_in_chunk = set(chunk_df['base_coin'].unique()) & set(all_mints_map.keys())\n                if not relevant_mints_in_chunk: continue\n                chunk_df = chunk_df[chunk_df['base_coin'].isin(relevant_mints_in_chunk)]\n                if chunk_df.empty: continue\n\n                chunk_df['slot_min'] = chunk_df['base_coin'].map(all_mints_map)\n                chunk_df['slot_max_limit'] = chunk_df['slot_min'] + block_window\n                chunk_df = chunk_df[chunk_df['slot'] <= chunk_df['slot_max_limit']].copy()\n                if chunk_df.empty: continue\n\n                if creator_map:\n                    chunk_df['creator'] = chunk_df['base_coin'].map(creator_map)\n                    chunk_df['is_creator_tx'] = chunk_df['signing_wallet'] == chunk_df['creator']\n                else:\n                    chunk_df['is_creator_tx'] = False\n                chunk_df['relative_slot'] = chunk_df['slot'] - chunk_df['slot_min']\n\n                # Ensure categorical grouping works\n                if isinstance(chunk_df['base_coin'].dtype, pd.CategoricalDtype):\n                    grouped = chunk_df.groupby(chunk_df['base_coin'].cat.codes)\n                    mint_map = dict(enumerate(chunk_df['base_coin'].cat.categories))\n                else:\n                    grouped = chunk_df.groupby('base_coin')\n                    mint_map = {mint: mint for mint in chunk_df['base_coin'].unique()}\n\n                for group_key, group in grouped:\n                    mint = mint_map[group_key]\n                    token_features = feature_agg[mint]\n                    group_creator = creator_map.get(mint)\n\n                    # Basic Aggregations\n                    token_features['tx_count'] += len(group)\n                    buys = group[group['direction'] == 'buy']\n                    sells = group[group['direction'] == 'sell']\n                    token_features['buy_count'] += len(buys)\n                    token_features['sell_count'] += len(sells)\n                    token_features['_unique_wallets_set'].update(group['signing_wallet'].unique())\n                    token_features['total_sol_volume'] += group['quote_coin_amount'].sum()\n                    buy_vol = buys['quote_coin_amount'].sum()\n                    sell_vol = sells['quote_coin_amount'].sum()\n                    token_features['buy_sol_volume'] += buy_vol\n                    token_features['sell_sol_volume'] += sell_vol\n                    token_features['total_token_volume'] += group['base_coin_amount'].sum()\n                    buy_token_vol = buys['base_coin_amount'].sum() # Needed for avg prices\n                    sell_token_vol = sells['base_coin_amount'].sum() # Needed for avg prices\n                    token_features['buy_token_volume'] += buy_token_vol\n                    token_features['sell_token_volume'] += sell_token_vol\n                    token_features['sum_sq_sol_buy'] += (buys['quote_coin_amount']**2).sum()\n                    token_features['sum_sq_sol_sell'] += (sells['quote_coin_amount']**2).sum()\n\n                    # Iterate for detailed tracking (prices, wallet activity, market depth proxies)\n                    for _, row in group.iterrows():\n                        wallet = row['signing_wallet']\n                        sol_amount = row['quote_coin_amount']\n                        token_amount = row['base_coin_amount']\n                        relative_slot = row['relative_slot']\n                        direction = row['direction']\n                        slot = row['slot']\n\n                        # 1. Transaction Clustering\n                        token_features['_transactions'].append((slot, direction, sol_amount, token_amount)) # Use slot\n    \n                        # 2. Wash Trading\n                        token_features['_wallet_tx_history'][wallet].append((direction, slot, sol_amount))  # Use slot\n    \n                        # 3. Wallet Profiling\n                        wallet_activity = token_features['_wallet_activity'][wallet]\n                        wallet_activity['tx_count'] += 1\n                        wallet_activity['first_tx'] = min(wallet_activity['first_tx'], slot)\n                        wallet_activity['last_tx'] = max(wallet_activity['last_tx'], slot)\n                        wallet_activity['num_tokens_traded'].add(mint)  # Track tokens traded by this wallet\n    \n                        if direction == 'buy':\n                            wallet_activity['buy_vol'] += sol_amount\n                            wallet_activity['buy_count'] += 1\n                        elif direction == 'sell':\n                            wallet_activity['sell_vol'] += sol_amount\n                            wallet_activity['sell_count'] += 1\n    \n                        # 4. Anomaly Detection (Volume)\n                        token_features['_sol_volumes'].append((slot, sol_amount))\n\n                        # Store slot and volume for velocity/acceleration calculations later\n                        token_features['_slot_volumes'].append((relative_slot, sol_amount, direction))\n\n                        # Wallet Activity (for whale analysis)\n                        if direction == 'buy':\n                            token_features['_wallet_activity'][wallet]['buy_vol'] += sol_amount\n                            token_features['_wallet_activity'][wallet]['buy_count'] += 1\n                        elif direction == 'sell':\n                             token_features['_wallet_activity'][wallet]['sell_vol'] += sol_amount\n                             token_features['_wallet_activity'][wallet]['sell_count'] += 1\n\n                        # Prices & Market Depth Proxies\n                        if token_amount > 1e-9: # Avoid division by zero or near-zero\n                            price = sol_amount / token_amount\n                            token_features['_prices_slots'].append((relative_slot, price))\n                            if direction == 'buy':\n                                token_features['_max_buy_price'] = max(token_features['_max_buy_price'], price)\n                            elif direction == 'sell':\n                                token_features['_min_sell_price'] = min(token_features['_min_sell_price'], price)\n\n                    # Update Balances, Slots, Fees, Gas\n                    if not group.empty:\n                        current_max_sol = group['virtual_sol_balance_after'].max()\n                        if current_max_sol > token_features['max_sol_balance']:\n                            token_features['max_sol_balance'] = current_max_sol\n\n                        # Find the actual latest transaction in this group based on slot and tx_idx\n                        group_sorted = group.sort_values(['slot', 'tx_idx'])\n                        latest_tx_in_group = group_sorted.iloc[-1]\n\n                        if latest_tx_in_group['slot'] >= token_features['last_slot']: # Ensure it's truly the latest overall\n                            token_features['last_sol_balance'] = latest_tx_in_group['virtual_sol_balance_after']\n                            token_features['last_slot'] = latest_tx_in_group['slot']\n\n                        current_min_slot = group['slot'].min()\n                        if current_min_slot < token_features['first_slot']:\n                            token_features['first_slot'] = current_min_slot\n\n                        token_features['total_fees'] += group['fee'].sum(skipna=True)\n                        token_features['total_gas_used'] += group['consumed_gas'].sum(skipna=True)\n\n                    # Creator Activity\n                    if group_creator:\n                        creator_txs = group[group['is_creator_tx']]\n                        creator_buys_group = creator_txs[creator_txs['direction'] == 'buy']\n                        creator_sells_group = creator_txs[creator_txs['direction'] == 'sell']\n                        token_features['creator_buys'] += len(creator_buys_group)\n                        token_features['creator_sells'] += len(creator_sells_group)\n                        token_features['creator_buy_vol'] += creator_buys_group['quote_coin_amount'].sum()\n                        token_features['creator_sell_vol'] += creator_sells_group['quote_coin_amount'].sum()\n                        del creator_txs, creator_buys_group, creator_sells_group\n\n                    # Block-based counts (for velocity/acceleration)\n                    token_features['first_10_block_tx'] += (group['relative_slot'] < 10).sum()\n                    token_features['first_20_block_tx'] += (group['relative_slot'] < 20).sum()\n                    token_features['first_30_block_tx'] += (group['relative_slot'] < 30).sum()\n                    token_features['first_50_block_tx'] += (group['relative_slot'] < 50).sum()\n                    # 'last_50_block_tx' calculated during finalization\n\n                del chunk_df, grouped, mint_map, group, buys, sells # group_sorted? latest_tx_in_group?\n                gc.collect()\n        except Exception as e:\n            print(f\"Error processing chunk {chunk_file}: {e}\")\n            # import traceback\n            traceback.print_exc() # Print traceback for detailed debugging\n            continue\n\n    # --- Load Metadata ---\n    print(\"  Loading metadata for final merge...\")\n    dune_metadata, onchain_metadata = None, None\n    try: dune_metadata = pd.read_csv(dune_info_file)\n    except Exception as e: print(f\"    Warning: Cannot load dune metadata: {e}\")\n    try: onchain_metadata = pd.read_csv(onchain_info_file)\n    except Exception as e: print(f\"    Warning: Cannot load onchain metadata: {e}\")\n\n    # --- Finalize Features ---\n    print(\"  Finalizing features...\")\n    feature_list = []\n    for mint, data in tqdm(list(feature_agg.items()), desc=\"  Finalizing Features\"):\n        features = {'mint': mint}\n\n        # --- Basic Features (mostly copied from original) ---\n        features['tx_count'] = data['tx_count']\n        features['buy_count'] = data['buy_count']\n        features['sell_count'] = data['sell_count']\n        features['unique_wallets'] = len(data.pop('_unique_wallets_set', set()))\n        features['buy_ratio'] = features['buy_count'] / features['tx_count'] if features['tx_count'] > 0 else 0\n        features['sell_ratio'] = features['sell_count'] / features['tx_count'] if features['tx_count'] > 0 else 0\n        features['total_sol_volume'] = data['total_sol_volume']\n        features['buy_sol_volume'] = data['buy_sol_volume']\n        features['sell_sol_volume'] = data['sell_sol_volume']\n        features['avg_buy_sol_volume'] = features['buy_sol_volume'] / features['buy_count'] if features['buy_count'] > 0 else 0\n        features['avg_sell_sol_volume'] = features['sell_sol_volume'] / features['sell_count'] if features['sell_count'] > 0 else 0\n        features['net_sol_volume'] = features['buy_sol_volume'] - features['sell_sol_volume']\n        features['net_sol_volume_ratio'] = (features['buy_sol_volume'] - features['sell_sol_volume']) / (features['buy_sol_volume'] + features['sell_sol_volume'] + 1e-9)\n        features['sol_volume_per_wallet'] = features['total_sol_volume'] / features['unique_wallets'] if features['unique_wallets'] > 0 else 0\n        features['max_sol_balance'] = data['max_sol_balance'] if data['max_sol_balance'] != -1.0 else 0.0\n        features['last_sol_balance'] = data['last_sol_balance'] if data['last_sol_balance'] != -1.0 else 0.0\n        if features['buy_count'] > 1:\n            mean_buy = features['avg_buy_sol_volume']\n            variance_buy = (np.float64(data['sum_sq_sol_buy']) / features['buy_count']) - (np.float64(mean_buy)**2)\n            features['std_buy_sol_volume'] = np.sqrt(variance_buy) if variance_buy >= 0 else 0\n        else: features['std_buy_sol_volume'] = 0.0\n        if features['sell_count'] > 1:\n            mean_sell = features['avg_sell_sol_volume']\n            variance_sell = (np.float64(data['sum_sq_sol_sell']) / features['sell_count']) - (np.float64(mean_sell)**2)\n            features['std_sell_sol_volume'] = np.sqrt(variance_sell) if variance_sell >= 0 else 0\n        else: features['std_sell_sol_volume'] = 0.0\n        slot_min_actual = all_mints_map.get(mint, None)\n        if slot_min_actual is not None and data['first_slot'] != float('inf'):\n            features['first_tx_slot_diff'] = data['first_slot'] - slot_min_actual\n        else: features['first_tx_slot_diff'] = -1\n        if slot_min_actual is not None and data['last_slot'] != -1:\n            features['last_tx_slot_diff'] = data['last_slot'] - slot_min_actual\n            if data['first_slot'] != float('inf'): features['tx_time_range'] = data['last_slot'] - data['first_slot']\n            else: features['tx_time_range'] = 0\n        else:\n            features['last_tx_slot_diff'] = -1\n            features['tx_time_range'] = -1\n        features['avg_fee'] = data['total_fees'] / features['tx_count'] if features['tx_count'] > 0 else 0\n        features['avg_gas_used'] = data['total_gas_used'] / features['tx_count'] if features['tx_count'] > 0 else 0\n        features['creator_buys'] = data.get('creator_buys', 0)\n        features['creator_sells'] = data.get('creator_sells', 0)\n        features['creator_buy_vol'] = data.get('creator_buy_vol', 0.0)\n        features['creator_sell_vol'] = data.get('creator_sell_vol', 0.0)\n        features['creator_net_vol'] = features['creator_buy_vol'] - features['creator_sell_vol']\n        features['creator_buy_ratio_tx'] = features['creator_buys'] / features['tx_count'] if features['tx_count'] > 0 else 0\n        features['creator_sell_ratio_tx'] = features['creator_sells'] / features['tx_count'] if features['tx_count'] > 0 else 0\n        features['creator_did_sell'] = 1 if features['creator_sells'] > 0 else 0\n        features['first_10_block_tx'] = data.get('first_10_block_tx', 0)\n        features['first_20_block_tx'] = data.get('first_20_block_tx', 0)\n        features['first_30_block_tx'] = data.get('first_30_block_tx', 0)\n        features['first_50_block_tx'] = data.get('first_50_block_tx', 0)\n        features['last_50_block_tx'] = features['tx_count'] - features['first_50_block_tx'] # Calculated here\n        features['first_last_50_ratio'] = features['first_50_block_tx'] / (features['last_50_block_tx'] + 1e-9) # Avoid zero division\n        features['activity_ratio_10_50'] = features['first_10_block_tx'] / (features['first_50_block_tx'] + 1e-9)\n        features['activity_ratio_20_50'] = features['first_20_block_tx'] / (features['first_50_block_tx'] + 1e-9)\n        features['activity_ratio_30_50'] = features['first_30_block_tx'] / (features['first_50_block_tx'] + 1e-9)\n\n        # Wallet concentration (slightly modified logic from original)\n        wallet_volumes = {w: d['buy_vol'] + d['sell_vol'] for w, d in data['_wallet_activity'].items()} # Total vol per wallet\n        sorted_wallets = sorted(wallet_volumes.items(), key=lambda item: item[1], reverse=True)\n        total_vol_check = features['total_sol_volume']\n        top_5_volume = sum([vol for _, vol in sorted_wallets[:5]])\n        top_10_volume = sum([vol for _, vol in sorted_wallets[:10]])\n        features['top_5_wallet_concentration'] = top_5_volume / (total_vol_check + 1e-9)\n        features['top_10_wallet_concentration'] = top_10_volume / (total_vol_check + 1e-9)\n\n        # --- NEW FEATURE CALCULATIONS ---\n\n        # 1. Token Velocity Metrics\n        # Transaction Acceleration (Proxy: change in rate between intervals)\n        tx_0_9 = features['first_10_block_tx']\n        tx_10_19 = features['first_20_block_tx'] - tx_0_9\n        tx_20_29 = features['first_30_block_tx'] - features['first_20_block_tx']\n        tx_30_49 = features['first_50_block_tx'] - features['first_30_block_tx']\n        tx_50_99 = features['last_50_block_tx'] # Assuming window=100\n\n        # Rates per 10 blocks (approx velocity)\n        rate_0_9 = tx_0_9 / 10.0\n        rate_10_19 = tx_10_19 / 10.0\n        rate_20_29 = tx_20_29 / 10.0\n        rate_30_49 = tx_30_49 / 20.0 # 20 blocks interval\n        rate_50_99 = tx_50_99 / 50.0 # 50 blocks interval\n\n        # Acceleration (change in rate) - comparing adjacent 10-block intervals where possible\n        features['tx_accel_10_vs_0'] = rate_10_19 - rate_0_9\n        features['tx_accel_20_vs_10'] = rate_20_29 - rate_10_19\n        # Could add more complex acceleration metrics if needed\n\n        # Price Velocity (Change) & Volatility\n        prices_slots = sorted(data['_prices_slots'], key=lambda x: x[0]) # Sort by relative_slot\n        prices = [p for s, p in prices_slots]\n        if len(prices) > 0:\n            features['first_price'] = prices[0]\n            features['last_price'] = prices[-1]\n            features['price_change_abs'] = features['last_price'] - features['first_price']\n            features['price_change_rel'] = features['price_change_abs'] / (features['first_price'] + 1e-9)\n            if len(prices) > 1:\n                features['price_volatility'] = np.std(prices)\n                # Price velocity (change over time range)\n                time_diff = features['last_tx_slot_diff'] - features['first_tx_slot_diff']\n                features['price_velocity'] = features['price_change_abs'] / (time_diff + 1e-9) if time_diff > 0 else 0\n            else:\n                features['price_volatility'] = 0.0\n                features['price_velocity'] = 0.0\n        else:\n            features['first_price'] = 0.0\n            features['last_price'] = 0.0\n            features['price_change_abs'] = 0.0\n            features['price_change_rel'] = 0.0\n            features['price_volatility'] = 0.0\n            features['price_velocity'] = 0.0\n\n        # 2. Market Depth Features\n        # Spread Proxies\n        features['avg_buy_price'] = features['buy_sol_volume'] / (data['buy_token_volume'] + 1e-9)\n        features['avg_sell_price'] = features['sell_sol_volume'] / (data['sell_token_volume'] + 1e-9)\n        features['spread_proxy_avg'] = features['avg_buy_price'] - features['avg_sell_price']\n\n        max_buy_p = data['_max_buy_price']\n        min_sell_p = data['_min_sell_price']\n        if max_buy_p > 0 and min_sell_p != float('inf'):\n             features['spread_proxy_extreme'] = max_buy_p - min_sell_p\n        else: # Handle cases where only buys or only sells occurred, or no valid prices\n             features['spread_proxy_extreme'] = 0.0 # Or perhaps NaN/median later?\n\n        # Liquidity Proxy (Volume / Volatility)\n        features['liquidity_proxy_vol_std'] = features['total_sol_volume'] / (features['price_volatility'] + 1e-9)\n\n\n        # 3. Pattern Recognition (Whale Wallets)\n        wallet_activity = data['_wallet_activity']\n        # Sort wallets by BUY volume to find top buyers\n        top_buyers = sorted(wallet_activity.items(), key=lambda item: item[1]['buy_vol'], reverse=True)[:whale_top_n]\n\n        total_flip_ratio = 0.0\n        sold_count = 0\n        top_buyer_buy_vol_sum = 0.0\n        top_buyer_sell_vol_sum = 0.0\n\n        if top_buyers: # Check if list is not empty\n            for wallet, activity_data in top_buyers:\n                buy_vol = activity_data['buy_vol']\n                sell_vol = activity_data['sell_vol']\n                top_buyer_buy_vol_sum += buy_vol\n                top_buyer_sell_vol_sum += sell_vol\n                if buy_vol > 1e-9: # Avoid division by zero for flip ratio\n                    flip_ratio = sell_vol / buy_vol\n                    total_flip_ratio += flip_ratio\n                    if sell_vol > 1e-9: # Consider sold if they sold *anything*\n                        sold_count += 1\n\n            features['whale_avg_flip_ratio'] = total_flip_ratio / len(top_buyers)\n            features['whale_sell_proportion'] = sold_count / len(top_buyers) # Proportion of top N buyers who also sold\n            features['whale_net_volume_ratio'] = (top_buyer_buy_vol_sum - top_buyer_sell_vol_sum) / (top_buyer_buy_vol_sum + 1e-9)\n        else:\n            features['whale_avg_flip_ratio'] = 0.0\n            features['whale_sell_proportion'] = 0.0\n            features['whale_net_volume_ratio'] = 0.0\n\n\n        # --- Type Casting (Include New Features) ---\n        # Integer/Count Features\n        for col, dtype in {\n            'tx_count': np.uint32, 'buy_count': np.uint32, 'sell_count': np.uint32,\n            'unique_wallets': np.uint32, 'creator_buys': np.uint16, 'creator_sells': np.uint16,\n            'creator_did_sell': np.uint8, 'first_10_block_tx': np.uint32, 'first_20_block_tx': np.uint32,\n            'first_30_block_tx': np.uint32, 'first_50_block_tx': np.uint32, 'last_50_block_tx': np.uint32,\n            'first_tx_slot_diff': np.int32, 'last_tx_slot_diff': np.int32, 'tx_time_range': np.int32\n            }.items():\n            if col in features: features[col] = np.nan_to_num(features[col], nan=-1).astype(dtype)\n\n        # Float Features\n        for col, dtype in {\n            'buy_ratio': np.float32, 'sell_ratio': np.float32, 'total_sol_volume': np.float32,\n            'buy_sol_volume': np.float32, 'sell_sol_volume': np.float32, 'avg_buy_sol_volume': np.float32,\n            'avg_sell_sol_volume': np.float32, 'net_sol_volume': np.float32, 'net_sol_volume_ratio': np.float32,\n            'sol_volume_per_wallet': np.float32, 'max_sol_balance': np.float64, 'last_sol_balance': np.float64,\n            'std_buy_sol_volume': np.float32, 'std_sell_sol_volume': np.float32,\n            'avg_fee': np.float32, 'avg_gas_used': np.float32, 'creator_buy_vol': np.float32,\n            'creator_sell_vol': np.float32, 'creator_net_vol': np.float32,\n            'creator_buy_ratio_tx': np.float32, 'creator_sell_ratio_tx': np.float32,\n            'first_last_50_ratio': np.float32, 'activity_ratio_10_50': np.float32, 'activity_ratio_20_50': np.float32,\n            'activity_ratio_30_50': np.float32, 'top_5_wallet_concentration': np.float32, 'top_10_wallet_concentration': np.float32,\n            # New Float Features\n            'tx_accel_10_vs_0': np.float32, 'tx_accel_20_vs_10': np.float32,\n            'first_price': np.float64, 'last_price': np.float64, 'price_change_abs': np.float64,\n            'price_change_rel': np.float32, 'price_volatility': np.float32, 'price_velocity': np.float32,\n            'avg_buy_price': np.float64, 'avg_sell_price': np.float64, 'spread_proxy_avg': np.float64,\n            'spread_proxy_extreme': np.float64, 'liquidity_proxy_vol_std': np.float32,\n            'whale_avg_flip_ratio': np.float32, 'whale_sell_proportion': np.float32, 'whale_net_volume_ratio': np.float32\n            }.items():\n            if col in features: features[col] = np.nan_to_num(features[col], nan=0.0).astype(dtype) # Use 0.0 for float NaNs\n\n        # 1. Transaction Clustering (Example)\n        \n        transactions = data['_transactions']\n        if len(transactions) > 5:\n            # Prepare data for clustering (using numpy for speed)\n            transaction_data = np.array([[tx[0], tx[2]] for tx in transactions]) # slot, sol_amount\n            try:\n                dbscan = DBSCAN(eps=50, min_samples=5)  # Adjust eps and min_samples\n                clusters = dbscan.fit_predict(transaction_data)\n                n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0) # Ignore noise\n\n                # Calculate cluster statistics\n                largest_cluster_size = 0\n                if n_clusters > 0:\n                    for cluster_id in range(n_clusters):\n                        cluster_size = np.sum(clusters == cluster_id)\n                        largest_cluster_size = max(largest_cluster_size, cluster_size)\n\n                features['tx_n_clusters'] = n_clusters\n                features['tx_largest_cluster_size'] = largest_cluster_size\n            except Exception as e:\n                print(f\"Clustering error: {e}\")\n                features['tx_n_clusters'] = -1\n                features['tx_largest_cluster_size'] = -1\n        else:\n            features['tx_n_clusters'] = 0\n            features['tx_largest_cluster_size'] = 0\n\n        # 2. Anomalous Volume Spikes\n        volumes = data['_sol_volumes']\n        if len(volumes) > 10:\n            # import pandas as pd\n            df_volumes = pd.DataFrame(volumes, columns=['slot', 'volume'])\n            df_volumes = df_volumes.set_index('slot').sort_index() # Important for correct rolling calc\n\n            window = 10 # define the number of slots\n            volume_ma = df_volumes['volume'].rolling(window=window, min_periods=1).mean()\n            volume_std = df_volumes['volume'].rolling(window=window, min_periods=1).std()\n            last_volume = df_volumes['volume'].iloc[-1]\n            last_ma = volume_ma.iloc[-1]\n            last_std = volume_std.iloc[-1]\n\n            if last_std > 0:\n                features['volume_zscore'] = (last_volume - last_ma) / last_std\n            else:\n                features['volume_zscore'] = 0 # if no std\n\n        else:\n            features['volume_zscore'] = 0\n\n        # 3. Wash Trading (More Robust)\n        wallet_tx_history = data.get('_wallet_tx_history', {})\n        wash_trades = 0\n        for wallet, txs in wallet_tx_history.items():\n            if len(txs) < 2:\n                continue  # Need at least two transactions\n            for i in range(1, len(txs)):\n                direction1, time1, volume1 = txs[i - 1]\n                direction2, time2, volume2 = txs[i]\n                if direction1 != direction2 and abs(time2 - time1) < 20:  # Short time frame\n                   wash_trades += 1\n                   # Add volume check: wash trading usually involves similar amounts\n                   if abs(volume1 - volume2) < (volume1 + volume2) / 10: # vol diff less than 10%\n                     wash_trades += 1\n        features['wash_trades'] = wash_trades\n\n        # 4. Wallet Profiling (Whale volume percentage, first activity)\n        total_volume = features['total_sol_volume']\n        whale_volume = 0\n        num_active_wallets = 0\n\n        for wallet, activity in data['_wallet_activity'].items():\n            wallet_volume = activity['buy_vol'] + activity['sell_vol']\n            if wallet_volume > 0.1 * total_volume: # Consider wallet a whale if over 10%\n                whale_volume += wallet_volume\n            if activity['tx_count'] > 0:\n                num_active_wallets += 1\n\n        if total_volume > 0:\n            features['whale_volume_proportion'] = whale_volume / total_volume\n        else:\n            features['whale_volume_proportion'] = 0\n        features['num_active_wallets'] = num_active_wallets\n\n        earliest_wallet_tx = float('inf')\n        for wallet, activity in data['_wallet_activity'].items():\n           earliest_wallet_tx = min(activity['first_tx'], earliest_wallet_tx) #get earliest first_tx\n\n        if earliest_wallet_tx != float('inf'):\n             features['time_since_first_wallet_tx'] = data['last_slot'] - earliest_wallet_tx\n        else:\n             features['time_since_first_wallet_tx'] = -1\n\n        # Add more wallet features here (number of tokens traded, etc.)\n\n        # Add rate of transaction of top users\n        # Get sorted user wallet\n        wallet_activity = data['_wallet_activity']\n        # Sort wallets by BUY volume to find top buyers\n        top_buyers = sorted(wallet_activity.items(), key=lambda item: item[1]['buy_vol'], reverse=True)[:5]\n\n        total_flip_ratio = 0.0\n        sold_count = 0\n        top_buyer_buy_vol_sum = 0.0\n        top_buyer_sell_vol_sum = 0.0\n\n        if top_buyers: # Check if list is not empty\n            for wallet, activity_data in top_buyers:\n                buy_vol = activity_data['buy_vol']\n                sell_vol = activity_data['sell_vol']\n                top_buyer_buy_vol_sum += buy_vol\n                top_buyer_sell_vol_sum += sell_vol\n                if buy_vol > 1e-9: # Avoid division by zero for flip ratio\n                    flip_ratio = sell_vol / buy_vol\n                    total_flip_ratio += flip_ratio\n                    if sell_vol > 1e-9: # Consider sold if they sold *anything*\n                        sold_count += 1\n\n            features['whale_avg_flip_ratio'] = total_flip_ratio / len(top_buyers)\n            features['whale_sell_proportion'] = sold_count / len(top_buyers) # Proportion of top N buyers who also sold\n            features['whale_net_volume_ratio'] = (top_buyer_buy_vol_sum - top_buyer_sell_vol_sum) / (top_buyer_buy_vol_sum + 1e-9)\n        else:\n            features['whale_avg_flip_ratio'] = 0.0\n            features['whale_sell_proportion'] = 0.0\n            features['whale_net_volume_ratio'] = 0.0\n            features['activity_rate_top_user']=0\n        # 5. Wallet Network Analysis (Requires external data; skipping for now)\n\n\n        feature_list.append(features)\n\n    features_out_df = pd.DataFrame(feature_list)\n    del feature_list, feature_agg # Explicitly delete large intermediate structures\n    gc.collect()\n\n    # --- Merge Metadata (Same as before) ---\n    print(\"  Merging metadata features...\")\n    # Dune Metadata Merge\n    if dune_metadata is not None:\n        dune_meta = dune_metadata[['token_mint_address', 'decimals', 'name', 'symbol', 'token_uri']].rename(columns={'token_mint_address': 'mint'})\n        dune_meta['has_dune_meta'] = 1; dune_meta['has_token_uri'] = dune_meta['token_uri'].notna().astype(np.uint8)\n        dune_meta['name_len'] = dune_meta['name'].str.len().fillna(0).astype(np.uint16); dune_meta['symbol_len'] = dune_meta['symbol'].str.len().fillna(0).astype(np.uint8)\n        features_out_df = features_out_df.merge(dune_meta[['mint', 'decimals', 'has_dune_meta', 'has_token_uri', 'name_len', 'symbol_len']], on='mint', how='left')\n        features_out_df['has_dune_meta'].fillna(0, inplace=True); features_out_df['has_token_uri'].fillna(0, inplace=True); features_out_df['decimals'].fillna(-1, inplace=True)\n        features_out_df['name_len'].fillna(0, inplace=True); features_out_df['symbol_len'].fillna(0, inplace=True)\n        features_out_df = features_out_df.astype({ 'has_dune_meta': np.uint8, 'has_token_uri': np.uint8, 'decimals': np.int8, 'name_len': np.uint16, 'symbol_len': np.uint8 })\n        del dune_meta; gc.collect(); print(f\"    Merged Dune metadata. Shape: {features_out_df.shape}\")\n    else:\n        features_out_df = features_out_df.assign(has_dune_meta=np.uint8(0), has_token_uri=np.uint8(0), decimals=np.int8(-1), name_len=np.uint16(0), symbol_len=np.uint8(0))\n\n    # Onchain Metadata Merge\n    if onchain_metadata is not None:\n        onchain_meta = onchain_metadata[['mint', 'bundle_size', 'gas_used']].copy()\n        onchain_meta.rename(columns={'gas_used': 'creation_gas_used'}, inplace=True)\n        onchain_meta['has_onchain_meta'] = 1\n        features_out_df = features_out_df.merge(onchain_meta[['mint', 'bundle_size', 'creation_gas_used', 'has_onchain_meta']], on='mint', how='left')\n        features_out_df['has_onchain_meta'].fillna(0, inplace=True); features_out_df['bundle_size'].fillna(0, inplace=True)\n        median_gas = features_out_df['creation_gas_used'].median()\n        features_out_df['creation_gas_used'].fillna(median_gas if pd.notna(median_gas) else 0, inplace=True)\n        features_out_df = features_out_df.astype({ 'has_onchain_meta': np.uint8, 'bundle_size': np.uint16, 'creation_gas_used': np.float32 })\n        del onchain_meta; gc.collect(); print(f\"    Merged Onchain metadata. Shape: {features_out_df.shape}\")\n    else:\n        features_out_df = features_out_df.assign(has_onchain_meta=np.uint8(0), bundle_size=np.uint16(0), creation_gas_used=np.float32(0))\n\n    print(f\"  Final feature set shape: {features_out_df.shape}\")\n    return features_out_df\n\n\n# Cell 4: Feature Engineering Execution & Saving\nprint(\"Cell 4: Running memory-optimized feature engineering...\")\nif RUN_FEATURE_ENGINEERING or not os.path.exists(FEATURES_PKL):\n    CHUNK_FILES = sorted(glob.glob(os.path.join(DATA_DIR, 'chunk_*.csv')))\n    if not CHUNK_FILES: print(f\"Error: No chunk*.csv files found in {DATA_DIR}.\"); exit()\n    else: print(f\"Found {len(CHUNK_FILES)} chunk files in {DATA_DIR}.\")\n\n    features_df = generate_chunk_features_optimized(\n        CHUNK_FILES, train_df, test_df, DUNE_INFO_FILE, ONCHAIN_INFO_FILE,\n        BLOCK_WINDOW, CHUNKSIZE, WHALE_TOP_N\n    )\n    if features_df.empty or features_df.shape[0] < 1: print(\"Error: Feature generation resulted in empty DF.\"); exit()\n    print(\"Saving features to pickle file...\")\n    features_df.to_pickle(FEATURES_PKL); print(f\"Features saved to {FEATURES_PKL}\")\nelse:\n    print(f\"Loading features from {FEATURES_PKL}...\"); features_df = pd.read_pickle(FEATURES_PKL); print(\"Features loaded.\")\n\nprint(\"\\n--- Final Features Info ---\")\nprint(f\"Shape: {features_df.shape}\")\n# print(features_df.head()) # Uncomment to view head\n# features_df.info(memory_usage='deep') # Uncomment for detailed info\n# print(\"\\nFeature columns:\", features_df.columns.tolist()) # Uncomment to see all feature names\nprint(f\"Number of features generated: {features_df.shape[1] - 1}\") # Subtract 1 for 'mint' column\ngc.collect()\n\n# Cell 5: Data Preparation for Model (Same as before)\nprint(\"Cell 5: Preparing data for modeling...\")\ntrain_merged = train_df.merge(features_df, on='mint', how='left')\ntest_merged = test_df.merge(features_df, on='mint', how='left')\ntest_mints_final = test_merged['mint'].copy()\ntest_mints_final.to_pickle(TEST_MINTS_PKL) # Save test mints\nfeature_cols = [col for col in features_df.columns if col not in ['mint']]\nX = train_merged[feature_cols].copy() # Use copy to avoid SettingWithCopy issues later\ny = train_merged['has_graduated'].astype(int).copy()\nX_test = test_merged[feature_cols].copy()\n\nprint(f\"Training data shape (X): {X.shape}\"); print(f\"Target data shape (y): {y.shape}\"); print(f\"Test data shape (X_test): {X_test.shape}\")\nprint(f\"Number of features: {len(feature_cols)}\")\ndel train_df, test_df, train_merged, test_merged, features_df; gc.collect()\n\n\n# Cell 6: Preprocessing (Imputation) & Saving Processed Data\nprint(\"Cell 6: Preprocessing data (Imputation)...\")\nprint(f\"NaNs in X before imputation: {X.isna().sum().sum()}\"); print(f\"NaNs in X_test before imputation: {X_test.isna().sum().sum()}\")\nX.replace([np.inf, -np.inf], np.nan, inplace=True); X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\nprint(f\"NaNs in X after replacing inf: {X.isna().sum().sum()}\"); print(f\"NaNs in X_test after replacing inf: {X_test.isna().sum().sum()}\")\n\n# Check if any columns are ALL NaN before imputation (imputer fails on these)\ncols_all_nan_X = X.columns[X.isna().all()].tolist()\ncols_all_nan_X_test = X_test.columns[X_test.isna().all()].tolist()\nif cols_all_nan_X: print(f\"Warning: Columns in X are all NaN: {cols_all_nan_X}\")\nif cols_all_nan_X_test: print(f\"Warning: Columns in X_test are all NaN: {cols_all_nan_X_test}\")\n# Optional: Drop or fill these columns specifically if they occur\n# X.drop(columns=cols_all_nan_X, inplace=True)\n# X_test.drop(columns=cols_all_nan_X_test, inplace=True)\n# feature_cols = [col for col in feature_cols if col not in cols_all_nan_X] # Adjust feature list\n\n\nimputer = SimpleImputer(strategy='median')\ntry:\n    # Check for object columns - imputer works only on numeric\n    numeric_cols_X = X.select_dtypes(include=np.number).columns\n    numeric_cols_X_test = X_test.select_dtypes(include=np.number).columns\n    non_numeric_X = X.select_dtypes(exclude=np.number).columns\n    non_numeric_X_test = X_test.select_dtypes(exclude=np.number).columns\n\n    if len(non_numeric_X) > 0: print(f\"Warning: Non-numeric columns found in X: {non_numeric_X.tolist()}. Imputer will skip them.\")\n    if len(non_numeric_X_test) > 0: print(f\"Warning: Non-numeric columns found in X_test: {non_numeric_X_test.tolist()}. Imputer will skip them.\")\n\n\n    X_imputed = X.copy() # Create copy to modify\n    X_test_imputed = X_test.copy()\n\n    # Fit on numeric training data and transform both train and test\n    imputer.fit(X[numeric_cols_X])\n    X_imputed[numeric_cols_X] = imputer.transform(X[numeric_cols_X])\n    X_test_imputed[numeric_cols_X_test] = imputer.transform(X_test[numeric_cols_X_test]) # Use numeric cols from test\n\n    print(\"Imputation successful.\")\n    X = pd.DataFrame(X_imputed, columns=X.columns); X_test = pd.DataFrame(X_test_imputed, columns=X_test.columns)\n    # Re-apply original dtypes if necessary after imputation (optional, usually fine)\n\n    print(f\"NaNs in X after imputation: {X.isna().sum().sum()}\"); print(f\"NaNs in X_test after imputation: {X_test.isna().sum().sum()}\")\n    # Final check for any remaining NaNs (shouldn't happen with median unless a col was all NaN)\n    if X.isna().sum().sum() > 0 or X_test.isna().sum().sum() > 0:\n        print(\"Warning: NaNs remain after imputation. Check data or imputation strategy.\")\n        print(\"X NaNs per column:\\n\", X.isna().sum()[X.isna().sum() > 0])\n        print(\"X_test NaNs per column:\\n\", X_test.isna().sum()[X_test.isna().sum() > 0])\n\n    print(\"Saving processed data...\"); X.to_pickle(X_TRAIN_PKL); y.to_pickle(Y_TRAIN_PKL); X_test.to_pickle(X_TEST_PKL); print(\"Processed data saved.\")\nexcept Exception as e: print(f\"Error during imputation: {e}\"); traceback.print_exc(); exit()\ndel X_imputed, X_test_imputed; gc.collect()\n\nprint(\"\\nScript finished.\")\n\n# Cell 7: Ensemble Model Training & Prediction (Modified)\nprint(\"Cell 7: Training Ensemble Models with CV...\")\n\n# Load processed data if not already in memory\nif 'X' not in globals() or 'y' not in globals() or 'X_test' not in globals():\n    print(\"Reloading processed data for final training...\")\n    X = pd.read_pickle(X_TRAIN_PKL)\n    y = pd.read_pickle(Y_TRAIN_PKL)\n    X_test = pd.read_pickle(X_TEST_PKL)\n\nkf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n\n# Initialize OOF and Test prediction arrays for each model\noof_lgbm = np.zeros(len(X))\noof_xgb = np.zeros(len(X))\noof_cat = np.zeros(len(X))\ntest_lgbm_agg = np.zeros(len(X_test))\ntest_xgb_agg = np.zeros(len(X_test))\ntest_cat_agg = np.zeros(len(X_test))\n\n# Store scores and importances per model\nscores_lgbm, scores_xgb, scores_cat = [], [], []\nimportances_lgbm = pd.DataFrame(index=feature_cols)\nimportances_xgb = pd.DataFrame(index=feature_cols)\nimportances_cat = pd.DataFrame(index=feature_cols)\n\n\n# --- Final Cross-validation loop for Ensemble---\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n    print(f\"\\n--- Fold {fold+1}/{N_SPLITS} ---\")\n    X_tr, y_tr = X.iloc[train_idx], y.iloc[train_idx]\n    X_va, y_va = X.iloc[val_idx], y.iloc[val_idx]\n\n    # --- LightGBM ---\n    print(\"Training LightGBM...\")\n    lgbm_model = lgb.LGBMClassifier(**LGBM_BEST_PARAMS)\n    lgbm_model.fit(X_tr, y_tr,\n                    eval_set=[(X_va, y_va)],\n                    eval_metric='logloss',\n                    callbacks=[lgb.early_stopping(EARLY_STOPPING_ROUNDS, verbose=100)])\n    best_iter_lgbm = lgbm_model.best_iteration_ if lgbm_model.best_iteration_ else LGBM_BEST_PARAMS['n_estimators']\n    val_preds_lgbm = lgbm_model.predict_proba(X_va)[:, 1]\n    test_preds_lgbm = lgbm_model.predict_proba(X_test, num_iteration=best_iter_lgbm)[:, 1]\n    # Clip predictions\n    val_preds_lgbm = np.clip(val_preds_lgbm, 1e-15, 1 - 1e-15)\n    test_preds_lgbm = np.clip(test_preds_lgbm, 1e-15, 1 - 1e-15)\n    # Store predictions and score\n    oof_lgbm[val_idx] = val_preds_lgbm\n    test_lgbm_agg += test_preds_lgbm / N_SPLITS\n    fold_score_lgbm = log_loss(y_va, val_preds_lgbm)\n    scores_lgbm.append(fold_score_lgbm)\n    importances_lgbm[f'Fold_{fold+1}'] = lgbm_model.feature_importances_\n    print(f\"LGBM Fold {fold+1} LogLoss: {fold_score_lgbm:.6f} (Best iter: {best_iter_lgbm})\")\n    del lgbm_model; gc.collect() # Cleanup model\n\n\n    # --- XGBoost ---\n    print(\"Training XGBoost...\")\n    # Note: XGBoost early stopping uses eval_set parameter directly in fit\n    xgb_model = xgb.XGBClassifier(**XGB_PARAMS, n_estimators=XGB_N_ESTIMATORS, use_label_encoder=False)\n    xgb_model.fit(X_tr, y_tr,\n                    eval_set=[(X_va, y_va)],\n                    early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n                    verbose=False) # verbose=False keeps output clean\n    best_iter_xgb = xgb_model.best_iteration if hasattr(xgb_model, 'best_iteration') else XGB_N_ESTIMATORS # Get best iteration if early stopping triggered\n    val_preds_xgb = xgb_model.predict_proba(X_va)[:, 1]\n    test_preds_xgb = xgb_model.predict_proba(X_test)[:, 1] # XGB uses best iteration by default if stopped early\n    # Clip predictions\n    val_preds_xgb = np.clip(val_preds_xgb, 1e-15, 1 - 1e-15)\n    test_preds_xgb = np.clip(test_preds_xgb, 1e-15, 1 - 1e-15)\n    # Store predictions and score\n    oof_xgb[val_idx] = val_preds_xgb\n    test_xgb_agg += test_preds_xgb / N_SPLITS\n    fold_score_xgb = log_loss(y_va, val_preds_xgb)\n    scores_xgb.append(fold_score_xgb)\n    importances_xgb[f'Fold_{fold+1}'] = xgb_model.feature_importances_\n    print(f\"XGB Fold {fold+1} LogLoss: {fold_score_xgb:.6f} (Best iter: {best_iter_xgb})\")\n    del xgb_model; gc.collect()\n\n\n    # --- CatBoost ---\n    print(\"Training CatBoost...\")\n    cat_model = cb.CatBoostClassifier(**CAT_PARAMS, iterations=CAT_ITERATIONS)\n    cat_model.fit(X_tr, y_tr,\n                    eval_set=[(X_va, y_va)],\n                    # early_stopping_rounds handled by CAT_PARAMS\n                    verbose=0) # verbose=0 keeps output clean\n    best_iter_cat = cat_model.best_iteration_ if hasattr(cat_model, 'best_iteration_') and cat_model.best_iteration_ is not None else CAT_ITERATIONS\n    val_preds_cat = cat_model.predict_proba(X_va)[:, 1]\n    test_preds_cat = cat_model.predict_proba(X_test)[:, 1] # Predicts using best iteration by default\n    # Clip predictions\n    val_preds_cat = np.clip(val_preds_cat, 1e-15, 1 - 1e-15)\n    test_preds_cat = np.clip(test_preds_cat, 1e-15, 1 - 1e-15)\n    # Store predictions and score\n    oof_cat[val_idx] = val_preds_cat\n    test_cat_agg += test_preds_cat / N_SPLITS\n    fold_score_cat = log_loss(y_va, val_preds_cat)\n    scores_cat.append(fold_score_cat)\n    importances_cat[f'Fold_{fold+1}'] = cat_model.get_feature_importance()\n    print(f\"CAT Fold {fold+1} LogLoss: {fold_score_cat:.6f} (Best iter: {best_iter_cat})\")\n    del cat_model; gc.collect()\n\n    del X_tr, y_tr, X_va, y_va # Clean up fold data\n    gc.collect()\n\n# --- Calculate Final Scores and Display Info ---\nprint(f\"\\n--- Cross-Validation Summary ---\")\nprint(f\"LGBM Mean CV LogLoss: {np.mean(scores_lgbm):.6f} +/- {np.std(scores_lgbm):.6f}\")\nprint(f\"XGB  Mean CV LogLoss: {np.mean(scores_xgb):.6f} +/- {np.std(scores_xgb):.6f}\")\nprint(f\"CAT  Mean CV LogLoss: {np.mean(scores_cat):.6f} +/- {np.std(scores_cat):.6f}\")\n\n# Calculate OOF score for each model\noof_score_lgbm = log_loss(y, oof_lgbm)\noof_score_xgb = log_loss(y, oof_xgb)\noof_score_cat = log_loss(y, oof_cat)\nprint(f\"\\nOverall OOF LGBM LogLoss: {oof_score_lgbm:.6f}\")\nprint(f\"Overall OOF XGB  LogLoss: {oof_score_xgb:.6f}\")\nprint(f\"Overall OOF CAT  LogLoss: {oof_score_cat:.6f}\")\n\n# Calculate Simple Average Ensemble OOF Score\noof_ensemble = (oof_lgbm + oof_xgb + oof_cat) / 3.0\noof_score_ensemble = log_loss(y, oof_ensemble)\nprint(f\"\\nOverall OOF ENSEMBLE (Avg) LogLoss: {oof_score_ensemble:.6f}\")\n\n\n# Cell 8: Submission File Creation (Using Ensemble Predictions)\nprint(\"Cell 8: Creating ensemble submission file...\")\n# --- Create Submission File ---\nprint(\"\\nCreating submission file...\")\n# Load test mints if needed\nif 'test_mints_final' not in globals():\n    test_mints_final = pd.read_pickle(TEST_MINTS_PKL)\n\n# Simple Average Ensemble for test predictions\nensemble_test_predictions = (test_lgbm_agg + test_xgb_agg + test_cat_agg) / 3.0\n\nsubmission_df = pd.DataFrame({\n    'mint': test_mints_final,\n    'has_graduated': ensemble_test_predictions # Use ensembled predictions\n})\n\nprint(f\"Test data shape (X_test): {X_test.shape}\")\nprint(f\"Submission DataFrame shape: {submission_df.shape}\")\nassert submission_df.shape[0] == X_test.shape[0], \\\n    f\"Submission rows ({submission_df.shape[0]}) != Test rows ({X_test.shape[0]})\"\n\nsubmission_df.to_csv(SUBMISSION_FILE, index=False, float_format='%.8f')\n\nprint(f\"Submission file created successfully at: {SUBMISSION_FILE}\")\nprint(\"Sample submission:\")\nprint(submission_df.head())\n\n\n# Cell 9: Feature Importance Display (Per Model)\nprint(\"Cell 9: Displaying feature importance for each model...\")\n\ndef display_feature_importance(importances_df, model_name):\n    try:\n        importances_df['mean_importance'] = importances_df.mean(axis=1)\n        importances_df = importances_df.sort_values('mean_importance', ascending=False)\n        print(f\"\\n--- Top 15 Feature Importances ({model_name}) ---\")\n        with pd.option_context('display.max_rows', 15):\n            print(importances_df[['mean_importance']].head(15))\n    except Exception as e:\n        print(f\"Error displaying feature importance for {model_name}: {e}\")\n\ndisplay_feature_importance(importances_lgbm, \"LightGBM\")\ndisplay_feature_importance(importances_xgb, \"XGBoost\")\ndisplay_feature_importance(importances_cat, \"CatBoost\")\n\nprint(\"\\n--- Notebook execution completed ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T08:55:24.706218Z","iopub.execute_input":"2025-06-26T08:55:24.706484Z","execution_failed":"2025-06-26T10:57:03.401Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"},{"name":"stdout","text":"Cell 2: Loading base data...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_10/857248932.py:113: DtypeWarning: Columns (1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n  try: onchain_df = pd.read_csv(ONCHAIN_INFO_FILE)\n","output_type":"stream"},{"name":"stdout","text":"Loaded 639557 training examples and 478832 test examples.\nCell 3: Defining memory-optimized feature engineering function with enhancements...\nCell 4: Running memory-optimized feature engineering...\nFound 41 chunk files in /kaggle/input/pump-fun-graduation-february-2025.\n  Combining mint info...\n  Total unique mints to process: 1118389\n  Starting feature aggregation from chunks...\n  Creator map created with 306001 entries.\n","output_type":"stream"},{"name":"stderr","text":"  Processing Chunks:  98%|█████████▊| 40/41 [1:57:49<03:09, 189.36s/it]  Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x79a3e680dcc0>>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 781, in _clean_thread_parent_frames\n    def _clean_thread_parent_frames(\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 600, in sigint_handler\n    raise KeyboardInterrupt\nKeyboardInterrupt: \n  Processing Chunks: 100%|██████████| 41/41 [2:01:08<00:00, 177.29s/it]\n","output_type":"stream"},{"name":"stdout","text":"  Loading metadata for final merge...\n","output_type":"stream"}],"execution_count":null}]}
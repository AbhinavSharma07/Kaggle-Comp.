{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5282b21c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T07:36:12.048075Z",
     "iopub.status.busy": "2025-06-16T07:36:12.047754Z",
     "iopub.status.idle": "2025-06-16T07:36:13.920303Z",
     "shell.execute_reply": "2025-06-16T07:36:13.919513Z"
    },
    "papermill": {
     "duration": 1.877229,
     "end_time": "2025-06-16T07:36:13.922034",
     "exception": false,
     "start_time": "2025-06-16T07:36:12.044805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # References\n",
    "# \n",
    "# https://www.kaggle.com/code/conjuring92/pii-metric-fine-grained-eval\n",
    "# \n",
    "# Modified to use F1 score\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "class PRFScore:\n",
    "    \"\"\"A precision / recall / F score.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        tp: int = 0,\n",
    "        fp: int = 0,\n",
    "        fn: int = 0,\n",
    "    ) -> None:\n",
    "        self.tp = tp\n",
    "        self.fp = fp\n",
    "        self.fn = fn\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.tp + self.fp + self.fn\n",
    "\n",
    "    def __iadd__(self, other):  # in-place add\n",
    "        self.tp += other.tp\n",
    "        self.fp += other.fp\n",
    "        self.fn += other.fn\n",
    "        return self\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return PRFScore(\n",
    "            tp=self.tp + other.tp, fp=self.fp + other.fp, fn=self.fn + other.fn\n",
    "        )\n",
    "\n",
    "    def score_set(self, cand: set, gold: set) -> None:\n",
    "        self.tp += len(cand.intersection(gold))\n",
    "        self.fp += len(cand - gold)\n",
    "        self.fn += len(gold - cand)\n",
    "\n",
    "    @property\n",
    "    def precision(self) -> float:\n",
    "        return self.tp / (self.tp + self.fp + 1e-100)\n",
    "\n",
    "    @property\n",
    "    def recall(self) -> float:\n",
    "        return self.tp / (self.tp + self.fn + 1e-100)\n",
    "\n",
    "    @property\n",
    "    def f1(self) -> float:\n",
    "        p = self.precision\n",
    "        r = self.recall\n",
    "        return 2 * ((p * r) / (p + r + 1e-100))\n",
    "\n",
    "    def to_dict(self) -> Dict[str, float]:\n",
    "        return {\"p\": self.precision, \"r\": self.recall, \"f1\": self.f1}\n",
    "\n",
    "\n",
    "def compute_metrics(pred_df, gt_df):\n",
    "    \"\"\"\n",
    "    Compute the LB metric (lb) and other auxiliary metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    references = {(row.dataset_id, row.type) for row in gt_df.itertuples()}\n",
    "    predictions = {(row.dataset_id, row.type) for row in pred_df.itertuples()}\n",
    "\n",
    "    score_per_type = defaultdict(PRFScore)\n",
    "    references = set(references)\n",
    "\n",
    "    for ex in predictions:\n",
    "        pred_type = ex[-1] # (dataset_id, type)\n",
    "            \n",
    "        if pred_type not in score_per_type:\n",
    "            score_per_type[pred_type] = PRFScore()\n",
    "\n",
    "        if ex in references:\n",
    "            score_per_type[pred_type].tp += 1\n",
    "            references.remove(ex)\n",
    "        else:\n",
    "            score_per_type[pred_type].fp += 1\n",
    "\n",
    "    for _, ref_type in references:\n",
    "        \n",
    "        if ref_type not in score_per_type:\n",
    "            score_per_type[ref_type] = PRFScore()\n",
    "        score_per_type[ref_type].fn += 1\n",
    "\n",
    "    totals = PRFScore()\n",
    "    \n",
    "    for prf in score_per_type.values():\n",
    "        totals += prf\n",
    "\n",
    "    return {\n",
    "        \"ents_p\": totals.precision,\n",
    "        \"ents_r\": totals.recall,\n",
    "        \"ents_f1\": totals.f1,\n",
    "        \"ents_per_type\": {k: v.to_dict() for k, v in score_per_type.items()},\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.801555,
   "end_time": "2025-06-16T07:36:14.442473",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-16T07:36:07.640918",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444d4443",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T06:55:06.934837Z",
     "iopub.status.busy": "2025-06-16T06:55:06.934604Z",
     "iopub.status.idle": "2025-06-16T06:58:31.660509Z",
     "shell.execute_reply": "2025-06-16T06:58:31.659759Z"
    },
    "papermill": {
     "duration": 204.730455,
     "end_time": "2025-06-16T06:58:31.662434",
     "exception": false,
     "start_time": "2025-06-16T06:55:06.931979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7ab0c83b6010>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pypdf2/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7ab0c83df050>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pypdf2/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7ab0c8301510>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pypdf2/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7ab0c82ed050>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pypdf2/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7ab0c83f9490>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pypdf2/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement PyPDF2 (from versions: none)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for PyPDF2\u001b[0m\u001b[31m\r\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a867653",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T06:58:31.667910Z",
     "iopub.status.busy": "2025-06-16T06:58:31.667374Z",
     "iopub.status.idle": "2025-06-16T06:58:40.291160Z",
     "shell.execute_reply": "2025-06-16T06:58:40.290320Z"
    },
    "papermill": {
     "duration": 8.627826,
     "end_time": "2025-06-16T06:58:40.292496",
     "exception": false,
     "start_time": "2025-06-16T06:58:31.664670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ pandas is installed.\n",
      "✅ numpy is installed.\n",
      "✅ spacy is installed.\n",
      "❌ scikit-learn is NOT installed.\n",
      "✅ joblib is installed.\n",
      "❌ PyPDF2 is NOT installed.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "# List of packages to check\n",
    "packages = [\"pandas\", \"numpy\", \"spacy\", \"scikit-learn\", \"joblib\", \"PyPDF2\"]\n",
    "\n",
    "# Check each package\n",
    "for package in packages:\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "        print(f\"✅ {package} is installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"❌ {package} is NOT installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91f72073",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T06:58:40.298103Z",
     "iopub.status.busy": "2025-06-16T06:58:40.297612Z",
     "iopub.status.idle": "2025-06-16T07:02:34.559590Z",
     "shell.execute_reply": "2025-06-16T07:02:34.558789Z"
    },
    "papermill": {
     "duration": 234.266091,
     "end_time": "2025-06-16T07:02:34.560760",
     "exception": false,
     "start_time": "2025-06-16T06:58:40.294669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Generating training samples (with negative sampling)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [03:22<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2330 training samples.\n",
      "Not_A_Citation    2187\n",
      "Primary             79\n",
      "Secondary           64\n",
      "Name: count, dtype: int64\n",
      "Cross-val F1: 0.9583 ± 0.0017\n",
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:13<00:00,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5 predictions to submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    VotingClassifier,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    nlp = None\n",
    "    print(\n",
    "        \"Run `python -m spacy download en_core_web_sm` to enable NLP features.\"\n",
    "    )\n",
    "\n",
    "try:\n",
    "    import fitz  # PyMuPDF\n",
    "except ImportError:\n",
    "    fitz = None\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x: x  # fallback\n",
    "\n",
    "\n",
    "class DataCitationDetector:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = DictVectorizer(sparse=False)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.nlp = nlp\n",
    "\n",
    "        self.classifier = VotingClassifier(\n",
    "            estimators=[\n",
    "                (\n",
    "                    \"rf\",\n",
    "                    RandomForestClassifier(\n",
    "                        n_estimators=200,\n",
    "                        max_depth=10,\n",
    "                        class_weight=\"balanced\",\n",
    "                        random_state=42,\n",
    "                    ),\n",
    "                ),\n",
    "                (\n",
    "                    \"gb\",\n",
    "                    GradientBoostingClassifier(\n",
    "                        n_estimators=100, max_depth=6, random_state=42\n",
    "                    ),\n",
    "                ),\n",
    "                (\n",
    "                    \"lr\",\n",
    "                    LogisticRegression(\n",
    "                        max_iter=1000,\n",
    "                        class_weight=\"balanced\",\n",
    "                        random_state=42,\n",
    "                    ),\n",
    "                ),\n",
    "            ],\n",
    "            voting=\"soft\",\n",
    "        )\n",
    "\n",
    "        self.primary_patterns = [\n",
    "            r\"\\b(?:we\\s+)?(?:generated|collected|produced|created|measured|recorded|obtained|acquired)\\b\",\n",
    "            r\"\\bthis\\s+study\\b\",\n",
    "            r\"\\bour\\s+(?:data|dataset|measurements|results)\\b\",\n",
    "            r\"\\bnew\\s+data\\b\",\n",
    "            r\"\\boriginal\\s+data\\b\",\n",
    "            r\"\\bnewly\\s+(?:generated|collected)\\b\",\n",
    "            r\"\\bspecifically\\s+(?:for\\s+)?this\\s+(?:study|work)\\b\",\n",
    "            r\"\\bin-house\\s+(?:generated|created)\\b\",\n",
    "        ]\n",
    "\n",
    "        self.secondary_patterns = [\n",
    "            r\"\\bobtained\\s+from\\b\",\n",
    "            r\"\\breused?\\b\",\n",
    "            r\"\\bexisting\\s+data\\b\",\n",
    "            r\"\\bpreviously\\s+(?:published|reported)\\b\",\n",
    "            r\"\\bderived\\s+from\\b\",\n",
    "            r\"\\bopen\\s+data\\b\",\n",
    "            r\"\\bthird-party\\s+data\\b\",\n",
    "            r\"\\bbenchmark\\s+data\\b\",\n",
    "        ]\n",
    "\n",
    "        self.section_patterns = {\n",
    "            \"methods\": r\"\\bmethods?\\b\",\n",
    "            \"results\": r\"\\bresults?\\b\",\n",
    "            \"introduction\": r\"\\bintroduction\\b\",\n",
    "            \"discussion\": r\"\\bdiscussion\\b\",\n",
    "            \"references\": r\"\\breferences?\\b\",\n",
    "            \"data_availability\": r\"\\bdata\\s+availability\\b\",\n",
    "        }\n",
    "\n",
    "        self.citation_patterns = {\n",
    "            \"doi\": r\"(?:https?://)?(?:dx\\.)?doi\\.org/(10\\.\\d{4,9}/[\\w./-]+)\",\n",
    "            \"zenodo\": r\"(?:https?://)?zenodo\\.org/record/(\\d+)\",\n",
    "            \"github\": r\"(?:https?://)?github\\.com/[\\w\\-_]+/[\\w\\-_]+\",\n",
    "            \"gse\": r\"GSE\\d+\",\n",
    "            \"sra\": r\"SRA\\d+\",\n",
    "            \"prjna\": r\"PRJNA\\d+\",\n",
    "            \"chembl\": r\"CHEMBL\\d+\",\n",
    "            \"pdb\": r\"PDB:\\w+\",\n",
    "            \"uniprot\": r\"UniProt:\\w+\",\n",
    "        }\n",
    "\n",
    "    def extract_text_from_xml(self, xml_path):\n",
    "        try:\n",
    "            tree = ET.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "            return \" \".join(\n",
    "                elem.text.strip() for elem in root.iter() if elem.text\n",
    "            )\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        try:\n",
    "            if fitz:\n",
    "                doc = fitz.open(pdf_path)\n",
    "                text = \"\".join([page.get_text() for page in doc])\n",
    "                doc.close()\n",
    "                return text\n",
    "        except:\n",
    "            return \"\"\n",
    "        return \"\"\n",
    "\n",
    "    def load_documents(self, base_path, split=\"train\"):\n",
    "        documents = {}\n",
    "        for xml_file in Path(base_path, split, \"XML\").glob(\"*.xml\"):\n",
    "            documents[xml_file.stem] = self.extract_text_from_xml(xml_file)\n",
    "        for pdf_file in Path(base_path, split, \"PDF\").glob(\"*.pdf\"):\n",
    "            if pdf_file.stem not in documents:\n",
    "                documents[pdf_file.stem] = self.extract_text_from_pdf(\n",
    "                    pdf_file\n",
    "                )\n",
    "        return documents\n",
    "\n",
    "    def extract_data_citations(self, text):\n",
    "        citations = set()\n",
    "        for name, pattern in self.citation_patterns.items():\n",
    "            for match in re.findall(pattern, text, re.IGNORECASE):\n",
    "                if name == \"doi\":\n",
    "                    # Normalize DOI by removing trailing characters that are not part of it\n",
    "                    clean_match = match.rstrip(\".,;)\")\n",
    "                    citations.add(f\"https://doi.org/{clean_match}\")\n",
    "                elif name == \"zenodo\":\n",
    "                    citations.add(f\"https://zenodo.org/record/{match}\")\n",
    "                else:\n",
    "                    citations.add(match.strip().rstrip(\".,;)\"))\n",
    "        return list(citations)\n",
    "\n",
    "    def create_features(self, text, citation):\n",
    "        features = {}\n",
    "        text_lower = text.lower()\n",
    "        # Normalize citation for matching\n",
    "        citation_lower = citation.lower().replace(\"https://doi.org/\", \"\")\n",
    "        citation_escaped = re.escape(citation_lower)\n",
    "\n",
    "        context_match = re.search(\n",
    "            f\".{{0,500}}{citation_escaped}.{{0,500}}\", text_lower\n",
    "        )\n",
    "        context = context_match.group() if context_match else text_lower\n",
    "\n",
    "        features[\"text_length\"] = len(text)\n",
    "        features[\"citation_count\"] = text_lower.count(citation_lower)\n",
    "\n",
    "        for i, pattern in enumerate(self.primary_patterns):\n",
    "            features[f\"primary_{i}\"] = len(\n",
    "                re.findall(pattern, context, re.IGNORECASE)\n",
    "            )\n",
    "        for i, pattern in enumerate(self.secondary_patterns):\n",
    "            features[f\"secondary_{i}\"] = len(\n",
    "                re.findall(pattern, context, re.IGNORECASE)\n",
    "            )\n",
    "\n",
    "        for sec, sec_pattern in self.section_patterns.items():\n",
    "            features[f\"in_{sec}\"] = int(bool(re.search(sec_pattern, context)))\n",
    "\n",
    "        features[\"is_doi\"] = int(\"doi.org\" in citation.lower())\n",
    "        features[\"is_github\"] = int(\"github.com\" in citation.lower())\n",
    "        features[\"is_database_id\"] = int(\n",
    "            any(\n",
    "                x in citation.lower()\n",
    "                for x in [\"chembl\", \"gse\", \"sra\", \"prjna\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        features[\"near_figure\"] = len(\n",
    "            re.findall(r\"fig(?:ure)?\\s*\\d+\", context, re.IGNORECASE)\n",
    "        )\n",
    "        features[\"near_table\"] = len(\n",
    "            re.findall(r\"table\\s*\\d+\", context, re.IGNORECASE)\n",
    "        )\n",
    "        features[\"near_supplement\"] = len(\n",
    "            re.findall(r\"supplement\", context, re.IGNORECASE)\n",
    "        )\n",
    "\n",
    "        if self.nlp:\n",
    "            doc = self.nlp(context[:1000])\n",
    "            features[\"num_entities\"] = len(doc.ents)\n",
    "        else:\n",
    "            features[\"num_entities\"] = 0\n",
    "\n",
    "        return features\n",
    "\n",
    "    def train_model(self, base_path):\n",
    "        print(\"Training model...\")\n",
    "        documents = self.load_documents(base_path, \"train\")\n",
    "        labels_df = pd.read_csv(Path(base_path) / \"train_labels.csv\")\n",
    "\n",
    "        \n",
    "        # Create a lookup for true labels for faster access\n",
    "        true_labels = {}\n",
    "        for _, row in labels_df.iterrows():\n",
    "            article_id = row[\"article_id\"]\n",
    "            dataset_id = row[\"dataset_id\"]\n",
    "            citation_type = row[\"type\"]\n",
    "            if article_id not in true_labels:\n",
    "                true_labels[article_id] = {}\n",
    "            true_labels[article_id][dataset_id] = citation_type\n",
    "\n",
    "        X_dicts, y = [], []\n",
    "        print(\"Generating training samples (with negative sampling)...\")\n",
    "        for article_id, text in tqdm(documents.items()):\n",
    "            # Skip if this article_id is not in our labels file\n",
    "            if article_id not in true_labels:\n",
    "                continue\n",
    "\n",
    "            # Find all potential citations in the text\n",
    "            found_citations = self.extract_data_citations(text)\n",
    "            true_citations_for_article = true_labels.get(article_id, {})\n",
    "\n",
    "            for citation in found_citations:\n",
    "                features = self.create_features(text, citation)\n",
    "                X_dicts.append(features)\n",
    "\n",
    "                # Check if this found citation is a true label\n",
    "                if citation in true_citations_for_article:\n",
    "                    # It's a true positive, use the real label\n",
    "                    y.append(true_citations_for_article[citation])\n",
    "                else:\n",
    "                    # It's a false positive, label it as \"Not_A_Citation\"\n",
    "                    y.append(\"Not_A_Citation\")\n",
    "\n",
    "        if not X_dicts:\n",
    "            print(\"No training data found.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Generated {len(X_dicts)} training samples.\")\n",
    "        print(pd.Series(y).value_counts())\n",
    "\n",
    "        X = self.vectorizer.fit_transform(X_dicts)\n",
    "        y_enc = self.label_encoder.fit_transform(y)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        scores = []\n",
    "        for train_idx, val_idx in skf.split(X, y_enc):\n",
    "            self.classifier.fit(X[train_idx], y_enc[train_idx])\n",
    "            preds = self.classifier.predict(X[val_idx])\n",
    "            scores.append(f1_score(y_enc[val_idx], preds, average=\"weighted\"))\n",
    "\n",
    "        print(f\"Cross-val F1: {np.mean(scores):.4f} ± {np.std(scores):.4f}\")\n",
    "        self.classifier.fit(X, y_enc)\n",
    "\n",
    "    def predict(self, base_path):\n",
    "        print(\"Generating predictions...\")\n",
    "        documents = self.load_documents(base_path, \"test\")\n",
    "        predictions = []\n",
    "        row_id = 0\n",
    "\n",
    "        for article_id, text in tqdm(documents.items()):\n",
    "            citations = self.extract_data_citations(text)\n",
    "            for citation in citations:\n",
    "                features = self.create_features(text, citation)\n",
    "                X = self.vectorizer.transform([features])\n",
    "                pred_encoded = self.classifier.predict(X)[0]\n",
    "                citation_type = self.label_encoder.inverse_transform(\n",
    "                    [pred_encoded]\n",
    "                )[0]\n",
    "\n",
    "                \n",
    "                # Only include the prediction if it's NOT a \"Not_A_Citation\"\n",
    "                if citation_type != \"Not_A_Citation\":\n",
    "                    predictions.append(\n",
    "                        {\n",
    "                            \"row_id\": row_id,\n",
    "                            \"article_id\": article_id,\n",
    "                            \"dataset_id\": citation,\n",
    "                            \"type\": citation_type,\n",
    "                        }\n",
    "                    )\n",
    "                    row_id += 1\n",
    "\n",
    "        return pd.DataFrame(predictions)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        joblib.dump(\n",
    "            {\n",
    "                \"classifier\": self.classifier,\n",
    "                \"label_encoder\": self.label_encoder,\n",
    "                \"vectorizer\": self.vectorizer,\n",
    "            },\n",
    "            path,\n",
    "        )\n",
    "\n",
    "    def load_model(self, path):\n",
    "        data = joblib.load(path)\n",
    "        self.classifier = data[\"classifier\"]\n",
    "        self.label_encoder = data[\"label_encoder\"]\n",
    "        self.vectorizer = data[\"vectorizer\"]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_path = \"/kaggle/input/make-data-count-finding-data-references\"\n",
    "    detector = DataCitationDetector()\n",
    "    detector.train_model(base_path)\n",
    "    df = detector.predict(base_path)\n",
    "    df.to_csv(\"submission.csv\", index=False)\n",
    "    print(f\"Saved {len(df)} predictions to submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12656064,
     "sourceId": 82370,
     "sourceType": "competition"
    },
    {
     "sourceId": 245122625,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 453.167926,
   "end_time": "2025-06-16T07:02:36.091601",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-16T06:55:02.923675",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

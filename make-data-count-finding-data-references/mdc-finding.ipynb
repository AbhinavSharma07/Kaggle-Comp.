{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "215fe3d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T11:10:20.254337Z",
     "iopub.status.busy": "2025-06-16T11:10:20.253555Z",
     "iopub.status.idle": "2025-06-16T11:10:25.678574Z",
     "shell.execute_reply": "2025-06-16T11:10:25.677811Z"
    },
    "papermill": {
     "duration": 5.430593,
     "end_time": "2025-06-16T11:10:25.680318",
     "exception": false,
     "start_time": "2025-06-16T11:10:20.249725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    VotingClassifier,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cba5de87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T11:10:25.686777Z",
     "iopub.status.busy": "2025-06-16T11:10:25.686029Z",
     "iopub.status.idle": "2025-06-16T11:10:40.333011Z",
     "shell.execute_reply": "2025-06-16T11:10:40.332212Z"
    },
    "papermill": {
     "duration": 14.651731,
     "end_time": "2025-06-16T11:10:40.334559",
     "exception": false,
     "start_time": "2025-06-16T11:10:25.682828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import spacy\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    nlp = None\n",
    "    print(\n",
    "        \"Run `python -m spacy download en_core_web_sm` to enable NLP features.\"\n",
    "    )\n",
    "\n",
    "try:\n",
    "    import fitz  # PyMuPDF\n",
    "except ImportError:\n",
    "    fitz = None\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x: x  # fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bbcbab0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T11:10:40.340328Z",
     "iopub.status.busy": "2025-06-16T11:10:40.339642Z",
     "iopub.status.idle": "2025-06-16T11:10:40.367741Z",
     "shell.execute_reply": "2025-06-16T11:10:40.367080Z"
    },
    "papermill": {
     "duration": 0.032268,
     "end_time": "2025-06-16T11:10:40.369020",
     "exception": false,
     "start_time": "2025-06-16T11:10:40.336752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataCitationDetector:\n",
    "    def __init__(self):\n",
    "        # Initialize the vectorizer and label encoder\n",
    "        self.vectorizer = DictVectorizer(sparse=False)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.nlp = nlp  # spaCy NLP model (may be None if not available)\n",
    "\n",
    "        # Ensemble classifier using voting from RF, GB, and Logistic Regression\n",
    "        self.classifier = VotingClassifier(\n",
    "            estimators=[\n",
    "                (\n",
    "                    \"rf\",\n",
    "                    RandomForestClassifier(\n",
    "                        n_estimators=200,\n",
    "                        max_depth=10,\n",
    "                        class_weight=\"balanced\",\n",
    "                        random_state=42,\n",
    "                    ),\n",
    "                ),\n",
    "                (\n",
    "                    \"gb\",\n",
    "                    GradientBoostingClassifier(\n",
    "                        n_estimators=100, max_depth=6, random_state=42\n",
    "                    ),\n",
    "                ),\n",
    "                (\n",
    "                    \"lr\",\n",
    "                    LogisticRegression(\n",
    "                        max_iter=1000,\n",
    "                        class_weight=\"balanced\",\n",
    "                        random_state=42,\n",
    "                    ),\n",
    "                ),\n",
    "            ],\n",
    "            voting=\"soft\",  # Use predicted probabilities for voting\n",
    "        )\n",
    "\n",
    "        # Regex patterns indicating *primary* evidence of new data generation\n",
    "        self.primary_patterns = [\n",
    "            r\"\\b(?:we\\s+)?(?:generated|collected|produced|created|measured|recorded|obtained|acquired)\\b\",\n",
    "            r\"\\bthis\\s+study\\b\",\n",
    "            r\"\\bour\\s+(?:data|dataset|measurements|results)\\b\",\n",
    "            r\"\\bnew\\s+data\\b\",\n",
    "            r\"\\boriginal\\s+data\\b\",\n",
    "            r\"\\bnewly\\s+(?:generated|collected)\\b\",\n",
    "            r\"\\bspecifically\\s+(?:for\\s+)?this\\s+(?:study|work)\\b\",\n",
    "            r\"\\bin-house\\s+(?:generated|created)\\b\",\n",
    "        ]\n",
    "\n",
    "        # Regex patterns indicating *secondary* use of existing or external data\n",
    "        self.secondary_patterns = [\n",
    "            r\"\\bobtained\\s+from\\b\",\n",
    "            r\"\\breused?\\b\",\n",
    "            r\"\\bexisting\\s+data\\b\",\n",
    "            r\"\\bpreviously\\s+(?:published|reported)\\b\",\n",
    "            r\"\\bderived\\s+from\\b\",\n",
    "            r\"\\bopen\\s+data\\b\",\n",
    "            r\"\\bthird-party\\s+data\\b\",\n",
    "            r\"\\bbenchmark\\s+data\\b\",\n",
    "        ]\n",
    "\n",
    "        # Section-related keyword patterns to contextualize citations\n",
    "        self.section_patterns = {\n",
    "            \"methods\": r\"\\bmethods?\\b\",\n",
    "            \"results\": r\"\\bresults?\\b\",\n",
    "            \"introduction\": r\"\\bintroduction\\b\",\n",
    "            \"discussion\": r\"\\bdiscussion\\b\",\n",
    "            \"references\": r\"\\breferences?\\b\",\n",
    "            \"data_availability\": r\"\\bdata\\s+availability\\b\",\n",
    "        }\n",
    "\n",
    "        # Known citation patterns (DOIs, databases, repositories, etc.)\n",
    "        self.citation_patterns = {\n",
    "            \"doi\": r\"(?:https?://)?(?:dx\\.)?doi\\.org/(10\\.\\d{4,9}/[\\w./-]+)\",\n",
    "            \"zenodo\": r\"(?:https?://)?zenodo\\.org/record/(\\d+)\",\n",
    "            \"github\": r\"(?:https?://)?github\\.com/[\\w\\-_]+/[\\w\\-_]+\",\n",
    "            \"gse\": r\"GSE\\d+\",\n",
    "            \"sra\": r\"SRA\\d+\",\n",
    "            \"prjna\": r\"PRJNA\\d+\",\n",
    "            \"chembl\": r\"CHEMBL\\d+\",\n",
    "            \"pdb\": r\"PDB:\\w+\",\n",
    "            \"uniprot\": r\"UniProt:\\w+\",\n",
    "        }\n",
    "\n",
    "    def extract_text_from_xml(self, xml_path):\n",
    "        # Parse XML and extract plain text from all elements\n",
    "        try:\n",
    "            tree = ET.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "            return \" \".join(\n",
    "                elem.text.strip() for elem in root.iter() if elem.text\n",
    "            )\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        # Extract text from PDF using PyMuPDF (if available)\n",
    "        try:\n",
    "            if fitz:\n",
    "                doc = fitz.open(pdf_path)\n",
    "                text = \"\".join([page.get_text() for page in doc])\n",
    "                doc.close()\n",
    "                return text\n",
    "        except:\n",
    "            return \"\"\n",
    "        return \"\"\n",
    "\n",
    "    def load_documents(self, base_path, split=\"train\"):\n",
    "        # Load and combine text from XML and PDF files for each document\n",
    "        documents = {}\n",
    "        for xml_file in Path(base_path, split, \"XML\").glob(\"*.xml\"):\n",
    "            documents[xml_file.stem] = self.extract_text_from_xml(xml_file)\n",
    "        for pdf_file in Path(base_path, split, \"PDF\").glob(\"*.pdf\"):\n",
    "            if pdf_file.stem not in documents:\n",
    "                documents[pdf_file.stem] = self.extract_text_from_pdf(pdf_file)\n",
    "        return documents\n",
    "\n",
    "    def extract_data_citations(self, text):\n",
    "        # Extract all matched data citations using predefined patterns\n",
    "        citations = set()\n",
    "        for name, pattern in self.citation_patterns.items():\n",
    "            for match in re.findall(pattern, text, re.IGNORECASE):\n",
    "                if name == \"doi\":\n",
    "                    clean_match = match.rstrip(\".,;)\")\n",
    "                    citations.add(f\"https://doi.org/{clean_match}\")\n",
    "                elif name == \"zenodo\":\n",
    "                    citations.add(f\"https://zenodo.org/record/{match}\")\n",
    "                else:\n",
    "                    citations.add(match.strip().rstrip(\".,;)\"))\n",
    "        return list(citations)\n",
    "\n",
    "    def create_features(self, text, citation):\n",
    "        # Create feature dictionary for a given citation's context in the text\n",
    "        features = {}\n",
    "        text_lower = text.lower()\n",
    "        citation_lower = citation.lower().replace(\"https://doi.org/\", \"\")\n",
    "        citation_escaped = re.escape(citation_lower)\n",
    "\n",
    "        # Extract context around the citation for pattern matching\n",
    "        context_match = re.search(\n",
    "            f\".{{0,500}}{citation_escaped}.{{0,500}}\", text_lower\n",
    "        )\n",
    "        context = context_match.group() if context_match else text_lower\n",
    "\n",
    "        # Base features\n",
    "        features[\"text_length\"] = len(text)\n",
    "        features[\"citation_count\"] = text_lower.count(citation_lower)\n",
    "\n",
    "        # Count primary and secondary pattern matches in context\n",
    "        for i, pattern in enumerate(self.primary_patterns):\n",
    "            features[f\"primary_{i}\"] = len(re.findall(pattern, context, re.IGNORECASE))\n",
    "        for i, pattern in enumerate(self.secondary_patterns):\n",
    "            features[f\"secondary_{i}\"] = len(re.findall(pattern, context, re.IGNORECASE))\n",
    "\n",
    "        # Section presence as binary features\n",
    "        for sec, sec_pattern in self.section_patterns.items():\n",
    "            features[f\"in_{sec}\"] = int(bool(re.search(sec_pattern, context)))\n",
    "\n",
    "        # Citation type flags\n",
    "        features[\"is_doi\"] = int(\"doi.org\" in citation.lower())\n",
    "        features[\"is_github\"] = int(\"github.com\" in citation.lower())\n",
    "        features[\"is_database_id\"] = int(\n",
    "            any(x in citation.lower() for x in [\"chembl\", \"gse\", \"sra\", \"prjna\"])\n",
    "        )\n",
    "\n",
    "        # Additional context clues\n",
    "        features[\"near_figure\"] = len(re.findall(r\"fig(?:ure)?\\s*\\d+\", context, re.IGNORECASE))\n",
    "        features[\"near_table\"] = len(re.findall(r\"table\\s*\\d+\", context, re.IGNORECASE))\n",
    "        features[\"near_supplement\"] = len(re.findall(r\"supplement\", context, re.IGNORECASE))\n",
    "\n",
    "        # Named entity recognition feature (if spaCy is enabled)\n",
    "        if self.nlp:\n",
    "            doc = self.nlp(context[:1000])\n",
    "            features[\"num_entities\"] = len(doc.ents)\n",
    "        else:\n",
    "            features[\"num_entities\"] = 0\n",
    "\n",
    "        return features\n",
    "\n",
    "    def train_model(self, base_path):\n",
    "        # Train model using labeled data\n",
    "        print(\"Training model...\")\n",
    "        documents = self.load_documents(base_path, \"train\")\n",
    "        labels_df = pd.read_csv(Path(base_path) / \"train_labels.csv\")\n",
    "\n",
    "        # Build lookup for ground truth labels\n",
    "        true_labels = {}\n",
    "        for _, row in labels_df.iterrows():\n",
    "            article_id = row[\"article_id\"]\n",
    "            dataset_id = row[\"dataset_id\"]\n",
    "            citation_type = row[\"type\"]\n",
    "            if article_id not in true_labels:\n",
    "                true_labels[article_id] = {}\n",
    "            true_labels[article_id][dataset_id] = citation_type\n",
    "\n",
    "        X_dicts, y = [], []\n",
    "        print(\"Generating training samples (with negative sampling)...\")\n",
    "        for article_id, text in tqdm(documents.items()):\n",
    "            if article_id not in true_labels:\n",
    "                continue\n",
    "\n",
    "            found_citations = self.extract_data_citations(text)\n",
    "            true_citations_for_article = true_labels.get(article_id, {})\n",
    "\n",
    "            for citation in found_citations:\n",
    "                features = self.create_features(text, citation)\n",
    "                X_dicts.append(features)\n",
    "\n",
    "                # Label as true class or as \"Not_A_Citation\"\n",
    "                if citation in true_citations_for_article:\n",
    "                    y.append(true_citations_for_article[citation])\n",
    "                else:\n",
    "                    y.append(\"Not_A_Citation\")\n",
    "\n",
    "        if not X_dicts:\n",
    "            print(\"No training data found.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Generated {len(X_dicts)} training samples.\")\n",
    "        print(pd.Series(y).value_counts())\n",
    "\n",
    "        # Encode and train\n",
    "        X = self.vectorizer.fit_transform(X_dicts)\n",
    "        y_enc = self.label_encoder.fit_transform(y)\n",
    "\n",
    "        # Cross-validation to evaluate model\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        scores = []\n",
    "        for train_idx, val_idx in skf.split(X, y_enc):\n",
    "            self.classifier.fit(X[train_idx], y_enc[train_idx])\n",
    "            preds = self.classifier.predict(X[val_idx])\n",
    "            scores.append(f1_score(y_enc[val_idx], preds, average=\"weighted\"))\n",
    "\n",
    "        print(f\"Cross-val F1: {np.mean(scores):.4f} ± {np.std(scores):.4f}\")\n",
    "\n",
    "        # Final training on all data\n",
    "        self.classifier.fit(X, y_enc)\n",
    "\n",
    "    def predict(self, base_path):\n",
    "        # Generate predictions for test set\n",
    "        print(\"Generating predictions...\")\n",
    "        documents = self.load_documents(base_path, \"test\")\n",
    "        predictions = []\n",
    "        row_id = 0\n",
    "\n",
    "        for article_id, text in tqdm(documents.items()):\n",
    "            citations = self.extract_data_citations(text)\n",
    "            for citation in citations:\n",
    "                features = self.create_features(text, citation)\n",
    "                X = self.vectorizer.transform([features])\n",
    "                pred_encoded = self.classifier.predict(X)[0]\n",
    "                citation_type = self.label_encoder.inverse_transform([pred_encoded])[0]\n",
    "\n",
    "                # Keep only meaningful predictions\n",
    "                if citation_type != \"Not_A_Citation\":\n",
    "                    predictions.append({\n",
    "                        \"row_id\": row_id,\n",
    "                        \"article_id\": article_id,\n",
    "                        \"dataset_id\": citation,\n",
    "                        \"type\": citation_type,\n",
    "                    })\n",
    "                    row_id += 1\n",
    "\n",
    "        return pd.DataFrame(predictions)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        # Save trained components to disk\n",
    "        joblib.dump(\n",
    "            {\n",
    "                \"classifier\": self.classifier,\n",
    "                \"label_encoder\": self.label_encoder,\n",
    "                \"vectorizer\": self.vectorizer,\n",
    "            },\n",
    "            path,\n",
    "        )\n",
    "\n",
    "    def load_model(self, path):\n",
    "        # Load trained components from disk\n",
    "        data = joblib.load(path)\n",
    "        self.classifier = data[\"classifier\"]\n",
    "        self.label_encoder = data[\"label_encoder\"]\n",
    "        self.vectorizer = data[\"vectorizer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "823771d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T11:10:40.373891Z",
     "iopub.status.busy": "2025-06-16T11:10:40.373447Z",
     "iopub.status.idle": "2025-06-16T11:15:16.310977Z",
     "shell.execute_reply": "2025-06-16T11:15:16.309974Z"
    },
    "papermill": {
     "duration": 275.941357,
     "end_time": "2025-06-16T11:15:16.312432",
     "exception": false,
     "start_time": "2025-06-16T11:10:40.371075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Generating training samples (with negative sampling)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [03:59<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2330 training samples.\n",
      "Not_A_Citation    2187\n",
      "Primary             79\n",
      "Secondary           64\n",
      "Name: count, dtype: int64\n",
      "Cross-val F1: 0.9529 ± 0.0113\n",
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:17<00:00,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5 predictions to submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    base_path = \"/kaggle/input/make-data-count-finding-data-references\"\n",
    "    detector = DataCitationDetector()\n",
    "    detector.train_model(base_path)\n",
    "    df = detector.predict(base_path)\n",
    "    df.to_csv(\"submission.csv\", index=False)\n",
    "    print(f\"Saved {len(df)} predictions to submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12656064,
     "isSourceIdPinned": false,
     "sourceId": 82370,
     "sourceType": "competition"
    },
    {
     "sourceId": 245387354,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 245698518,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 322000,
     "modelInstanceId": 301506,
     "sourceId": 363124,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 322000,
     "modelInstanceId": 322452,
     "sourceId": 391615,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31042,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 305.32193,
   "end_time": "2025-06-16T11:15:19.268384",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-16T11:10:13.946454",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
